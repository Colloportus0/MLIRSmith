        -:    0:Source:/data/xcy/llvm-project-fdbc55a5-2/mlir/lib/Dialect/SparseTensor/Transforms/SparseTensorRewriting.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/SparseTensor/Transforms/CMakeFiles/obj.MLIRSparseTensorTransforms.dir/SparseTensorRewriting.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/SparseTensor/Transforms/CMakeFiles/obj.MLIRSparseTensorTransforms.dir/SparseTensorRewriting.cpp.gcda
        -:    0:Runs:128629
        -:    1://===- SparseTensorRewriting.cpp - Sparse tensor rewriting rules ----------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements rewriting rules that are specific to sparse tensors.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#include "CodegenUtils.h"
        -:   14:
        -:   15:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   16:#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
        -:   17:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   18:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   19:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   20:#include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
        -:   21:#include "mlir/Dialect/SparseTensor/Transforms/Passes.h"
        -:   22:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   23:#include "mlir/IR/AffineMap.h"
        -:   24:#include "mlir/IR/Matchers.h"
        -:   25:#include "mlir/Support/LLVM.h"
        -:   26:
        -:   27:using namespace mlir;
        -:   28:using namespace mlir::bufferization;
        -:   29:using namespace mlir::linalg;
        -:   30:using namespace mlir::sparse_tensor;
        -:   31:
        -:   32://===---------------------------------------------------------------------===//
        -:   33:// Helper methods for the actual rewriting rules.
        -:   34://===---------------------------------------------------------------------===//
        -:   35:
        -:   36:// Helper method to match any typed zero.
function _ZL11isZeroValueN4mlir5ValueE called 0 returned 0% blocks executed 0%
    #####:   37:static bool isZeroValue(Value val) {
    #####:   38:  return matchPattern(val, m_Zero()) || matchPattern(val, m_AnyZeroFloat());
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
        -:   39:}
        -:   40:
        -:   41:// Helper to detect a sparse tensor type operand.
function _ZL14isSparseTensorPN4mlir9OpOperandE called 0 returned 0% blocks executed 0%
    #####:   42:static bool isSparseTensor(OpOperand *op) {
    #####:   43:  if (auto enc = getSparseTensorEncoding(op->get().getType())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   44:    if (llvm::is_contained(enc.getDimLevelType(), DimLevelType::Compressed))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   45:      return true;
        -:   46:  }
    #####:   47:  return false;
        -:   48:}
        -:   49:
        -:   50:// Helper method to find zero/uninitialized allocation.
function _ZL7isAllocPN4mlir9OpOperandEb called 0 returned 0% blocks executed 0%
    #####:   51:static bool isAlloc(OpOperand *op, bool isZero) {
    #####:   52:  Value val = op->get();
call    0 never executed
    #####:   53:  if (auto alloc = val.getDefiningOp<AllocTensorOp>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   54:    Value copy = alloc.getCopy();
call    0 never executed
    #####:   55:    if (isZero)
branch  0 never executed
branch  1 never executed
    #####:   56:      return copy && isZeroValue(copy);
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:   57:    return !copy;
        -:   58:  }
    #####:   59:  return false;
        -:   60:}
        -:   61:
        -:   62:// Helper to detect sampling operation.
function _ZL10isSamplingN4mlir6linalg9GenericOpE called 0 returned 0% blocks executed 0%
    #####:   63:static bool isSampling(GenericOp op) {
    #####:   64:  auto yieldOp = cast<linalg::YieldOp>(op.getRegion().front().getTerminator());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:   65:  if (auto *def = yieldOp.getOperand(0).getDefiningOp()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   66:    if (isa<arith::MulFOp>(def) || isa<arith::MulIOp>(def)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
        -:   67:      // Both scalar input arguments used exactly once.
    #####:   68:      Value s1 = op.getBlock()->getArgument(0);
call    0 never executed
call    1 never executed
    #####:   69:      Value s2 = op.getBlock()->getArgument(1);
call    0 never executed
    #####:   70:      return (def->getOperand(0) == s1 && def->getOperand(1) == s2) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:   71:             (def->getOperand(1) == s1 && def->getOperand(0) == s2);
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -:   72:    }
        -:   73:  }
        -:   74:  return false;
        -:   75:}
        -:   76:
        -:   77:// Helper to detect chain of multiplications that do not involve x.
function _ZL10isMulChainN4mlir5ValueES0_ called 0 returned 0% blocks executed 0%
    #####:   78:static bool isMulChain(Value val, Value x) {
    #####:   79:  if (auto arg = val.dyn_cast<BlockArgument>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   80:    return arg != x;
    #####:   81:  if (auto *def = val.getDefiningOp()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   82:    if (isa<arith::MulFOp>(def) || isa<arith::MulIOp>(def))
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:   83:      return isMulChain(def->getOperand(0), x) &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:   84:             isMulChain(def->getOperand(1), x);
call    0 never executed
call    1 never executed
        -:   85:  }
        -:   86:  return false;
        -:   87:}
        -:   88:
        -:   89:// Helper to detect x = x + <multiplications>.
function _ZL10isSumOfMulN4mlir6linalg9GenericOpE called 0 returned 0% blocks executed 0%
    #####:   90:static bool isSumOfMul(GenericOp op) {
    #####:   91:  auto yieldOp = cast<linalg::YieldOp>(op.getRegion().front().getTerminator());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:   92:  if (auto *def = yieldOp.getOperand(0).getDefiningOp()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   93:    if (isa<arith::AddFOp>(def) || isa<arith::AddIOp>(def)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:   94:      Value x = op.getBlock()->getArguments().back();
call    0 never executed
call    1 never executed
    #####:   95:      return (def->getOperand(0) == x && isMulChain(def->getOperand(1), x)) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
    #####:   96:             (def->getOperand(1) == x && isMulChain(def->getOperand(0), x));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
        -:   97:    }
        -:   98:  }
        -:   99:  return false;
        -:  100:}
        -:  101:
        -:  102:// Helper to detect direct yield of a zero value.
function _ZL11isZeroYieldN4mlir6linalg9GenericOpE called 0 returned 0% blocks executed 0%
    #####:  103:static bool isZeroYield(GenericOp op) {
    #####:  104:  auto yieldOp = cast<linalg::YieldOp>(op.getRegion().front().getTerminator());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  105:  if (auto arg = yieldOp.getOperand(0).dyn_cast<BlockArgument>()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  106:    if (arg.getOwner()->getParentOp() == op) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  107:      return isZeroValue(op->getOperand(arg.getArgNumber()));
call    0 never executed
call    1 never executed
        -:  108:    }
        -:  109:  }
    #####:  110:  return isZeroValue(yieldOp.getOperand(0));
call    0 never executed
call    1 never executed
        -:  111:}
        -:  112:
        -:  113:/// Populates given sizes array from type (for static sizes) and from
        -:  114:/// the tensor (for dynamic sizes).
function _ZL14sizesForTensorRN4mlir9OpBuilderERN4llvm11SmallVectorINS_5ValueELj4EEENS_8LocationENS_10ShapedTypeES4_ called 0 returned 0% blocks executed 0%
    #####:  115:static void sizesForTensor(OpBuilder &builder, SmallVector<Value, 4> &sizes,
        -:  116:                           Location loc, ShapedType stp, Value tensor) {
    #####:  117:  for (const auto &d : enumerate(stp.getShape())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  118:    Value dim;
    #####:  119:    if (d.value() == ShapedType::kDynamicSize)
branch  0 never executed
branch  1 never executed
    #####:  120:      dim = builder.create<tensor::DimOp>(loc, tensor, d.index());
call    0 never executed
        -:  121:    else
    #####:  122:      dim = constantIndex(builder, loc, d.value());
call    0 never executed
    #####:  123:    sizes.push_back(dim);
call    0 never executed
        -:  124:  }
    #####:  125:}
        -:  126:
        -:  127:// TODO: The dim level property of the COO type relies on input tensors, the
        -:  128:// shape relies on the output tensor
        -:  129:// Helpers to setup a COO type.
function _ZL23getUnorderedCOOFromTypeN4mlir16RankedTensorTypeE called 0 returned 0% blocks executed 0%
    #####:  130:static RankedTensorType getUnorderedCOOFromType(RankedTensorType src) {
    #####:  131:  auto *ctx = src.getContext();
call    0 never executed
    #####:  132:  auto rank = src.getRank();
call    0 never executed
    #####:  133:  SmallVector<DimLevelType, 4> dims;
call    0 never executed
        -:  134:
        -:  135:  // An unordered and non-unique compressed dim at beginning.
    #####:  136:  dims.push_back(DimLevelType::CompressedNuNo);
call    0 never executed
        -:  137:
    #####:  138:  if (rank > 1) {
branch  0 never executed
branch  1 never executed
        -:  139:    // TODO: it is actually ordered at the level for ordered input.
        -:  140:    // Followed by unordered non-unique n-2 singleton levels.
    #####:  141:    std::fill_n(std::back_inserter(dims), rank - 2,
    #####:  142:                DimLevelType::SingletonNuNo);
        -:  143:    // TODO: only if all the inputs (for concatentate) are unique at the last
        -:  144:    // level should the COO has a unique level at the end. Ends by a unordered
        -:  145:    // unique singleton level unless the tensor rank is 1.
    #####:  146:    dims.push_back(DimLevelType::SingletonNo);
call    0 never executed
        -:  147:  }
    #####:  148:  SparseTensorEncodingAttr encSrc = getSparseTensorEncoding(src);
call    0 never executed
        -:  149:  // TODO: Maybe pick the bitwidth based on input/output tensors (probably the
        -:  150:  // largest one among them) in the original operation instead of using the
        -:  151:  // default value.
    #####:  152:  auto enc = SparseTensorEncodingAttr::get(
        -:  153:      ctx, dims, AffineMap::getMultiDimIdentityMap(rank, ctx), AffineMap(),
    #####:  154:      encSrc.getPointerBitWidth(), encSrc.getIndexBitWidth());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  155:  return RankedTensorType::get(src.getShape(), src.getElementType(), enc);
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  156:}
        -:  157:
        -:  158:/// Collects the dynamic dimension sizes for `tp` with the assumption that
        -:  159:/// `sizes` are the dimension sizes for the type. Stores the dynamic dimension
        -:  160:/// sizes to dynSizes.
function _ZL15getDynamicSizesN4mlir16RankedTensorTypeERKN4llvm15SmallVectorImplINS_5ValueEEERS4_ called 0 returned 0% blocks executed 0%
    #####:  161:static void getDynamicSizes(RankedTensorType tp,
        -:  162:                            const SmallVectorImpl<Value> &sizes,
        -:  163:                            SmallVectorImpl<Value> &dynSizes) {
    #####:  164:  for (const auto &d : enumerate(tp.getShape())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  165:    if (d.value() == ShapedType::kDynamicSize)
branch  0 never executed
branch  1 never executed
    #####:  166:      dynSizes.push_back(sizes[d.index()]);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  167:  }
    #####:  168:}
        -:  169:
        -:  170://===---------------------------------------------------------------------===//
        -:  171:// The actual sparse tensor rewriting rules.
        -:  172://===---------------------------------------------------------------------===//
        -:  173:
        -:  174:namespace {
        -:  175:
        -:  176:/// Rewriting rule that converts direct yield of zero with initial allocation.
        -:  177:struct FoldInvariantYield : public OpRewritePattern<GenericOp> {
        -:  178:public:
        -:  179:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -:  180:
function _ZNK12_GLOBAL__N_118FoldInvariantYield15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  181:  LogicalResult matchAndRewrite(GenericOp op,
        -:  182:                                PatternRewriter &rewriter) const override {
    #####:  183:    if (!op.hasTensorSemantics() || op.getNumResults() != 1 ||
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  184:        !isAlloc(op.getDpsInitOperand(0), /*isZero=*/false) ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  185:        !isZeroYield(op) || !op.getDpsInitOperand(0)->get().hasOneUse())
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  186:      return failure();
    #####:  187:    auto outputType = op.getResult(0).getType().cast<RankedTensorType>();
call    0 never executed
        -:  188:    // Yielding zero on newly allocated (all-zero) sparse tensors can be
        -:  189:    // optimized out directly (regardless of dynamic or static size).
    #####:  190:    if (getSparseTensorEncoding(outputType)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  191:      rewriter.replaceOp(op, op.getDpsInitOperand(0)->get());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  192:      return success();
        -:  193:    }
        -:  194:    // Incorporate zero value into allocation copy.
    #####:  195:    if (!outputType.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  196:      return failure();
    #####:  197:    Value zero = constantZero(rewriter, op.getLoc(), op.getResult(0).getType());
call    0 never executed
    #####:  198:    AllocTensorOp a =
    #####:  199:        op.getDpsInitOperand(0)->get().getDefiningOp<AllocTensorOp>();
call    0 never executed
call    1 never executed
function _ZZNK12_GLOBAL__N_118FoldInvariantYield15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterEENKUlvE_clEv.isra.0 called 0 returned 0% blocks executed 0%
    #####:  200:    rewriter.updateRootInPlace(a, [&]() { a.getCopyMutable().assign(zero); });
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  201:    rewriter.replaceOp(op, op.getDpsInitOperand(0)->get());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  202:    return success();
        -:  203:  }
        -:  204:};
        -:  205:
        -:  206:/// Rewriting rule that converts two kernels:
        -:  207:///
        -:  208:///      T(i,j) = SUM(k, A(i,j,k) * B(i,j,k) * ... )
        -:  209:///      X(i,j) = S(i,j) * T(i,j)
        -:  210:///
        -:  211:/// into a single kernel, using distributive law:
        -:  212:///
        -:  213:///      X(i,j) = SUM(k, S(i,j) * A(i,j,k) * B(i,j,k) * ... )
        -:  214:///
        -:  215:/// This kind of fusion (merging two ops into one but using arithmetic
        -:  216:/// equalities that may not hold for floating-point computations) would
        -:  217:/// be undesirable in the dense case, since we distribute the multiplication
        -:  218:/// into the reduction loop. However, for sparse sampling tensor S, such
        -:  219:/// a fusion may actually reduce the asymptotic complexity of the kernel,
        -:  220:/// since intermediate results may be nullified.
        -:  221:struct FuseSparseMultiplyOverAdd : public OpRewritePattern<GenericOp> {
        -:  222:public:
        -:  223:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -:  224:
function _ZNK12_GLOBAL__N_125FuseSparseMultiplyOverAdd15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  225:  LogicalResult matchAndRewrite(GenericOp op,
        -:  226:                                PatternRewriter &rewriter) const override {
        -:  227:    // Check consumer.
    #####:  228:    if (!op.hasTensorSemantics() || op.getNumDpsInputs() != 2 ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  229:        op.getNumResults() != 1 ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  230:        op.getNumParallelLoops() != op.getNumLoops() ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  231:        !op.getMatchingIndexingMap(op.getDpsInitOperand(0)).isIdentity() ||
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  232:        !op.getMatchingIndexingMap(op.getDpsInputOperand(0)).isIdentity() ||
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####:  233:        !op.getMatchingIndexingMap(op.getDpsInputOperand(1)).isIdentity())
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  234:      return failure();
        -:  235:    // Find consuming OP2(sparse, other) or OP2(other, sparse). The other
        -:  236:    // operand can be sparse or dense, since the point of this rewriting rule
        -:  237:    // is detecting a situation in which *more* sparsity is introduced into
        -:  238:    // a computation, be it already sparse or still dense.
    #####:  239:    unsigned other = 0;
    #####:  240:    if (isSparseTensor(op.getDpsInputOperand(0)))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  241:      other = 1;
    #####:  242:    else if (!isSparseTensor(op.getDpsInputOperand(1)))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  243:      return failure();
        -:  244:    // Check producer.
    #####:  245:    auto prod = dyn_cast_or_null<GenericOp>(
    #####:  246:        op.getDpsInputOperand(other)->get().getDefiningOp());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  247:    if (!prod || !prod.hasTensorSemantics() || prod.getNumResults() != 1 ||
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  248:        !prod.getResult(0).hasOneUse())
branch  0 never executed
branch  1 never executed
    #####:  249:      return failure();
        -:  250:    // Sampling consumer and sum of multiplication chain producer.
    #####:  251:    if (!isAlloc(op.getDpsInitOperand(0), /*isZero=*/false) ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  252:        !isAlloc(prod.getDpsInitOperand(0), /*isZero=*/true) ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  253:        !isSampling(op) || !isSumOfMul(prod))
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  254:      return failure();
        -:  255:    // Modify operand structure of producer and consumer.
    #####:  256:    Location loc = prod.getLoc();
call    0 never executed
    #####:  257:    SmallVector<Value> inputOps = prod.getInputs();
call    0 never executed
call    1 never executed
    #####:  258:    SmallVector<Value> outputOps = op.getOutputs();
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  259:    SmallVector<AffineMap> fusedIndexMaps = prod.getIndexingMapsArray();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  260:    inputOps.push_back(op.getDpsInputOperand(1 - other)->get());
call    0 never executed
call    1 never executed
    #####:  261:    fusedIndexMaps.push_back(fusedIndexMaps.back()); // mimic other
call    0 never executed
call    1 never executed
        -:  262:    // Fuse producer and consumer into a new generic op.
    #####:  263:    auto fusedOp = rewriter.create<GenericOp>(
    #####:  264:        loc, op.getResult(0).getType(), inputOps, outputOps,
call    0 never executed
    #####:  265:        rewriter.getAffineMapArrayAttr(fusedIndexMaps), prod.getIteratorTypes(),
call    0 never executed
call    1 never executed
    #####:  266:        /*doc=*/nullptr, /*library_call=*/nullptr);
call    0 never executed
call    1 never executed
    #####:  267:    Block &prodBlock = prod.getRegion().front();
call    0 never executed
call    1 never executed
    #####:  268:    Block &consBlock = op.getRegion().front();
call    0 never executed
call    1 never executed
    #####:  269:    BlockAndValueMapping mapper;
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  270:    Block *fusedBlock = new Block();
call    0 never executed
call    1 never executed
    #####:  271:    fusedOp.getRegion().push_back(fusedBlock);
call    0 never executed
call    1 never executed
    #####:  272:    unsigned num = prodBlock.getNumArguments();
    #####:  273:    for (unsigned i = 0; i < num - 1; i++)
branch  0 never executed
branch  1 never executed
    #####:  274:      addArg(mapper, fusedBlock, prodBlock.getArgument(i));
call    0 never executed
    #####:  275:    addArg(mapper, fusedBlock, consBlock.getArgument(1 - other));
call    0 never executed
    #####:  276:    addArg(mapper, fusedBlock, prodBlock.getArgument(num - 1));
call    0 never executed
        -:  277:    // Clone bodies of the producer and consumer in new evaluation order.
    #####:  278:    auto *acc = prodBlock.getTerminator()->getOperand(0).getDefiningOp();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  279:    auto *sampler = consBlock.getTerminator()->getOperand(0).getDefiningOp();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  280:    rewriter.setInsertionPointToStart(fusedBlock);
branch  0 never executed
branch  1 never executed
    #####:  281:    Value last;
    #####:  282:    for (auto &op : prodBlock.without_terminator())
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  283:      if (&op != acc) {
branch  0 never executed
branch  1 never executed
    #####:  284:        last = op.getResult(0);
call    0 never executed
    #####:  285:        rewriter.clone(op, mapper);
call    0 never executed
        -:  286:      }
    #####:  287:    mapper.map(consBlock.getArgument(other), fusedBlock->back().getResult(0));
call    0 never executed
call    1 never executed
    #####:  288:    mapper.map(last, rewriter.clone(*sampler, mapper)->getResult(0));
call    0 never executed
call    1 never executed
    #####:  289:    last = rewriter.clone(*acc, mapper)->getResult(0);
call    0 never executed
call    1 never executed
    #####:  290:    rewriter.create<linalg::YieldOp>(loc, last);
call    0 never executed
        -:  291:    // Force initial value on merged allocation for dense outputs.
    #####:  292:    if (!getSparseTensorEncoding(op.getResult(0).getType())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  293:      Value init = prod.getDpsInitOperand(0)
call    0 never executed
    #####:  294:                       ->get()
call    0 never executed
    #####:  295:                       .getDefiningOp<AllocTensorOp>()
call    0 never executed
    #####:  296:                       .getCopy();
call    0 never executed
    #####:  297:      AllocTensorOp a =
    #####:  298:          op.getDpsInitOperand(0)->get().getDefiningOp<AllocTensorOp>();
call    0 never executed
call    1 never executed
function _ZZNK12_GLOBAL__N_125FuseSparseMultiplyOverAdd15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterEENKUlvE_clEv.isra.0 called 0 returned 0% blocks executed 0%
    #####:  299:      rewriter.updateRootInPlace(a, [&]() { a.getCopyMutable().assign(init); });
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
        -:  300:    }
        -:  301:    // Replace consumer with fused operation. Old producer
        -:  302:    // and consumer ops will be removed by DCE.
    #####:  303:    rewriter.replaceOp(op, fusedOp->getResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  304:    return success();
call    0 never executed
        -:  305:  }
        -:  306:
        -:  307:private:
        -:  308:  // Helper to add argument and record the mapping.
function _ZN12_GLOBAL__N_125FuseSparseMultiplyOverAdd6addArgERN4mlir20BlockAndValueMappingEPNS1_5BlockENS1_13BlockArgumentE called 0 returned 0% blocks executed 0%
    #####:  309:  static void addArg(BlockAndValueMapping &mapper, Block *b, BlockArgument a) {
    #####:  310:    mapper.map(a, b->addArgument(a.getType(), a.getLoc()));
call    0 never executed
call    1 never executed
    #####:  311:  }
        -:  312:};
        -:  313:
        -:  314:/// Sparse rewriting rule for sparse-to-sparse reshape operator.
        -:  315:template <typename ReshapeOp>
        -:  316:struct Sparse2SparseReshapeRewriter : public OpRewritePattern<ReshapeOp> {
        -:  317:public:
        -:  318:  using OpRewritePattern<ReshapeOp>::OpRewritePattern;
        -:  319:
    #####:  320:  LogicalResult matchAndRewrite(ReshapeOp op,
        -:  321:                                PatternRewriter &rewriter) const override {
    #####:  322:    Location loc = op.getLoc();
    #####:  323:    Value srcTensor = op.getSrc();
    #####:  324:    auto srcTp = srcTensor.getType().template cast<RankedTensorType>();
    #####:  325:    auto dstTp = op.getResult().getType().template cast<RankedTensorType>();
    #####:  326:    SparseTensorEncodingAttr encSrc = getSparseTensorEncoding(srcTp);
    #####:  327:    SparseTensorEncodingAttr encDst = getSparseTensorEncoding(dstTp);
    #####:  328:    if (!encDst || !encSrc) {
    #####:  329:      return failure();
        -:  330:    }
        -:  331:
        -:  332:    // Generate code to represent the static dimension constants or compute
        -:  333:    // the dynamic dimension values.
    #####:  334:    SmallVector<Value, 4> srcSizes;
    #####:  335:    sizesForTensor(rewriter, srcSizes, loc, srcTp, srcTensor);
    #####:  336:    SmallVector<Value, 4> dstSizes;
    #####:  337:    SmallVector<Value, 4> dstDynSizes;
    #####:  338:    if (dstTp.hasStaticShape()) {
    #####:  339:      for (auto d : dstTp.getShape())
    #####:  340:        dstSizes.push_back(constantIndex(rewriter, loc, d));
        -:  341:    } else {
    #####:  342:      ArrayRef<int64_t> dstShape = dstTp.getShape();
    #####:  343:      genReshapeDstShape(loc, rewriter, dstSizes, srcSizes, dstShape,
        -:  344:                         op.getReassociationIndices());
    #####:  345:      for (auto &d : llvm::enumerate(dstShape)) {
    #####:  346:        if (d.value() == ShapedType::kDynamicSize)
    #####:  347:          dstDynSizes.push_back(dstSizes[d.index()]);
        -:  348:      }
        -:  349:    }
        -:  350:
        -:  351:    // Implement the sparse2sparse reshape as follows:
        -:  352:    //   %tmp = bufferization.alloc_tensor : unordered COO
        -:  353:    //   foreach srcCoords %srcTensor
        -:  354:    //     insert translateIndicesArray(srcCoords), %tmp
        -:  355:    //   %t = sparse_tensor.cast %tmp
    #####:  356:    RankedTensorType cooTp = getUnorderedCOOFromType(dstTp);
        -:  357:    auto cooBuffer =
    #####:  358:        rewriter.create<AllocTensorOp>(loc, cooTp, dstDynSizes).getResult();
    #####:  359:    rewriter.create<ForeachOp>(
    #####:  360:        loc, srcTensor, [&](OpBuilder &builder, Location loc, ValueRange args) {
    #####:  361:          SmallVector<Value, 4> srcIndices;
    #####:  362:          SmallVector<Value, 4> dstIndices;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  363:          for (int64_t i = 0, e = srcTp.getRank(); i < e; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  364:            uint64_t dim = toStoredDim(encSrc, i);
call    0 never executed
call    1 never executed
    #####:  365:            srcIndices.push_back(args[dim]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  366:          }
    #####:  367:          translateIndicesArray(builder, loc, op.getReassociationIndices(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
        -:  368:                                srcIndices, srcSizes, dstSizes, dstIndices);
    #####:  369:          builder.create<InsertOp>(loc, args.back(), cooBuffer, dstIndices);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  370:          builder.create<sparse_tensor::YieldOp>(loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
        -:  371:        });
        -:  372:
    #####:  373:    rewriter.replaceOpWithNewOp<ConvertOp>(op, dstTp, cooBuffer);
    #####:  374:    return success();
        -:  375:  }
------------------
_ZNK12_GLOBAL__N_128Sparse2SparseReshapeRewriterIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_128Sparse2SparseReshapeRewriterIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  320:  LogicalResult matchAndRewrite(ReshapeOp op,
        -:  321:                                PatternRewriter &rewriter) const override {
    #####:  322:    Location loc = op.getLoc();
call    0 never executed
    #####:  323:    Value srcTensor = op.getSrc();
call    0 never executed
    #####:  324:    auto srcTp = srcTensor.getType().template cast<RankedTensorType>();
call    0 never executed
    #####:  325:    auto dstTp = op.getResult().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  326:    SparseTensorEncodingAttr encSrc = getSparseTensorEncoding(srcTp);
call    0 never executed
    #####:  327:    SparseTensorEncodingAttr encDst = getSparseTensorEncoding(dstTp);
call    0 never executed
    #####:  328:    if (!encDst || !encSrc) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  329:      return failure();
        -:  330:    }
        -:  331:
        -:  332:    // Generate code to represent the static dimension constants or compute
        -:  333:    // the dynamic dimension values.
    #####:  334:    SmallVector<Value, 4> srcSizes;
call    0 never executed
    #####:  335:    sizesForTensor(rewriter, srcSizes, loc, srcTp, srcTensor);
call    0 never executed
call    1 never executed
    #####:  336:    SmallVector<Value, 4> dstSizes;
branch  0 never executed
branch  1 never executed
    #####:  337:    SmallVector<Value, 4> dstDynSizes;
branch  0 never executed
branch  1 never executed
    #####:  338:    if (dstTp.hasStaticShape()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  339:      for (auto d : dstTp.getShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  340:        dstSizes.push_back(constantIndex(rewriter, loc, d));
call    0 never executed
call    1 never executed
        -:  341:    } else {
    #####:  342:      ArrayRef<int64_t> dstShape = dstTp.getShape();
call    0 never executed
    #####:  343:      genReshapeDstShape(loc, rewriter, dstSizes, srcSizes, dstShape,
call    0 never executed
call    1 never executed
call    2 never executed
        -:  344:                         op.getReassociationIndices());
    #####:  345:      for (auto &d : llvm::enumerate(dstShape)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  346:        if (d.value() == ShapedType::kDynamicSize)
branch  0 never executed
branch  1 never executed
    #####:  347:          dstDynSizes.push_back(dstSizes[d.index()]);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  348:      }
        -:  349:    }
        -:  350:
        -:  351:    // Implement the sparse2sparse reshape as follows:
        -:  352:    //   %tmp = bufferization.alloc_tensor : unordered COO
        -:  353:    //   foreach srcCoords %srcTensor
        -:  354:    //     insert translateIndicesArray(srcCoords), %tmp
        -:  355:    //   %t = sparse_tensor.cast %tmp
    #####:  356:    RankedTensorType cooTp = getUnorderedCOOFromType(dstTp);
call    0 never executed
        -:  357:    auto cooBuffer =
    #####:  358:        rewriter.create<AllocTensorOp>(loc, cooTp, dstDynSizes).getResult();
call    0 never executed
call    1 never executed
    #####:  359:    rewriter.create<ForeachOp>(
call    0 never executed
        -:  360:        loc, srcTensor, [&](OpBuilder &builder, Location loc, ValueRange args) {
        -:  361:          SmallVector<Value, 4> srcIndices;
        -:  362:          SmallVector<Value, 4> dstIndices;
        -:  363:          for (int64_t i = 0, e = srcTp.getRank(); i < e; i++) {
        -:  364:            uint64_t dim = toStoredDim(encSrc, i);
        -:  365:            srcIndices.push_back(args[dim]);
        -:  366:          }
        -:  367:          translateIndicesArray(builder, loc, op.getReassociationIndices(),
        -:  368:                                srcIndices, srcSizes, dstSizes, dstIndices);
        -:  369:          builder.create<InsertOp>(loc, args.back(), cooBuffer, dstIndices);
        -:  370:          builder.create<sparse_tensor::YieldOp>(loc);
        -:  371:        });
        -:  372:
    #####:  373:    rewriter.replaceOpWithNewOp<ConvertOp>(op, dstTp, cooBuffer);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  374:    return success();
branch  0 never executed
branch  1 never executed
        -:  375:  }
------------------
_ZNK12_GLOBAL__N_128Sparse2SparseReshapeRewriterIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_128Sparse2SparseReshapeRewriterIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  320:  LogicalResult matchAndRewrite(ReshapeOp op,
        -:  321:                                PatternRewriter &rewriter) const override {
    #####:  322:    Location loc = op.getLoc();
call    0 never executed
    #####:  323:    Value srcTensor = op.getSrc();
call    0 never executed
    #####:  324:    auto srcTp = srcTensor.getType().template cast<RankedTensorType>();
call    0 never executed
    #####:  325:    auto dstTp = op.getResult().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  326:    SparseTensorEncodingAttr encSrc = getSparseTensorEncoding(srcTp);
call    0 never executed
    #####:  327:    SparseTensorEncodingAttr encDst = getSparseTensorEncoding(dstTp);
call    0 never executed
    #####:  328:    if (!encDst || !encSrc) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  329:      return failure();
        -:  330:    }
        -:  331:
        -:  332:    // Generate code to represent the static dimension constants or compute
        -:  333:    // the dynamic dimension values.
    #####:  334:    SmallVector<Value, 4> srcSizes;
call    0 never executed
    #####:  335:    sizesForTensor(rewriter, srcSizes, loc, srcTp, srcTensor);
call    0 never executed
call    1 never executed
    #####:  336:    SmallVector<Value, 4> dstSizes;
branch  0 never executed
branch  1 never executed
    #####:  337:    SmallVector<Value, 4> dstDynSizes;
branch  0 never executed
branch  1 never executed
    #####:  338:    if (dstTp.hasStaticShape()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  339:      for (auto d : dstTp.getShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  340:        dstSizes.push_back(constantIndex(rewriter, loc, d));
call    0 never executed
call    1 never executed
        -:  341:    } else {
    #####:  342:      ArrayRef<int64_t> dstShape = dstTp.getShape();
call    0 never executed
    #####:  343:      genReshapeDstShape(loc, rewriter, dstSizes, srcSizes, dstShape,
call    0 never executed
call    1 never executed
call    2 never executed
        -:  344:                         op.getReassociationIndices());
    #####:  345:      for (auto &d : llvm::enumerate(dstShape)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  346:        if (d.value() == ShapedType::kDynamicSize)
branch  0 never executed
branch  1 never executed
    #####:  347:          dstDynSizes.push_back(dstSizes[d.index()]);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  348:      }
        -:  349:    }
        -:  350:
        -:  351:    // Implement the sparse2sparse reshape as follows:
        -:  352:    //   %tmp = bufferization.alloc_tensor : unordered COO
        -:  353:    //   foreach srcCoords %srcTensor
        -:  354:    //     insert translateIndicesArray(srcCoords), %tmp
        -:  355:    //   %t = sparse_tensor.cast %tmp
    #####:  356:    RankedTensorType cooTp = getUnorderedCOOFromType(dstTp);
call    0 never executed
        -:  357:    auto cooBuffer =
    #####:  358:        rewriter.create<AllocTensorOp>(loc, cooTp, dstDynSizes).getResult();
call    0 never executed
call    1 never executed
    #####:  359:    rewriter.create<ForeachOp>(
call    0 never executed
        -:  360:        loc, srcTensor, [&](OpBuilder &builder, Location loc, ValueRange args) {
        -:  361:          SmallVector<Value, 4> srcIndices;
        -:  362:          SmallVector<Value, 4> dstIndices;
        -:  363:          for (int64_t i = 0, e = srcTp.getRank(); i < e; i++) {
        -:  364:            uint64_t dim = toStoredDim(encSrc, i);
        -:  365:            srcIndices.push_back(args[dim]);
        -:  366:          }
        -:  367:          translateIndicesArray(builder, loc, op.getReassociationIndices(),
        -:  368:                                srcIndices, srcSizes, dstSizes, dstIndices);
        -:  369:          builder.create<InsertOp>(loc, args.back(), cooBuffer, dstIndices);
        -:  370:          builder.create<sparse_tensor::YieldOp>(loc);
        -:  371:        });
        -:  372:
    #####:  373:    rewriter.replaceOpWithNewOp<ConvertOp>(op, dstTp, cooBuffer);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  374:    return success();
branch  0 never executed
branch  1 never executed
        -:  375:  }
------------------
        -:  376:};
        -:  377:
        -:  378:/// Sparse rewriting rule for sparse-to-dense and dense-to-sparse reshape
        -:  379:/// operator.
        -:  380:template <typename ReshapeOp>
        -:  381:struct ReshapeRewriter : public OpRewritePattern<ReshapeOp> {
        -:  382:public:
        -:  383:  using OpRewritePattern<ReshapeOp>::OpRewritePattern;
        -:  384:
    #####:  385:  LogicalResult matchAndRewrite(ReshapeOp op,
        -:  386:                                PatternRewriter &rewriter) const override {
    #####:  387:    Location loc = op->getLoc();
    #####:  388:    auto encDst = getSparseTensorEncoding(op.getResult().getType());
    #####:  389:    auto encSrc = getSparseTensorEncoding(op.getSrc().getType());
        -:  390:    // Since a pure dense expansion is very cheap (change of view), for
        -:  391:    // a sparse2dense or dense2sparse, we can simply unfuse a sparse
        -:  392:    // conversion from the reshape operation itself.
        -:  393:    // All other cases are handled elsewhere.
    #####:  394:    if (encDst && encSrc) {
    #####:  395:      return failure();
        -:  396:    }
    #####:  397:    if (encSrc) {
    #####:  398:      RankedTensorType rtp =
    #####:  399:          op.getSrc().getType().template cast<RankedTensorType>();
        -:  400:      auto denseTp =
    #####:  401:          RankedTensorType::get(rtp.getShape(), rtp.getElementType());
    #####:  402:      auto convert = rewriter.create<ConvertOp>(loc, denseTp, op.getSrc());
    #####:  403:      op->setOperand(0, convert);
    #####:  404:      return success();
        -:  405:    }
    #####:  406:    if (encDst) {
    #####:  407:      RankedTensorType rtp =
    #####:  408:          op.getResult().getType().template cast<RankedTensorType>();
        -:  409:      auto denseTp =
    #####:  410:          RankedTensorType::get(rtp.getShape(), rtp.getElementType());
    #####:  411:      auto reshape = rewriter.create<ReshapeOp>(loc, denseTp, op.getSrc(),
        -:  412:                                                op.getReassociation());
    #####:  413:      Value convert = rewriter.create<ConvertOp>(loc, rtp, reshape);
    #####:  414:      rewriter.replaceOp(op, convert);
    #####:  415:      return success();
        -:  416:    }
    #####:  417:    return failure();
        -:  418:  }
------------------
_ZNK12_GLOBAL__N_115ReshapeRewriterIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_115ReshapeRewriterIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  385:  LogicalResult matchAndRewrite(ReshapeOp op,
        -:  386:                                PatternRewriter &rewriter) const override {
    #####:  387:    Location loc = op->getLoc();
call    0 never executed
    #####:  388:    auto encDst = getSparseTensorEncoding(op.getResult().getType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  389:    auto encSrc = getSparseTensorEncoding(op.getSrc().getType());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  390:    // Since a pure dense expansion is very cheap (change of view), for
        -:  391:    // a sparse2dense or dense2sparse, we can simply unfuse a sparse
        -:  392:    // conversion from the reshape operation itself.
        -:  393:    // All other cases are handled elsewhere.
    #####:  394:    if (encDst && encSrc) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  395:      return failure();
        -:  396:    }
    #####:  397:    if (encSrc) {
branch  0 never executed
branch  1 never executed
    #####:  398:      RankedTensorType rtp =
    #####:  399:          op.getSrc().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  400:      auto denseTp =
    #####:  401:          RankedTensorType::get(rtp.getShape(), rtp.getElementType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  402:      auto convert = rewriter.create<ConvertOp>(loc, denseTp, op.getSrc());
call    0 never executed
call    1 never executed
    #####:  403:      op->setOperand(0, convert);
call    0 never executed
    #####:  404:      return success();
        -:  405:    }
    #####:  406:    if (encDst) {
branch  0 never executed
branch  1 never executed
    #####:  407:      RankedTensorType rtp =
    #####:  408:          op.getResult().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  409:      auto denseTp =
    #####:  410:          RankedTensorType::get(rtp.getShape(), rtp.getElementType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  411:      auto reshape = rewriter.create<ReshapeOp>(loc, denseTp, op.getSrc(),
call    0 never executed
call    1 never executed
call    2 never executed
        -:  412:                                                op.getReassociation());
    #####:  413:      Value convert = rewriter.create<ConvertOp>(loc, rtp, reshape);
call    0 never executed
call    1 never executed
    #####:  414:      rewriter.replaceOp(op, convert);
call    0 never executed
call    1 never executed
    #####:  415:      return success();
        -:  416:    }
    #####:  417:    return failure();
        -:  418:  }
------------------
_ZNK12_GLOBAL__N_115ReshapeRewriterIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_115ReshapeRewriterIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  385:  LogicalResult matchAndRewrite(ReshapeOp op,
        -:  386:                                PatternRewriter &rewriter) const override {
    #####:  387:    Location loc = op->getLoc();
call    0 never executed
    #####:  388:    auto encDst = getSparseTensorEncoding(op.getResult().getType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  389:    auto encSrc = getSparseTensorEncoding(op.getSrc().getType());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  390:    // Since a pure dense expansion is very cheap (change of view), for
        -:  391:    // a sparse2dense or dense2sparse, we can simply unfuse a sparse
        -:  392:    // conversion from the reshape operation itself.
        -:  393:    // All other cases are handled elsewhere.
    #####:  394:    if (encDst && encSrc) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  395:      return failure();
        -:  396:    }
    #####:  397:    if (encSrc) {
branch  0 never executed
branch  1 never executed
    #####:  398:      RankedTensorType rtp =
    #####:  399:          op.getSrc().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  400:      auto denseTp =
    #####:  401:          RankedTensorType::get(rtp.getShape(), rtp.getElementType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  402:      auto convert = rewriter.create<ConvertOp>(loc, denseTp, op.getSrc());
call    0 never executed
call    1 never executed
    #####:  403:      op->setOperand(0, convert);
call    0 never executed
    #####:  404:      return success();
        -:  405:    }
    #####:  406:    if (encDst) {
branch  0 never executed
branch  1 never executed
    #####:  407:      RankedTensorType rtp =
    #####:  408:          op.getResult().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  409:      auto denseTp =
    #####:  410:          RankedTensorType::get(rtp.getShape(), rtp.getElementType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  411:      auto reshape = rewriter.create<ReshapeOp>(loc, denseTp, op.getSrc(),
call    0 never executed
call    1 never executed
call    2 never executed
        -:  412:                                                op.getReassociation());
    #####:  413:      Value convert = rewriter.create<ConvertOp>(loc, rtp, reshape);
call    0 never executed
call    1 never executed
    #####:  414:      rewriter.replaceOp(op, convert);
call    0 never executed
call    1 never executed
    #####:  415:      return success();
        -:  416:    }
    #####:  417:    return failure();
        -:  418:  }
------------------
        -:  419:};
        -:  420:
        -:  421:struct ConcatenateRewriter : public OpRewritePattern<ConcatenateOp> {
        -:  422:  using OpRewritePattern::OpRewritePattern;
function _ZNK12_GLOBAL__N_119ConcatenateRewriter15matchAndRewriteEN4mlir13sparse_tensor13ConcatenateOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  423:  LogicalResult matchAndRewrite(ConcatenateOp op,
        -:  424:                                PatternRewriter &rewriter) const override {
    #####:  425:    auto loc = op.getLoc();
call    0 never executed
    #####:  426:    auto rtp = op.getType().cast<RankedTensorType>();
call    0 never executed
call    1 never executed
        -:  427:    // TODO: Build the output shape if needed.
    #####:  428:    assert(rtp.hasStaticShape());
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  429:    auto rank = rtp.getRank();
call    0 never executed
    #####:  430:    size_t conDim = op.getDimension().getZExtValue();
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  431:    // %t = concatenate %s1, %s2, %s3 {dim = 1}
        -:  432:    // ==>
        -:  433:    // %tmp = bufferization.alloc_tensor : unordered COO
        -:  434:    // foreach in %s1 : insert d0, d1, %tmp
        -:  435:    // foreach in %s2 : insert d0, d1 + size(s1), %tmp
        -:  436:    // foreach in %s3 : insert d0, d1 + size(s1) + size(s2), %tmp
        -:  437:    // %t = sparse_tensor.cast %tmp
    #####:  438:    auto cooTp = getUnorderedCOOFromType(rtp);
call    0 never executed
    #####:  439:    auto cooBuffer =
    #####:  440:        rewriter.create<AllocTensorOp>(loc, cooTp, ValueRange()).getResult();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  441:
    #####:  442:    Value offset = constantIndex(rewriter, loc, 0);
call    0 never executed
    #####:  443:    for (Value input : op.getInputs()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  444:      // Builds the indexing map.
        -:  445:
        -:  446:      // Build a for op for each input tensor to append new values into the
        -:  447:      // output tensor.
    #####:  448:      rewriter.create<ForeachOp>(
function _ZZNK12_GLOBAL__N_119ConcatenateRewriter15matchAndRewriteEN4mlir13sparse_tensor13ConcatenateOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES7_S8_S9_ called 0 returned 0% blocks executed 0%
    #####:  449:          loc, input, [&](OpBuilder &builder, Location loc, ValueRange args) {
    #####:  450:            SmallVector<Value, 4> indices;
    #####:  451:            for (int64_t i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
    #####:  452:              uint64_t dim =
    #####:  453:                  toStoredDim(getSparseTensorEncoding(input.getType()), i);
call    0 never executed
call    1 never executed
    #####:  454:              Value idx = args[dim];
call    0 never executed
    #####:  455:              if (i == static_cast<int64_t>(conDim))
branch  0 never executed
branch  1 never executed
        -:  456:                // transform coordinates on matching dim
    #####:  457:                idx = builder.create<arith::AddIOp>(loc, idx, offset);
call    0 never executed
    #####:  458:              indices.push_back(idx);
call    0 never executed
        -:  459:            }
    #####:  460:            builder.create<InsertOp>(loc, args.back(), cooBuffer, indices);
call    0 never executed
call    1 never executed
    #####:  461:            builder.create<sparse_tensor::YieldOp>(loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  462:          });
call    0 never executed
        -:  463:      // Accumulates the offset. Note that only static-shaped inputs are allowed
        -:  464:      // by concatenate op verifier, which saves us from computing the offset
        -:  465:      // dynamically.
    #####:  466:      auto d = input.getType().cast<RankedTensorType>().getShape()[conDim];
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  467:      assert(!ShapedType::isDynamic(d));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  468:      offset = rewriter.create<arith::AddIOp>(loc, offset,
    #####:  469:                                              constantIndex(rewriter, loc, d));
call    0 never executed
call    1 never executed
        -:  470:    }
    #####:  471:    rewriter.replaceOpWithNewOp<ConvertOp>(op, rtp, cooBuffer);
call    0 never executed
    #####:  472:    return success();
        -:  473:  }
        -:  474:};
        -:  475:
        -:  476:/// Sparse rewriting rule for the convert operator.
        -:  477:struct ConvertRewriter : public OpRewritePattern<ConvertOp> {
        -:  478:  using OpRewritePattern::OpRewritePattern;
function _ZNK12_GLOBAL__N_115ConvertRewriter15matchAndRewriteEN4mlir13sparse_tensor9ConvertOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  479:  LogicalResult matchAndRewrite(ConvertOp op,
        -:  480:                                PatternRewriter &rewriter) const override {
    #####:  481:    auto encDst = getSparseTensorEncoding(op.getType());
call    0 never executed
call    1 never executed
    #####:  482:    auto encSrc = getSparseTensorEncoding(op.getSource().getType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  483:    if (encDst && encSrc) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  484:      // Trivial tensor conversion is handled in codegen.
    #####:  485:      if (encSrc == encDst)
branch  0 never executed
branch  1 never executed
    #####:  486:        return failure();
    #####:  487:      return sparse2SparseRewrite(op, rewriter);
call    0 never executed
        -:  488:    }
    #####:  489:    if (encSrc && !encDst)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  490:      return sparse2DenseRewrite(op, rewriter);
call    0 never executed
    #####:  491:    if (!encSrc && encDst)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  492:      return dense2SparseRewrite(op, rewriter);
call    0 never executed
        -:  493:
        -:  494:    // Dense-to-dense convert is a nop and handled by canonicalization.
    #####:  495:    return failure();
        -:  496:  }
        -:  497:
        -:  498:private:
        -:  499:  // Handles sparse constant to sparse tensor or dense tensor to sparse tensor
        -:  500:  // conversion as follows:
        -:  501:  //   t = new sparse COO tensor
        -:  502:  //   fill t using src
        -:  503:  //   dst = convert t
        -:  504:  //
        -:  505:  // To fill the COO tensor from a dense tensor:
        -:  506:  //   for i1 in dim1
        -:  507:  //    ..
        -:  508:  //     for ik in dimk
        -:  509:  //       val = a[i1,..,ik]
        -:  510:  //       if val != 0
        -:  511:  //         t->add(val, [i1,..,ik], [p1,..,pk])
        -:  512:  //
        -:  513:  // To fill the COO tensor from a sparse constant in COO format:
        -:  514:  //   for i in range(NNZ)
        -:  515:  //     val = values[i]
        -:  516:  //     [i1,..,ik] = indices[i]
        -:  517:  //     t->add(val, [i1,..,ik], [p1,..,pk])
        -:  518:  LogicalResult dense2SparseRewrite(ConvertOp op,
        -:  519:                                    PatternRewriter &rewriter) const {
        -:  520:    Location loc = op.getLoc();
        -:  521:    Value src = op.getSource();
        -:  522:    RankedTensorType dstTp = op.getType().cast<RankedTensorType>();
        -:  523:    SmallVector<Value, 4> sizes;
        -:  524:    sizesFromSrc(rewriter, sizes, loc, src);
        -:  525:    SmallVector<Value, 4> dynSizes;
        -:  526:    getDynamicSizes(dstTp, sizes, dynSizes);
        -:  527:
        -:  528:    RankedTensorType cooTp = getUnorderedCOOFromType(dstTp);
        -:  529:    auto cooBuffer =
        -:  530:        rewriter.create<AllocTensorOp>(loc, cooTp, dynSizes).getResult();
        -:  531:    unsigned rank = dstTp.cast<ShapedType>().getRank();
        -:  532:
        -:  533:    genDenseTensorOrSparseConstantIterLoop(
        -:  534:        rewriter, loc, src, rank,
    #####:  535:        [&](OpBuilder &builder, Location loc, Value val, ValueRange indices) {
    #####:  536:          builder.create<InsertOp>(loc, val, cooBuffer, indices);
call    0 never executed
        -:  537:        });
        -:  538:
        -:  539:    rewriter.setInsertionPointAfter(op);
        -:  540:    rewriter.replaceOpWithNewOp<ConvertOp>(op, dstTp, cooBuffer);
        -:  541:    rewriter.create<DeallocTensorOp>(loc, cooBuffer);
        -:  542:
        -:  543:    return success();
        -:  544:  }
        -:  545:
        -:  546:  // Handles sparse tensor to dense tensor conversion as follows:
        -:  547:  //   dst = new dense tensor;
        -:  548:  //   foreach elemment in src
        -:  549:  //     dst[elemment.indices] = element.value
        -:  550:  LogicalResult sparse2DenseRewrite(ConvertOp op,
        -:  551:                                    PatternRewriter &rewriter) const {
        -:  552:    Location loc = op->getLoc();
        -:  553:    RankedTensorType dstTp = op.getType().cast<RankedTensorType>();
        -:  554:    Value src = op.getSource();
        -:  555:    RankedTensorType srcTp = src.getType().cast<RankedTensorType>();
        -:  556:
        -:  557:    SmallVector<Value, 4> sizes;
        -:  558:    sizesForTensor(rewriter, sizes, loc, srcTp, src);
        -:  559:    Value dst = allocDenseTensor(rewriter, loc, dstTp, sizes);
        -:  560:
        -:  561:    rewriter.create<ForeachOp>(
function _ZZNK12_GLOBAL__N_115ConvertRewriter19sparse2DenseRewriteEN4mlir13sparse_tensor9ConvertOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES7_S8_S9_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  562:        loc, src, [&](OpBuilder &builder, Location loc, ValueRange args) {
    #####:  563:          builder.create<memref::StoreOp>(loc, args.back(), dst,
branch  0 never executed
branch  1 never executed
    #####:  564:                                          args.drop_back());
call    0 never executed
call    1 never executed
    #####:  565:          builder.create<sparse_tensor::YieldOp>(loc);
call    0 never executed
    #####:  566:        });
        -:  567:
        -:  568:    rewriter.replaceOpWithNewOp<bufferization::ToTensorOp>(op, dstTp, dst);
        -:  569:    return success();
        -:  570:  }
        -:  571:
        -:  572:  // Handles sparse tensor to sparse tensor conversion as follows:
        -:  573:  //   if src is not COO
        -:  574:  //       construct a COO to represent the src
        -:  575:  //   sort the src COO
        -:  576:  //   foreach elemment in the sorted src COO
        -:  577:  //     insert element to dst
        -:  578:  LogicalResult sparse2SparseRewrite(ConvertOp op,
        -:  579:                                     PatternRewriter &rewriter) const {
        -:  580:    Location loc = op->getLoc();
        -:  581:    Value src = op.getSource();
        -:  582:    RankedTensorType srcTp = src.getType().cast<RankedTensorType>();
        -:  583:    RankedTensorType dstTp = op.getType().cast<RankedTensorType>();
        -:  584:    SparseTensorEncodingAttr encSrc = getSparseTensorEncoding(srcTp);
        -:  585:    SparseTensorEncodingAttr encDst = getSparseTensorEncoding(dstTp);
        -:  586:
        -:  587:    SmallVector<Value, 4> srcSizes;
        -:  588:    sizesForTensor(rewriter, srcSizes, loc, srcTp, src);
        -:  589:    Value tmpCoo = Value();
        -:  590:    if (!isUniqueCOOType(srcTp)) {
        -:  591:      // Construct a COO tensor from the src tensor.
        -:  592:      // TODO: there may be cases for which more efficiently without
        -:  593:      // going through an intermediate COO, such as cases that only change
        -:  594:      // the overhead types.
        -:  595:      SmallVector<Value, 4> dynSrcSizes;
        -:  596:      getDynamicSizes(srcTp, srcSizes, dynSrcSizes);
        -:  597:      srcTp = getUnorderedCOOFromType(srcTp);
        -:  598:      tmpCoo =
        -:  599:          rewriter.create<AllocTensorOp>(loc, srcTp, dynSrcSizes).getResult();
        -:  600:      rewriter.create<ForeachOp>(
function _ZZNK12_GLOBAL__N_115ConvertRewriter20sparse2SparseRewriteEN4mlir13sparse_tensor9ConvertOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES7_S8_S9_ called 0 returned 0% blocks executed 0%
    #####:  601:          loc, src, [&](OpBuilder &builder, Location loc, ValueRange args) {
    #####:  602:            SmallVector<Value, 4> indices;
call    0 never executed
    #####:  603:            for (int64_t i = 0, e = srcTp.getRank(); i < e; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  604:              uint64_t dim = toStoredDim(encSrc, i);
call    0 never executed
    #####:  605:              indices.push_back(args[dim]);
call    0 never executed
call    1 never executed
        -:  606:            }
    #####:  607:            builder.create<InsertOp>(loc, args.back(), tmpCoo, indices);
call    0 never executed
call    1 never executed
    #####:  608:            builder.create<sparse_tensor::YieldOp>(loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  609:          });
        -:  610:      src = tmpCoo;
        -:  611:    }
        -:  612:
        -:  613:    // Sort the COO tensor so that its elements are ordered via increasing
        -:  614:    // indices for the storage ordering of the dst tensor.
        -:  615:    auto dynShape = {ShapedType::kDynamicSize};
        -:  616:    auto indTp =
        -:  617:        MemRefType::get(dynShape, getIndexOverheadType(rewriter, encSrc));
        -:  618:    uint64_t rank = dstTp.getRank();
        -:  619:    // Gather the indices-arrays in the dst tensor storage order.
        -:  620:    SmallVector<Value, 4> xs(rank, Value());
        -:  621:    for (uint64_t i = 0; i < rank; i++) {
        -:  622:      uint64_t orgDim = toOrigDim(encSrc, i);
        -:  623:      xs[toStoredDim(encDst, orgDim)] = rewriter.create<ToIndicesOp>(
        -:  624:          loc, indTp, src, rewriter.getIndexAttr(orgDim));
        -:  625:    }
        -:  626:
        -:  627:    // Retrieve NNZ.
        -:  628:    auto ptrTp =
        -:  629:        MemRefType::get(dynShape, getPointerOverheadType(rewriter, encSrc));
        -:  630:    Value p0 =
        -:  631:        rewriter.create<ToIndicesOp>(loc, ptrTp, src, rewriter.getIndexAttr(0));
        -:  632:    Value c1 = constantIndex(rewriter, loc, 1);
        -:  633:    Value nnz = rewriter.create<memref::LoadOp>(loc, p0, c1);
        -:  634:    nnz =
        -:  635:        rewriter.create<arith::IndexCastOp>(loc, rewriter.getIndexType(), nnz);
        -:  636:
        -:  637:    // Retrieve the values-array.
        -:  638:    auto valTp = MemRefType::get(dynShape, srcTp.getElementType());
        -:  639:    Value y = rewriter.create<ToValuesOp>(loc, valTp, src);
        -:  640:
        -:  641:    // Sort the COO tensor.
        -:  642:    rewriter.create<SortOp>(loc, nnz, xs, ValueRange{y});
        -:  643:
        -:  644:    // For each element in the COO tensor, insert the element to the dst tensor.
        -:  645:    SmallVector<Value, 4> dynDstSizes;
        -:  646:    getDynamicSizes(dstTp, srcSizes, dynDstSizes);
        -:  647:    Value dst =
        -:  648:        rewriter.create<AllocTensorOp>(loc, dstTp, dynDstSizes).getResult();
        -:  649:    rewriter.create<ForeachOp>(
function _ZZNK12_GLOBAL__N_115ConvertRewriter20sparse2SparseRewriteEN4mlir13sparse_tensor9ConvertOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE0_clES7_S8_S9_ called 0 returned 0% blocks executed 0%
    #####:  650:        loc, src, [&](OpBuilder &builder, Location loc, ValueRange args) {
    #####:  651:          SmallVector<Value, 4> indices;
call    0 never executed
    #####:  652:          for (int64_t i = 0, e = srcTp.getRank(); i < e; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  653:            uint64_t dim = toStoredDim(encDst, i);
call    0 never executed
    #####:  654:            indices.push_back(args[dim]);
call    0 never executed
call    1 never executed
        -:  655:          }
    #####:  656:          builder.create<InsertOp>(loc, args.back(), dst, indices);
call    0 never executed
call    1 never executed
    #####:  657:          builder.create<sparse_tensor::YieldOp>(loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  658:        });
        -:  659:
        -:  660:    // Release the temporary COO if it is created.
        -:  661:    if (tmpCoo)
        -:  662:      rewriter.create<DeallocTensorOp>(loc, tmpCoo);
        -:  663:
        -:  664:    // Directly replace op with dst results in bufferization error message
        -:  665:    // "sparse tensor allocation should not escape function".
        -:  666:    // As such, we insert a trivial tensor convert which will be removed by
        -:  667:    // codegen.
        -:  668:    rewriter.setInsertionPointAfter(op);
        -:  669:    rewriter.replaceOpWithNewOp<ConvertOp>(op, dstTp, dst);
        -:  670:    return success();
        -:  671:  }
        -:  672:};
        -:  673:
        -:  674:/// Sparse rewriting rule for the foreach operator.
        -:  675:struct ForeachRewriter : public OpRewritePattern<ForeachOp> {
        -:  676:public:
        -:  677:  using OpRewritePattern::OpRewritePattern;
        -:  678:
function _ZNK12_GLOBAL__N_115ForeachRewriter15matchAndRewriteEN4mlir13sparse_tensor9ForeachOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  679:  LogicalResult matchAndRewrite(ForeachOp op,
        -:  680:                                PatternRewriter &rewriter) const override {
        -:  681:
    #####:  682:    auto loc = op.getLoc();
call    0 never executed
    #####:  683:    Value input = op.getTensor();
call    0 never executed
    #####:  684:    auto rtp = input.getType().cast<RankedTensorType>();
call    0 never executed
    #####:  685:    int64_t rank = rtp.getRank();
call    0 never executed
    #####:  686:    auto enc = getSparseTensorEncoding(rtp);
call    0 never executed
        -:  687:
        -:  688:    // 1. Generates loop for the sparse input.
    #####:  689:    SparseTensorLoopEmitter loopEmitter(ValueRange{input});
call    0 never executed
call    1 never executed
    #####:  690:    loopEmitter.initializeLoopEmit(rewriter, loc);
call    0 never executed
    #####:  691:    for (int64_t i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
        -:  692:      // TODO: provide utility function for loop sequences that only contains
        -:  693:      // one for loop?
    #####:  694:      loopEmitter.enterNewLoopSeq(rewriter, loc, 0, static_cast<size_t>(i));
call    0 never executed
    #####:  695:      loopEmitter.enterLoopOverTensorAtDim(rewriter, loc, 0, i);
call    0 never executed
        -:  696:    }
        -:  697:
    #####:  698:    SmallVector<Value, 4> coords;
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  699:    coords.reserve(rank);
branch  0 never executed
branch  1 never executed
    #####:  700:    loopEmitter.getCoordinateArray(coords);
        -:  701:
    #####:  702:    Value vals = loopEmitter.getValBuffer()[0];
branch  0 never executed
branch  1 never executed
    #####:  703:    Value pidx = loopEmitter.getPidxs()[0].back();
branch  0 never executed
branch  1 never executed
        -:  704:    // Loads the value from sparse tensor using pointer index;
        -:  705:    // loads the value from dense tensor using coordinate array.
    #####:  706:    Value val = enc ? rewriter.create<memref::LoadOp>(loc, vals, pidx)
call    0 never executed
    #####:  707:                    : rewriter.create<memref::LoadOp>(loc, vals, coords);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  708:
        -:  709:    // 2. Inline the block in the foreach operator.
    #####:  710:    Block::iterator inlinePos = rewriter.getInsertionPoint();
call    0 never executed
    #####:  711:    Block *srcBlock = op.getBody();
call    0 never executed
        -:  712:    // Remove sparse_tensor.yield.
    #####:  713:    rewriter.eraseOp(srcBlock->getTerminator());
call    0 never executed
        -:  714:
    #####:  715:    for (int64_t i = 0; i < rank; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  716:      loopEmitter.exitCurrentLoop(rewriter, loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  717:      loopEmitter.exitCurrentLoopSeq();
call    0 never executed
        -:  718:    }
        -:  719:
    #####:  720:    SmallVector<Value, 4> args;
branch  0 never executed
branch  1 never executed
        -:  721:    // Remap coordinates.
    #####:  722:    for (int64_t i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
    #####:  723:      Value actual = coords[toOrigDim(enc, i)];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  724:      args.push_back(actual);
call    0 never executed
        -:  725:    }
        -:  726:    // Remap value.
    #####:  727:    args.push_back(val);
call    0 never executed
        -:  728:
        -:  729:    // Inline body.
    #####:  730:    rewriter.mergeBlockBefore(srcBlock, &*inlinePos, args);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  731:    // delete the foreach operator.
    #####:  732:    rewriter.eraseOp(op);
call    0 never executed
    #####:  733:    return success();
branch  0 never executed
branch  1 never executed
        -:  734:  }
        -:  735:};
        -:  736:
        -:  737:/// Sparse rewriting rule for the new operator.
        -:  738:struct NewRewriter : public OpRewritePattern<NewOp> {
        -:  739:  using OpRewritePattern::OpRewritePattern;
function _ZNK12_GLOBAL__N_111NewRewriter15matchAndRewriteEN4mlir13sparse_tensor5NewOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  740:  LogicalResult matchAndRewrite(NewOp op,
        -:  741:                                PatternRewriter &rewriter) const override {
    #####:  742:    Location loc = op.getLoc();
call    0 never executed
    #####:  743:    auto dstTp = op.getResult().getType().template cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  744:    SparseTensorEncodingAttr encDst = getSparseTensorEncoding(dstTp);
call    0 never executed
    #####:  745:    if (!encDst) {
branch  0 never executed
branch  1 never executed
    #####:  746:      return failure();
        -:  747:    }
        -:  748:
        -:  749:    // Create a sparse tensor reader.
    #####:  750:    Value fileName = op.getSource();
call    0 never executed
    #####:  751:    Type opaqueTp = getOpaquePointerType(rewriter);
call    0 never executed
    #####:  752:    Value reader = createFuncCall(rewriter, loc, "createSparseTensorReader",
    #####:  753:                                  {opaqueTp}, {fileName}, EmitCInterface::Off)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  754:                       .getResult(0);
        -:  755:
        -:  756:    // Allocate a temporary buffer for storing dimension sizes and indices.
    #####:  757:    Type indexTp = rewriter.getIndexType();
call    0 never executed
    #####:  758:    uint64_t rank = dstTp.getRank();
call    0 never executed
    #####:  759:    Value dimSizes = genAlloca(rewriter, loc, rank, indexTp);
call    0 never executed
        -:  760:
        -:  761:    // If the result tensor has dynamic dimensions, get the dynamic sizes from
        -:  762:    // the sparse tensor reader.
    #####:  763:    SmallVector<Value, 4> dynSizesArray;
call    0 never executed
    #####:  764:    if (!dstTp.hasStaticShape()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  765:      createFuncCall(rewriter, loc, "getSparseTensorReaderDimSizes", {},
    #####:  766:                     {reader, dimSizes}, EmitCInterface::On)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  767:          .getResult(0);
    #####:  768:      ArrayRef<int64_t> dstShape = dstTp.getShape();
call    0 never executed
    #####:  769:      for (auto &d : llvm::enumerate(dstShape)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  770:        if (d.value() == ShapedType::kDynamicSize) {
branch  0 never executed
branch  1 never executed
    #####:  771:          dynSizesArray.push_back(rewriter.create<memref::LoadOp>(
call    0 never executed
    #####:  772:              loc, dimSizes, constantIndex(rewriter, loc, d.index())));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  773:        }
        -:  774:      }
        -:  775:    }
        -:  776:
        -:  777:    // Implement the NewOp as follows:
        -:  778:    //   %tmp = bufferization.alloc_tensor : an unordered COO with identity
        -:  779:    //                                       storage ordering
        -:  780:    //   for i = 0 to nnz
        -:  781:    //     get the next element from the input file
        -:  782:    //     insert the element to %tmp
        -:  783:    //   %t = sparse_tensor.ConvertOp %tmp
    #####:  784:    RankedTensorType cooTp = getUnorderedCOOFromType(dstTp);
call    0 never executed
    #####:  785:    auto cooBuffer =
    #####:  786:        rewriter.create<AllocTensorOp>(loc, cooTp, dynSizesArray).getResult();
call    0 never executed
call    1 never executed
        -:  787:
    #####:  788:    Value c0 = constantIndex(rewriter, loc, 0);
call    0 never executed
    #####:  789:    Value c1 = constantIndex(rewriter, loc, 1);
call    0 never executed
    #####:  790:    Value nnz = createFuncCall(rewriter, loc, "getSparseTensorReaderNNZ",
    #####:  791:                               {indexTp}, {reader}, EmitCInterface::Off)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  792:                    .getResult(0);
    #####:  793:    Type eltTp = dstTp.getElementType();
call    0 never executed
    #####:  794:    Value value = genAllocaScalar(rewriter, loc, eltTp);
call    0 never executed
    #####:  795:    scf::ForOp forOp = rewriter.create<scf::ForOp>(loc, c0, nnz, c1);
call    0 never executed
    #####:  796:    rewriter.setInsertionPointToStart(forOp.getBody());
call    0 never executed
call    1 never executed
        -:  797:
    #####:  798:    SmallString<18> getNextFuncName{"getSparseTensorReaderNext",
call    0 never executed
    #####:  799:                                    primaryTypeFunctionSuffix(eltTp)};
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  800:    Value indices = dimSizes; // Reuse the indices memref to store indices.
    #####:  801:    createFuncCall(rewriter, loc, getNextFuncName, {eltTp},
    #####:  802:                   {reader, indices, value}, EmitCInterface::On)
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  803:        .getResult(0);
    #####:  804:    SmallVector<Value, 4> indicesArray;
branch  0 never executed
branch  1 never executed
    #####:  805:    for (uint64_t i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
    #####:  806:      indicesArray.push_back(rewriter.create<memref::LoadOp>(
call    0 never executed
    #####:  807:          loc, indices, constantIndex(rewriter, loc, i)));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  808:    }
    #####:  809:    Value v = rewriter.create<memref::LoadOp>(loc, value);
call    0 never executed
call    1 never executed
    #####:  810:    rewriter.create<InsertOp>(loc, v, cooBuffer, indicesArray);
call    0 never executed
    #####:  811:    rewriter.setInsertionPointAfter(forOp);
call    0 never executed
        -:  812:
        -:  813:    // Release the sparse tensor reader.
    #####:  814:    createFuncCall(rewriter, loc, "delSparseTensorReader", {}, {reader},
    #####:  815:                   EmitCInterface::Off);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  816:
    #####:  817:    Value newOp = rewriter.replaceOpWithNewOp<ConvertOp>(op, dstTp, cooBuffer);
call    0 never executed
call    1 never executed
        -:  818:
        -:  819:    // Release the unordered COO tensor buffer.
    #####:  820:    rewriter.setInsertionPointAfterValue(newOp);
call    0 never executed
    #####:  821:    rewriter.create<DeallocTensorOp>(loc, cooBuffer);
call    0 never executed
        -:  822:
    #####:  823:    return success();
branch  0 never executed
branch  1 never executed
        -:  824:  }
        -:  825:};
        -:  826:
        -:  827:struct OutRewriter : public OpRewritePattern<OutOp> {
        -:  828:  using OpRewritePattern::OpRewritePattern;
function _ZNK12_GLOBAL__N_111OutRewriter15matchAndRewriteEN4mlir13sparse_tensor5OutOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  829:  LogicalResult matchAndRewrite(OutOp op,
        -:  830:                                PatternRewriter &rewriter) const override {
    #####:  831:    Location loc = op.getLoc();
call    0 never executed
        -:  832:    // Calculate NNZ.
    #####:  833:    Value src = op.getTensor();
call    0 never executed
    #####:  834:    Value nnz = rewriter.create<NumberOfEntriesOp>(loc, src);
call    0 never executed
call    1 never executed
        -:  835:
        -:  836:    // Allocate a temporary buffer for storing dimension sizes and indices.
    #####:  837:    auto srcTp = src.getType().template cast<RankedTensorType>();
call    0 never executed
    #####:  838:    uint64_t rank = srcTp.getRank();
call    0 never executed
    #####:  839:    Type indexTp = rewriter.getIndexType();
call    0 never executed
    #####:  840:    Value dimSizes = genAlloca(rewriter, loc, rank, indexTp);
call    0 never executed
        -:  841:
        -:  842:    // Generate code to calculate dimension size values and store the values to
        -:  843:    // the buffer.
    #####:  844:    SmallVector<Value, 4> dims;
call    0 never executed
    #####:  845:    sizesForTensor(rewriter, dims, loc, srcTp, src);
call    0 never executed
    #####:  846:    for (uint64_t i = 0; i < rank; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  847:      rewriter.create<memref::StoreOp>(loc, dims[i], dimSizes,
    #####:  848:                                       constantIndex(rewriter, loc, i));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  849:    }
        -:  850:
        -:  851:    // Create a sparse tensor writer and output meta data.
    #####:  852:    Type opaqueTp = getOpaquePointerType(rewriter);
call    0 never executed
    #####:  853:    Value writer =
    #####:  854:        createFuncCall(rewriter, loc, "createSparseTensorWriter", {opaqueTp},
    #####:  855:                       {op.getDest()}, EmitCInterface::Off)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  856:            .getResult(0);
    #####:  857:    Value rankValue = constantIndex(rewriter, loc, rank);
call    0 never executed
    #####:  858:    createFuncCall(rewriter, loc, "outSparseTensorWriterMetaData", {},
    #####:  859:                   {writer, rankValue, nnz, dimSizes}, EmitCInterface::On);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  860:
    #####:  861:    Value indices = dimSizes; // Reuse the dimSizes buffer for indices.
    #####:  862:    Type eltTp = srcTp.getElementType();
call    0 never executed
    #####:  863:    SmallString<18> outNextFuncName{"outSparseTensorWriterNext",
call    0 never executed
    #####:  864:                                    primaryTypeFunctionSuffix(eltTp)};
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  865:    Value value = genAllocaScalar(rewriter, loc, eltTp);
call    0 never executed
    #####:  866:    ModuleOp module = op->getParentOfType<ModuleOp>();
call    0 never executed
        -:  867:    // For each element in the source tensor, output the element.
    #####:  868:    rewriter.create<ForeachOp>(
function _ZZNK12_GLOBAL__N_111OutRewriter15matchAndRewriteEN4mlir13sparse_tensor5OutOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES7_S8_S9_ called 0 returned 0% blocks executed 0%
    #####:  869:        loc, src, [&](OpBuilder &builder, Location loc, ValueRange args) {
    #####:  870:          for (uint64_t i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
    #####:  871:            rewriter.create<memref::StoreOp>(loc, args[i], indices,
    #####:  872:                                             constantIndex(builder, loc, i));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  873:          }
    #####:  874:          rewriter.create<memref::StoreOp>(loc, args.back(), value);
call    0 never executed
call    1 never executed
    #####:  875:          SmallVector<Value, 4> operands{writer, rankValue, indices, value};
call    0 never executed
    #####:  876:          FlatSymbolRefAttr fn = getFunc(module, outNextFuncName, {}, operands,
call    0 never executed
call    1 never executed
    #####:  877:                                         EmitCInterface::On);
call    0 never executed
call    1 never executed
    #####:  878:          builder.create<func::CallOp>(loc, TypeRange(), fn, operands);
call    0 never executed
call    1 never executed
    #####:  879:          builder.create<sparse_tensor::YieldOp>(loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  880:        });
call    0 never executed
        -:  881:
        -:  882:    // Release the writer.
    #####:  883:    createFuncCall(rewriter, loc, "delSparseTensorWriter", {}, {writer},
    #####:  884:                   EmitCInterface::Off);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  885:
    #####:  886:    rewriter.eraseOp(op);
call    0 never executed
    #####:  887:    return success();
branch  0 never executed
branch  1 never executed
        -:  888:  }
        -:  889:};
        -:  890:
        -:  891:} // namespace
        -:  892:
        -:  893://===---------------------------------------------------------------------===//
        -:  894:// Methods that add patterns described in this file to a pattern list.
        -:  895://===---------------------------------------------------------------------===//
function _ZN4mlir29populateSparseTensorRewritingERNS_17RewritePatternSetEbbb called 507 returned 100% blocks executed 60%
      507:  896:void mlir::populateSparseTensorRewriting(RewritePatternSet &patterns,
        -:  897:                                         bool enableRT, bool enableForeach,
        -:  898:                                         bool enableConvert) {
      507:  899:  patterns.add<FoldInvariantYield, FuseSparseMultiplyOverAdd,
        -:  900:               ReshapeRewriter<tensor::ExpandShapeOp>,
      507:  901:               ReshapeRewriter<tensor::CollapseShapeOp>>(patterns.getContext());
call    0 returned 100%
      507:  902:  if (enableForeach)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
      507:  903:    patterns.add<ForeachRewriter>(patterns.getContext());
call    0 returned 100%
        -:  904:  // TODO: If RT not enabled, rewrite concatenate ops, etc here.
      507:  905:  if (!enableRT) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  906:    patterns.add<ConcatenateRewriter, NewRewriter, OutRewriter,
        -:  907:                 Sparse2SparseReshapeRewriter<tensor::ExpandShapeOp>,
        -:  908:                 Sparse2SparseReshapeRewriter<tensor::CollapseShapeOp>>(
    #####:  909:        patterns.getContext());
call    0 never executed
    #####:  910:    if (enableConvert)
branch  0 never executed
branch  1 never executed
    #####:  911:      patterns.add<ConvertRewriter>(patterns.getContext());
call    0 never executed
        -:  912:  }
      507:  913:}
