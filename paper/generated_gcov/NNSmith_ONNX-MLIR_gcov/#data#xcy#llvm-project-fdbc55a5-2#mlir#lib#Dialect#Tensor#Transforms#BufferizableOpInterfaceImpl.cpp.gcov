        -:    0:Source:/data/xcy/llvm-project-fdbc55a5-2/mlir/lib/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Tensor/Transforms/CMakeFiles/obj.MLIRTensorTransforms.dir/BufferizableOpInterfaceImpl.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Tensor/Transforms/CMakeFiles/obj.MLIRTensorTransforms.dir/BufferizableOpInterfaceImpl.cpp.gcda
        -:    0:Runs:128633
        -:    1://===- BufferizableOpInterfaceImpl.cpp - Impl. of BufferizableOpInterface -===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8:
        -:    9:#include "mlir/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.h"
        -:   10:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   11:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   12:#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
        -:   13:#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
        -:   14:#include "mlir/Dialect/Bufferization/IR/DstBufferizableOpInterfaceImpl.h"
        -:   15:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   16:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   17:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   18:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   19:#include "mlir/Dialect/Utils/StaticValueUtils.h"
        -:   20:#include "mlir/IR/Dialect.h"
        -:   21:#include "mlir/IR/Operation.h"
        -:   22:
        -:   23:using namespace mlir;
        -:   24:using namespace mlir::bufferization;
        -:   25:using namespace mlir::tensor;
        -:   26:
        -:   27:namespace mlir {
        -:   28:namespace tensor {
        -:   29:namespace {
        -:   30:
    20474:   31:struct CastOpInterface
call    0 returned 100%
        -:   32:    : public BufferizableOpInterface::ExternalModel<CastOpInterface,
        -:   33:                                                    tensor::CastOp> {
    #####:   34:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:   35:                              const AnalysisState &state) const {
    #####:   36:    return false;
        -:   37:  }
        -:   38:
    #####:   39:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:   40:                               const AnalysisState &state) const {
    #####:   41:    return false;
        -:   42:  }
        -:   43:
    #####:   44:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:   45:                                            const AnalysisState &state) const {
    #####:   46:    return {op->getResult(0)};
call    0 never executed
        -:   47:  }
        -:   48:
    #####:   49:  BufferRelation bufferRelation(Operation *op, OpResult opResult,
        -:   50:                                const AnalysisState &state) const {
    #####:   51:    return BufferRelation::Equivalent;
        -:   52:  }
        -:   53:
        -:   54:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:   55:                          const BufferizationOptions &options) const {
        -:   56:    auto castOp = cast<tensor::CastOp>(op);
        -:   57:
        -:   58:    // The result buffer still has the old (pre-cast) type.
        -:   59:    FailureOr<Value> resultBuffer =
        -:   60:        getBuffer(rewriter, castOp.getSource(), options);
        -:   61:    if (failed(resultBuffer))
        -:   62:      return failure();
        -:   63:    auto sourceMemRefType = resultBuffer->getType().cast<BaseMemRefType>();
        -:   64:    TensorType resultTensorType =
        -:   65:        castOp.getResult().getType().cast<TensorType>();
        -:   66:    MemRefLayoutAttrInterface layout;
        -:   67:
        -:   68:    if (auto rankedMemRefType = sourceMemRefType.dyn_cast<MemRefType>())
        -:   69:      if (resultTensorType.isa<RankedTensorType>())
        -:   70:        layout = rankedMemRefType.getLayout();
        -:   71:
        -:   72:    // Compute the new memref type.
        -:   73:    Type resultMemRefType =
        -:   74:        getMemRefType(castOp.getResult(), options, layout,
        -:   75:                      sourceMemRefType.getMemorySpaceAsInt());
        -:   76:
        -:   77:    // Replace the op with a memref.cast.
        -:   78:    assert(memref::CastOp::areCastCompatible(resultBuffer->getType(),
        -:   79:                                             resultMemRefType) &&
        -:   80:           "CallOp::bufferize: cast incompatible");
        -:   81:    replaceOpWithNewBufferizedOp<memref::CastOp>(rewriter, op, resultMemRefType,
        -:   82:                                                 *resultBuffer);
        -:   83:
        -:   84:    return success();
        -:   85:  }
        -:   86:};
        -:   87:
        -:   88:/// Bufferization of tensor.collapse_shape. Replace with memref.collapse_shape.
    20474:   89:struct CollapseShapeOpInterface
call    0 returned 100%
        -:   90:    : public BufferizableOpInterface::ExternalModel<CollapseShapeOpInterface,
        -:   91:                                                    tensor::CollapseShapeOp> {
    #####:   92:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:   93:                              const AnalysisState &state) const {
    #####:   94:    return false;
        -:   95:  }
        -:   96:
    #####:   97:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:   98:                               const AnalysisState &state) const {
    #####:   99:    return false;
        -:  100:  }
        -:  101:
        -:  102:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  103:                                            const AnalysisState &state) const {
        -:  104:    if (&opOperand == &op->getOpOperand(0) /*src*/)
        -:  105:      return {op->getOpResult(0)};
        -:  106:    return {};
        -:  107:  }
        -:  108:
    #####:  109:  BufferRelation bufferRelation(Operation *op, OpResult opResult,
        -:  110:                                const AnalysisState &state) const {
    #####:  111:    return BufferRelation::Equivalent;
        -:  112:  }
        -:  113:
        -:  114:  FailureOr<BaseMemRefType>
        -:  115:  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
        -:  116:                const DenseMap<Value, BaseMemRefType> &fixedTypes) const {
        -:  117:    auto collapseShapeOp = cast<tensor::CollapseShapeOp>(op);
        -:  118:    auto maybeSrcBufferType = bufferization::getBufferType(
        -:  119:        collapseShapeOp.getSrc(), options, fixedTypes);
        -:  120:    if (failed(maybeSrcBufferType))
        -:  121:      return failure();
        -:  122:    auto srcBufferType = maybeSrcBufferType->cast<MemRefType>();
        -:  123:    bool canBeCollapsed = memref::CollapseShapeOp::isGuaranteedCollapsible(
        -:  124:        srcBufferType, collapseShapeOp.getReassociationIndices());
        -:  125:
        -:  126:    if (!canBeCollapsed) {
        -:  127:      // If dims cannot be collapsed, this op bufferizes to a new allocation.
        -:  128:      RankedTensorType tensorResultType = collapseShapeOp.getResultType();
        -:  129:      return bufferization::getMemRefTypeWithStaticIdentityLayout(
        -:  130:          tensorResultType, srcBufferType.getMemorySpaceAsInt());
        -:  131:    }
        -:  132:
        -:  133:    return memref::CollapseShapeOp::computeCollapsedType(
        -:  134:        srcBufferType, collapseShapeOp.getReassociationIndices());
        -:  135:  }
        -:  136:
        -:  137:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  138:                          const BufferizationOptions &options) const {
        -:  139:    auto collapseShapeOp = cast<tensor::CollapseShapeOp>(op);
        -:  140:    RankedTensorType tensorResultType = collapseShapeOp.getResultType();
        -:  141:    FailureOr<Value> maybeBuffer =
        -:  142:        getBuffer(rewriter, collapseShapeOp.getSrc(), options);
        -:  143:    if (failed(maybeBuffer))
        -:  144:      return failure();
        -:  145:    Value buffer = *maybeBuffer;
        -:  146:    auto bufferType = buffer.getType().cast<MemRefType>();
        -:  147:
        -:  148:    if (tensorResultType.getRank() == 0) {
        -:  149:      // 0-d collapses must go through a different op builder.
        -:  150:      MemRefType resultType;
        -:  151:
        -:  152:      if (bufferType.getLayout().isIdentity()) {
        -:  153:        // Standard layout: result type has no offset.
        -:  154:        MemRefLayoutAttrInterface layout;
        -:  155:        resultType = MemRefType::get({}, tensorResultType.getElementType(),
        -:  156:                                     layout, bufferType.getMemorySpace());
        -:  157:      } else {
        -:  158:        // Source memref has a layout map: result type has the same offset as
        -:  159:        // the source type.
        -:  160:        SmallVector<int64_t> strides;
        -:  161:        int64_t offset;
        -:  162:        if (failed(getStridesAndOffset(bufferType, strides, offset)))
        -:  163:          return failure();
        -:  164:        resultType = MemRefType::get(
        -:  165:            {}, tensorResultType.getElementType(),
        -:  166:            StridedLayoutAttr::get(op->getContext(), offset, {}),
        -:  167:            bufferType.getMemorySpace());
        -:  168:      }
        -:  169:
        -:  170:      replaceOpWithNewBufferizedOp<memref::CollapseShapeOp>(
        -:  171:          rewriter, op, resultType, buffer, collapseShapeOp.getReassociation());
        -:  172:      return success();
        -:  173:    }
        -:  174:
        -:  175:    // If the dims are not collapsible (due to an incompatible source layout
        -:  176:    // map), force an out-of-place bufferization, i.e., a buffer copy. This
        -:  177:    // newly allocated buffer will have no layout map and thus be collapsible.
        -:  178:    bool canBeCollapsed = memref::CollapseShapeOp::isGuaranteedCollapsible(
        -:  179:        bufferType, collapseShapeOp.getReassociationIndices());
        -:  180:    if (!canBeCollapsed) {
        -:  181:      // TODO: Create alloc_tensor ops during TensorCopyInsertion.
        -:  182:      AnalysisState analysisState(options);
        -:  183:      FailureOr<Value> tensorAlloc = allocateTensorForShapedValue(
        -:  184:          rewriter, op->getLoc(), collapseShapeOp.getSrc(),
        -:  185:          analysisState.isTensorYielded(collapseShapeOp.getResult()), options);
        -:  186:      if (failed(tensorAlloc))
        -:  187:        return failure();
        -:  188:      auto memrefType =
        -:  189:          MemRefType::get(collapseShapeOp.getSrcType().getShape(),
        -:  190:                          collapseShapeOp.getSrcType().getElementType(),
        -:  191:                          AffineMap(), bufferType.getMemorySpaceAsInt());
        -:  192:      buffer = rewriter.create<bufferization::ToMemrefOp>(
        -:  193:          op->getLoc(), memrefType, *tensorAlloc);
        -:  194:    }
        -:  195:
        -:  196:    // Result type is inferred by the builder.
        -:  197:    replaceOpWithNewBufferizedOp<memref::CollapseShapeOp>(
        -:  198:        rewriter, op, buffer, collapseShapeOp.getReassociationIndices());
        -:  199:    return success();
        -:  200:  }
        -:  201:};
        -:  202:
        -:  203:/// Bufferization of tensor.dim. Replace with memref.dim.
    20474:  204:struct DimOpInterface
call    0 returned 100%
        -:  205:    : public BufferizableOpInterface::ExternalModel<DimOpInterface,
        -:  206:                                                    tensor::DimOp> {
    #####:  207:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  208:                              const AnalysisState &state) const {
    #####:  209:    return true;
        -:  210:  }
        -:  211:
    #####:  212:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  213:                               const AnalysisState &state) const {
    #####:  214:    return false;
        -:  215:  }
        -:  216:
    #####:  217:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  218:                                            const AnalysisState &state) const {
    #####:  219:    return {};
        -:  220:  }
        -:  221:
        -:  222:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  223:                          const BufferizationOptions &options) const {
        -:  224:    auto dimOp = cast<tensor::DimOp>(op);
        -:  225:    FailureOr<Value> v = getBuffer(rewriter, dimOp.getSource(), options);
        -:  226:    if (failed(v))
        -:  227:      return failure();
        -:  228:    replaceOpWithNewBufferizedOp<memref::DimOp>(rewriter, op, *v,
        -:  229:                                                dimOp.getIndex());
        -:  230:    return success();
        -:  231:  }
        -:  232:};
        -:  233:
        -:  234:/// Bufferization of tensor.expand_shape. Replace with memref.expand_shape.
    20474:  235:struct ExpandShapeOpInterface
call    0 returned 100%
        -:  236:    : public BufferizableOpInterface::ExternalModel<ExpandShapeOpInterface,
        -:  237:                                                    tensor::ExpandShapeOp> {
    #####:  238:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  239:                              const AnalysisState &state) const {
    #####:  240:    return false;
        -:  241:  }
        -:  242:
    #####:  243:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  244:                               const AnalysisState &state) const {
    #####:  245:    return false;
        -:  246:  }
        -:  247:
        -:  248:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  249:                                            const AnalysisState &state) const {
        -:  250:    if (&opOperand == &op->getOpOperand(0) /*src*/)
        -:  251:      return {op->getOpResult(0)};
        -:  252:    return {};
        -:  253:  }
        -:  254:
    #####:  255:  BufferRelation bufferRelation(Operation *op, OpResult opResult,
        -:  256:                                const AnalysisState &state) const {
    #####:  257:    return BufferRelation::Equivalent;
        -:  258:  }
        -:  259:
        -:  260:  FailureOr<BaseMemRefType>
        -:  261:  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
        -:  262:                const DenseMap<Value, BaseMemRefType> &fixedTypes) const {
        -:  263:    auto expandShapeOp = cast<tensor::ExpandShapeOp>(op);
        -:  264:    auto maybeSrcBufferType = bufferization::getBufferType(
        -:  265:        expandShapeOp.getSrc(), options, fixedTypes);
        -:  266:    if (failed(maybeSrcBufferType))
        -:  267:      return failure();
        -:  268:    auto srcBufferType = maybeSrcBufferType->cast<MemRefType>();
        -:  269:    auto maybeResultType = memref::ExpandShapeOp::computeExpandedType(
        -:  270:        srcBufferType, expandShapeOp.getResultType().getShape(),
        -:  271:        expandShapeOp.getReassociationIndices());
        -:  272:    if (failed(maybeResultType))
        -:  273:      return failure();
        -:  274:    return *maybeResultType;
        -:  275:  }
        -:  276:
        -:  277:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  278:                          const BufferizationOptions &options) const {
        -:  279:    auto expandShapeOp = cast<tensor::ExpandShapeOp>(op);
        -:  280:    auto tensorResultType = expandShapeOp.getResultType();
        -:  281:    FailureOr<Value> buffer =
        -:  282:        getBuffer(rewriter, expandShapeOp.getSrc(), options);
        -:  283:    if (failed(buffer))
        -:  284:      return failure();
        -:  285:
        -:  286:    // Memref result type is inferred by the builder based on reassociation
        -:  287:    // indices and result shape.
        -:  288:    replaceOpWithNewBufferizedOp<memref::ExpandShapeOp>(
        -:  289:        rewriter, op, tensorResultType.getShape(), *buffer,
        -:  290:        expandShapeOp.getReassociationIndices());
        -:  291:    return success();
        -:  292:  }
        -:  293:};
        -:  294:
        -:  295:/// Bufferization of tensor.extract_slice. Replace with memref.subview.
    20474:  296:struct ExtractSliceOpInterface
call    0 returned 100%
        -:  297:    : public BufferizableOpInterface::ExternalModel<ExtractSliceOpInterface,
        -:  298:                                                    tensor::ExtractSliceOp> {
    #####:  299:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  300:                              const AnalysisState &state) const {
    #####:  301:    return false;
        -:  302:  }
        -:  303:
    #####:  304:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  305:                               const AnalysisState &state) const {
    #####:  306:    return false;
        -:  307:  }
        -:  308:
        -:  309:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  310:                                            const AnalysisState &state) const {
        -:  311:    if (&opOperand == &op->getOpOperand(0) /*source*/)
        -:  312:      return {op->getOpResult(0)};
        -:  313:    return {};
        -:  314:  }
        -:  315:
    #####:  316:  BufferRelation bufferRelation(Operation *op, OpResult opResult,
        -:  317:                                const AnalysisState &state) const {
    #####:  318:    return BufferRelation::None;
        -:  319:  }
        -:  320:
        -:  321:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  322:                          const BufferizationOptions &options) const {
        -:  323:    auto extractSliceOp = cast<tensor::ExtractSliceOp>(op);
        -:  324:    SmallVector<OpFoldResult> mixedOffsets = extractSliceOp.getMixedOffsets();
        -:  325:    SmallVector<OpFoldResult> mixedSizes = extractSliceOp.getMixedSizes();
        -:  326:    SmallVector<OpFoldResult> mixedStrides = extractSliceOp.getMixedStrides();
        -:  327:    Location loc = extractSliceOp.getLoc();
        -:  328:
        -:  329:    // Get source buffer.
        -:  330:    FailureOr<Value> srcMemref =
        -:  331:        getBuffer(rewriter, extractSliceOp.getSource(), options);
        -:  332:    if (failed(srcMemref))
        -:  333:      return failure();
        -:  334:
        -:  335:    // Take a subview of the source buffer.
        -:  336:    auto resultMemrefType =
        -:  337:        bufferization::getBufferType(extractSliceOp.getResult(), options);
        -:  338:    if (failed(resultMemrefType))
        -:  339:      return failure();
        -:  340:    Value subView = rewriter.create<memref::SubViewOp>(
        -:  341:        loc, resultMemrefType->cast<MemRefType>(), *srcMemref, mixedOffsets,
        -:  342:        mixedSizes, mixedStrides);
        -:  343:
        -:  344:    replaceOpWithBufferizedValues(rewriter, op, subView);
        -:  345:    return success();
        -:  346:  }
        -:  347:
        -:  348:  FailureOr<BaseMemRefType>
        -:  349:  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
        -:  350:                const DenseMap<Value, BaseMemRefType> &fixedTypes) const {
        -:  351:    auto extractSliceOp = cast<tensor::ExtractSliceOp>(op);
        -:  352:    assert(value == extractSliceOp.getResult() && "invalid value");
        -:  353:    auto srcMemrefType = bufferization::getBufferType(
        -:  354:        extractSliceOp.getSource(), options, fixedTypes);
        -:  355:    if (failed(srcMemrefType))
        -:  356:      return failure();
        -:  357:    SmallVector<OpFoldResult> mixedOffsets = extractSliceOp.getMixedOffsets();
        -:  358:    SmallVector<OpFoldResult> mixedSizes = extractSliceOp.getMixedSizes();
        -:  359:    SmallVector<OpFoldResult> mixedStrides = extractSliceOp.getMixedStrides();
        -:  360:    return memref::SubViewOp::inferRankReducedResultType(
        -:  361:               extractSliceOp.getType().getShape(),
        -:  362:               srcMemrefType->cast<MemRefType>(), mixedOffsets, mixedSizes,
        -:  363:               mixedStrides)
        -:  364:        .cast<BaseMemRefType>();
        -:  365:  }
        -:  366:};
        -:  367:
        -:  368:/// Bufferization of tensor.extract. Replace with memref.load.
    20474:  369:struct ExtractOpInterface
call    0 returned 100%
        -:  370:    : public BufferizableOpInterface::ExternalModel<ExtractOpInterface,
        -:  371:                                                    tensor::ExtractOp> {
    #####:  372:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  373:                              const AnalysisState &state) const {
    #####:  374:    return true;
        -:  375:  }
        -:  376:
    #####:  377:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  378:                               const AnalysisState &state) const {
    #####:  379:    return false;
        -:  380:  }
        -:  381:
    #####:  382:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  383:                                            const AnalysisState &state) const {
    #####:  384:    return {};
        -:  385:  }
        -:  386:
        -:  387:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  388:                          const BufferizationOptions &options) const {
        -:  389:    auto extractOp = cast<tensor::ExtractOp>(op);
        -:  390:    FailureOr<Value> srcMemref =
        -:  391:        getBuffer(rewriter, extractOp.getTensor(), options);
        -:  392:    if (failed(srcMemref))
        -:  393:      return failure();
        -:  394:    replaceOpWithNewBufferizedOp<memref::LoadOp>(rewriter, op, *srcMemref,
        -:  395:                                                 extractOp.getIndices());
        -:  396:    return success();
        -:  397:  }
        -:  398:};
        -:  399:
        -:  400:// Implements backtracking to traverse indices of the output buffer while
        -:  401:// iterating over op.elements().
function _ZN4mlir6tensor12_GLOBAL__N_1L12createStoresERNS_12RewriterBaseENS_8LocationEiNS_5ValueEN4llvm8ArrayRefIlEENS7_IS5_EERNS6_6detail27indexed_accessor_range_baseINS_12OperandRangeEPNS_9OpOperandES5_S5_S5_E8iteratorERNS6_15SmallVectorImplIS5_EE called 0 returned 0% blocks executed 0%
    #####:  402:static void createStores(RewriterBase &rewriter, Location loc, int dim,
        -:  403:                         Value buffer, ArrayRef<int64_t> shape,
        -:  404:                         ArrayRef<Value> constants,
        -:  405:                         OperandRange::iterator &elementIt,
        -:  406:                         SmallVectorImpl<Value> &indices) {
    #####:  407:  if (dim == static_cast<int>(shape.size()) - 1) {
branch  0 never executed
branch  1 never executed
    #####:  408:    for (int i = 0; i < shape.back(); ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  409:      indices.back() = constants[i];
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  410:      rewriter.create<memref::StoreOp>(loc, *elementIt, buffer, indices);
call    0 never executed
    #####:  411:      ++elementIt;
        -:  412:    }
        -:  413:    return;
        -:  414:  }
    #####:  415:  for (int i = 0; i < shape[dim]; ++i) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  416:    indices[dim] = constants[i];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  417:    createStores(rewriter, loc, dim + 1, buffer, shape, constants, elementIt,
call    0 never executed
        -:  418:                 indices);
        -:  419:  }
        -:  420:}
        -:  421:
        -:  422:/// Bufferization of tensor.from_elements.
    20474:  423:struct FromElementsOpInterface
call    0 returned 100%
        -:  424:    : public BufferizableOpInterface::ExternalModel<FromElementsOpInterface,
        -:  425:                                                    tensor::FromElementsOp> {
        -:  426:
    #####:  427:  bool bufferizesToAllocation(Operation *op, OpResult opResult) const {
    #####:  428:    return true;
        -:  429:  }
        -:  430:
        -:  431:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  432:                          const BufferizationOptions &options) const {
        -:  433:    auto fromElementsOp = cast<tensor::FromElementsOp>(op);
        -:  434:    // Should the buffer be deallocated?
        -:  435:    bool dealloc = shouldDeallocateOpResult(
        -:  436:        fromElementsOp.getResult().cast<OpResult>(), options);
        -:  437:
        -:  438:    // TODO: Implement memory space for this op.
        -:  439:    if (options.defaultMemorySpace != static_cast<unsigned>(0))
        -:  440:      return op->emitError("memory space not implemented yet");
        -:  441:
        -:  442:    // Allocate a buffer for the result.
        -:  443:    Location loc = op->getLoc();
        -:  444:    auto tensorType = fromElementsOp.getType().cast<RankedTensorType>();
        -:  445:    auto shape = tensorType.getShape();
        -:  446:    // TODO: Create alloc_tensor ops during TensorCopyInsertion.
        -:  447:    FailureOr<Value> tensorAlloc =
        -:  448:        allocateTensorForShapedValue(rewriter, loc, fromElementsOp.getResult(),
        -:  449:                                     /*escape=*/!dealloc, options,
        -:  450:                                     /*copy=*/false);
        -:  451:    if (failed(tensorAlloc))
        -:  452:      return failure();
        -:  453:    auto memrefType =
        -:  454:        MemRefType::get(tensorType.getShape(), tensorType.getElementType());
        -:  455:    Value buffer = rewriter.create<bufferization::ToMemrefOp>(
        -:  456:        op->getLoc(), memrefType, *tensorAlloc);
        -:  457:
        -:  458:    // Case: tensor<0xelem_type>.
        -:  459:    if (fromElementsOp.getElements().empty()) {
        -:  460:      replaceOpWithBufferizedValues(rewriter, op, buffer);
        -:  461:      return success();
        -:  462:    }
        -:  463:
        -:  464:    // Case: tensor<elem_type>.
        -:  465:    if (shape.empty()) {
        -:  466:      rewriter.create<memref::StoreOp>(
        -:  467:          loc, fromElementsOp.getElements().front(), buffer);
        -:  468:      replaceOpWithBufferizedValues(rewriter, op, buffer);
        -:  469:      return success();
        -:  470:    }
        -:  471:
        -:  472:    // Create constants for the range of possible indices [0, max{shape_i}).
        -:  473:    auto maxDim = *std::max_element(shape.begin(), shape.end());
        -:  474:    SmallVector<Value, 2> constants;
        -:  475:    constants.reserve(maxDim);
        -:  476:    for (int i = 0; i < maxDim; ++i)
        -:  477:      constants.push_back(rewriter.create<arith::ConstantIndexOp>(loc, i));
        -:  478:
        -:  479:    // Traverse all `elements` and create `memref.store` ops.
        -:  480:    auto elementIt = fromElementsOp.getElements().begin();
        -:  481:    SmallVector<Value, 2> indices(tensorType.getRank(), constants[0]);
        -:  482:    createStores(rewriter, loc, /*dim=*/0, buffer, shape, constants, elementIt,
        -:  483:                 indices);
        -:  484:
        -:  485:    replaceOpWithBufferizedValues(rewriter, op, buffer);
        -:  486:
        -:  487:    return success();
        -:  488:  }
        -:  489:};
        -:  490:
        -:  491:/// Lower the body of a tensor.generate like op (one index-typed bbArg per dim).
        -:  492:/// Such ops are lowered to linalg.map with the given tensor as a destination.
        -:  493:///
        -:  494:/// Example:
        -:  495:/// ```
        -:  496:/// %r = tensor.generate %x, %y {
        -:  497:///   ^bb0(%arg0: index, %arg1: index):
        -:  498:///   %0 = "some_op"(%arg0, %arg1) : (index, index) -> (index)
        -:  499:///   tensor.yield %0 : index
        -:  500:/// } : tensor<?x?xindex>
        -:  501:/// ```
        -:  502:///
        -:  503:/// Is lowered to:
        -:  504:/// ```
        -:  505:/// linalg.map ins() outs(%dest) {
        -:  506:///   %d0 = linalg.index 0 : index
        -:  507:///   %d1 = linalg.index 1 : index
        -:  508:///   %0 = "some_op"(%d0, %d1) : (index, index) -> (index)
        -:  509:///   linalg.yield %0 : index
        -:  510:/// }
        -:  511:/// ```
        -:  512:static Value lowerGenerateLikeOpBody(RewriterBase &rewriter, Location loc,
        -:  513:                                     Value tensorDestination,
        -:  514:                                     ValueRange dynamicSizes,
        -:  515:                                     Region &generateBody) {
        -:  516:  assert(generateBody.hasOneBlock() && "expected body with single block");
        -:  517:  auto tensorType = tensorDestination.getType().cast<RankedTensorType>();
        -:  518:  assert(generateBody.getNumArguments() == tensorType.getRank() &&
        -:  519:         "rank mismatch");
        -:  520:
        -:  521:  // Create linalg::MapOp.
        -:  522:  OpBuilder::InsertionGuard g(rewriter);
        -:  523:  auto linalgOp =
        -:  524:      rewriter.create<linalg::MapOp>(loc, tensorType, /*inputs=*/ValueRange(),
        -:  525:                                     /*init=*/tensorDestination);
        -:  526:  Block &linalgBody = linalgOp.getMapper().emplaceBlock();
        -:  527:
        -:  528:  // Create linalg::IndexOps.
        -:  529:  rewriter.setInsertionPointToStart(&linalgBody);
        -:  530:  SmallVector<Value> indices;
        -:  531:  for (int64_t dim = 0; dim < tensorType.getRank(); ++dim)
        -:  532:    indices.push_back(rewriter.create<linalg::IndexOp>(loc, dim));
        -:  533:
        -:  534:  // Move over body.
        -:  535:  rewriter.mergeBlocks(&generateBody.front(), &linalgBody, indices);
        -:  536:  auto yieldOp = cast<tensor::YieldOp>(linalgBody.getTerminator());
        -:  537:  rewriter.replaceOpWithNewOp<linalg::YieldOp>(yieldOp, yieldOp.getValue());
        -:  538:
        -:  539:  return linalgOp.getResult()[0];
        -:  540:}
        -:  541:
        -:  542:/// Bufferization of tensor.generate.
    20474:  543:struct GenerateOpInterface
call    0 returned 100%
        -:  544:    : public BufferizableOpInterface::ExternalModel<GenerateOpInterface,
        -:  545:                                                    tensor::GenerateOp> {
        -:  546:
    #####:  547:  bool bufferizesToAllocation(Operation *op, OpResult opResult) const {
    #####:  548:    return true;
        -:  549:  }
        -:  550:
        -:  551:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  552:                          const BufferizationOptions &options) const {
        -:  553:    auto generateOp = cast<tensor::GenerateOp>(op);
        -:  554:    // Should the buffer be deallocated?
        -:  555:    bool dealloc = shouldDeallocateOpResult(
        -:  556:        generateOp.getResult().cast<OpResult>(), options);
        -:  557:
        -:  558:    // TODO: Implement memory space for this op.
        -:  559:    if (options.defaultMemorySpace != static_cast<unsigned>(0))
        -:  560:      return op->emitError("memory space not implemented yet");
        -:  561:
        -:  562:    // Allocate memory.
        -:  563:    Location loc = op->getLoc();
        -:  564:    FailureOr<Value> tensorAlloc =
        -:  565:        allocateTensorForShapedValue(rewriter, loc, generateOp.getResult(),
        -:  566:                                     /*escape=*/!dealloc, options,
        -:  567:                                     /*copy=*/false);
        -:  568:    if (failed(tensorAlloc))
        -:  569:      return failure();
        -:  570:
        -:  571:    Value result = lowerGenerateLikeOpBody(rewriter, loc, *tensorAlloc,
        -:  572:                                           generateOp.getDynamicExtents(),
        -:  573:                                           generateOp.getBody());
        -:  574:    rewriter.replaceOp(generateOp, result);
        -:  575:
        -:  576:    return success();
        -:  577:  }
        -:  578:};
        -:  579:
        -:  580:/// Bufferization of tensor.insert. Replace with memref.store.
        -:  581:///
        -:  582:/// Note: DstBufferizableOpInterfaceExternalModel provides many default method
        -:  583:/// implementations for DestinationStyle ops.
    20474:  584:struct InsertOpInterface
call    0 returned 100%
        -:  585:    : public DstBufferizableOpInterfaceExternalModel<InsertOpInterface,
        -:  586:                                                     tensor::InsertOp> {
        -:  587:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  588:                          const BufferizationOptions &options) const {
        -:  589:    auto insertOp = cast<tensor::InsertOp>(op);
        -:  590:    FailureOr<Value> destMemref =
        -:  591:        getBuffer(rewriter, insertOp.getDest(), options);
        -:  592:    if (failed(destMemref))
        -:  593:      return failure();
        -:  594:    rewriter.create<memref::StoreOp>(insertOp.getLoc(), insertOp.getScalar(),
        -:  595:                                     *destMemref, insertOp.getIndices());
        -:  596:    replaceOpWithBufferizedValues(rewriter, op, *destMemref);
        -:  597:    return success();
        -:  598:  }
        -:  599:};
        -:  600:
        -:  601:/// Return true if the (ExtractSliceOp, InsertSliceOp) pair match (i.e.
        -:  602:/// equivalent operand / result and same offset/sizes/strides specification).
        -:  603:template <typename OpTy>
    #####:  604:static bool areEquivalentSlices(const AnalysisState &state,
        -:  605:                                ExtractSliceOp extractSliceOp,
        -:  606:                                OpTy insertSliceOp) {
    #####:  607:  if (!extractSliceOp || !insertSliceOp)
        -:  608:    return false;
    #####:  609:  if (extractSliceOp != insertSliceOp &&
    #####:  610:      !state.areEquivalentBufferizedValues(extractSliceOp.getSource(),
        -:  611:                                           insertSliceOp.getDest()))
    #####:  612:    return false;
    #####:  613:  if (!sameOffsetsSizesAndStrides(extractSliceOp, insertSliceOp,
        -:  614:                                  isEqualConstantIntOrValue))
    #####:  615:    return false;
        -:  616:  return true;
        -:  617:}
------------------
_ZN4mlir6tensor12_GLOBAL__N_1L19areEquivalentSlicesINS0_21ParallelInsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS0_14ExtractSliceOpET_:
function _ZN4mlir6tensor12_GLOBAL__N_1L19areEquivalentSlicesINS0_21ParallelInsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS0_14ExtractSliceOpET_ called 0 returned 0% blocks executed 0%
    #####:  604:static bool areEquivalentSlices(const AnalysisState &state,
branch  0 never executed
branch  1 never executed
        -:  605:                                ExtractSliceOp extractSliceOp,
        -:  606:                                OpTy insertSliceOp) {
    #####:  607:  if (!extractSliceOp || !insertSliceOp)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  608:    return false;
    #####:  609:  if (extractSliceOp != insertSliceOp &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  610:      !state.areEquivalentBufferizedValues(extractSliceOp.getSource(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  611:                                           insertSliceOp.getDest()))
    #####:  612:    return false;
    #####:  613:  if (!sameOffsetsSizesAndStrides(extractSliceOp, insertSliceOp,
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  614:                                  isEqualConstantIntOrValue))
    #####:  615:    return false;
        -:  616:  return true;
        -:  617:}
------------------
_ZN4mlir6tensor12_GLOBAL__N_1L19areEquivalentSlicesINS0_13InsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS0_14ExtractSliceOpET_:
function _ZN4mlir6tensor12_GLOBAL__N_1L19areEquivalentSlicesINS0_13InsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS0_14ExtractSliceOpET_ called 0 returned 0% blocks executed 0%
    #####:  604:static bool areEquivalentSlices(const AnalysisState &state,
branch  0 never executed
branch  1 never executed
        -:  605:                                ExtractSliceOp extractSliceOp,
        -:  606:                                OpTy insertSliceOp) {
    #####:  607:  if (!extractSliceOp || !insertSliceOp)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  608:    return false;
    #####:  609:  if (extractSliceOp != insertSliceOp &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  610:      !state.areEquivalentBufferizedValues(extractSliceOp.getSource(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  611:                                           insertSliceOp.getDest()))
    #####:  612:    return false;
    #####:  613:  if (!sameOffsetsSizesAndStrides(extractSliceOp, insertSliceOp,
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  614:                                  isEqualConstantIntOrValue))
    #####:  615:    return false;
        -:  616:  return true;
        -:  617:}
------------------
        -:  618:
        -:  619:/// Return true if `value` is originating from the InsertSliceOp's destination
        -:  620:/// or an ExtractSliceOp that matches the given InsertSliceOp.
        -:  621:template <typename OpTy>
    #####:  622:static bool matchesInsertDestination(const AnalysisState &state, Value value,
        -:  623:                                     OpTy insertSliceOp) {
        -:  624:  // Look for matching slices.
    #####:  625:  auto matchesSlice = [&](Value val) {
    #####:  626:    if (auto extractSliceOp = val.getDefiningOp<ExtractSliceOp>())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  627:      if (areEquivalentSlices(state, extractSliceOp, insertSliceOp))
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  628:        return true;
    #####:  629:    return false;
        -:  630:  };
    #####:  631:  if (llvm::all_of(state.findValueInReverseUseDefChain(value, matchesSlice),
        -:  632:                   matchesSlice))
        -:  633:    return true;
        -:  634:
        -:  635:  // Look for equivalent values.
    #####:  636:  auto isEquivalent = [&](Value val) {
    #####:  637:    return state.areEquivalentBufferizedValues(val, insertSliceOp.getDest());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
        -:  638:  };
    #####:  639:  if (llvm::all_of(state.findValueInReverseUseDefChain(
        -:  640:                       value, isEquivalent, /*followEquivalentOnly=*/true),
        -:  641:                   isEquivalent))
    #####:  642:    return true;
        -:  643:  return false;
        -:  644:}
------------------
_ZN4mlir6tensor12_GLOBAL__N_1L24matchesInsertDestinationINS0_21ParallelInsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS_5ValueET_:
function _ZN4mlir6tensor12_GLOBAL__N_1L24matchesInsertDestinationINS0_21ParallelInsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS_5ValueET_ called 0 returned 0% blocks executed 0%
    #####:  622:static bool matchesInsertDestination(const AnalysisState &state, Value value,
        -:  623:                                     OpTy insertSliceOp) {
        -:  624:  // Look for matching slices.
    #####:  625:  auto matchesSlice = [&](Value val) {
        -:  626:    if (auto extractSliceOp = val.getDefiningOp<ExtractSliceOp>())
        -:  627:      if (areEquivalentSlices(state, extractSliceOp, insertSliceOp))
        -:  628:        return true;
        -:  629:    return false;
        -:  630:  };
    #####:  631:  if (llvm::all_of(state.findValueInReverseUseDefChain(value, matchesSlice),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  632:                   matchesSlice))
        -:  633:    return true;
        -:  634:
        -:  635:  // Look for equivalent values.
    #####:  636:  auto isEquivalent = [&](Value val) {
        -:  637:    return state.areEquivalentBufferizedValues(val, insertSliceOp.getDest());
        -:  638:  };
    #####:  639:  if (llvm::all_of(state.findValueInReverseUseDefChain(
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  640:                       value, isEquivalent, /*followEquivalentOnly=*/true),
        -:  641:                   isEquivalent))
    #####:  642:    return true;
        -:  643:  return false;
        -:  644:}
------------------
_ZN4mlir6tensor12_GLOBAL__N_1L24matchesInsertDestinationINS0_13InsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS_5ValueET_:
function _ZN4mlir6tensor12_GLOBAL__N_1L24matchesInsertDestinationINS0_13InsertSliceOpEEEbRKNS_13bufferization13AnalysisStateENS_5ValueET_ called 0 returned 0% blocks executed 0%
    #####:  622:static bool matchesInsertDestination(const AnalysisState &state, Value value,
        -:  623:                                     OpTy insertSliceOp) {
        -:  624:  // Look for matching slices.
    #####:  625:  auto matchesSlice = [&](Value val) {
        -:  626:    if (auto extractSliceOp = val.getDefiningOp<ExtractSliceOp>())
        -:  627:      if (areEquivalentSlices(state, extractSliceOp, insertSliceOp))
        -:  628:        return true;
        -:  629:    return false;
        -:  630:  };
    #####:  631:  if (llvm::all_of(state.findValueInReverseUseDefChain(value, matchesSlice),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  632:                   matchesSlice))
        -:  633:    return true;
        -:  634:
        -:  635:  // Look for equivalent values.
    #####:  636:  auto isEquivalent = [&](Value val) {
        -:  637:    return state.areEquivalentBufferizedValues(val, insertSliceOp.getDest());
        -:  638:  };
    #####:  639:  if (llvm::all_of(state.findValueInReverseUseDefChain(
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  640:                       value, isEquivalent, /*followEquivalentOnly=*/true),
        -:  641:                   isEquivalent))
    #####:  642:    return true;
        -:  643:  return false;
        -:  644:}
------------------
        -:  645:
        -:  646:template <typename OpTy>
        -:  647:static bool isNotConflictingInsertSliceLikeOp(Operation *op, OpOperand *uRead,
        -:  648:                                              OpOperand *uConflictingWrite,
        -:  649:                                              const AnalysisState &state) {
        -:  650:  Operation *readingOp = uRead->getOwner();
        -:  651:  Operation *conflictingWritingOp = uConflictingWrite->getOwner();
        -:  652:
        -:  653:  // Special rules for matching ExtractSliceOp/InsertSliceOp pairs. If
        -:  654:  // uRead is an InsertSliceOp...
        -:  655:  if (auto insertSliceOp = dyn_cast<OpTy>(readingOp)) {
        -:  656:    // As an example, consider the following IR.
        -:  657:    //
        -:  658:    // %0 = tensor.extract_slice %t[%a, %b][%c, %d][1, 1] {inplace = [true] }
        -:  659:    // %1 = linalg.fill %cst, %0 {inplace= [true] }
        -:  660:    // %2 = tensor.insert_slice %1 into %t[%a, %b][%c, %d][1, 1]
        -:  661:    //     {inplace= [true] }
        -:  662:
        -:  663:    // TODO: Use insertSliceOp.getDestOpOperand etc. when available.
        -:  664:    if (uRead == &insertSliceOp->getOpOperand(1) /*dest*/ &&
        -:  665:        matchesInsertDestination(state, uConflictingWrite->get(),
        -:  666:                                 insertSliceOp))
        -:  667:      // Case 1: The main insight is that InsertSliceOp reads only part of
        -:  668:      // the destination tensor. The overwritten area is not read. If
        -:  669:      // uConflictingWrite writes into exactly the memory location that is
        -:  670:      // being read by uRead, this is not a conflict.
        -:  671:      //
        -:  672:      // In the above example:
        -:  673:      // uRead             = OpOperand 1 (%t) of tensor.insert_slice
        -:  674:      // uConflictingWrite = OpOperand 1 (%0) of linalg.fill
        -:  675:      //
        -:  676:      // The read of %t does not conflict with the write of the FillOp
        -:  677:      // (same aliases!) because the area that the FillOp operates on is
        -:  678:      // exactly the one that is *not* read via %t.
        -:  679:      return true;
        -:  680:
        -:  681:    if (uRead == &insertSliceOp->getOpOperand(0) /*source*/ &&
        -:  682:        uConflictingWrite == &insertSliceOp->getOpOperand(1) /*dest*/ &&
        -:  683:        matchesInsertDestination(state, uRead->get(), insertSliceOp))
        -:  684:      // Case 2: The read of the source tensor and the write to the dest
        -:  685:      // tensor via an InsertSliceOp is not a conflict if the read is
        -:  686:      // reading exactly that part of an equivalent tensor that the
        -:  687:      // InsertSliceOp is writing.
        -:  688:      //
        -:  689:      // In the above example:
        -:  690:      // uRead             = OpOperand 0 (%1) of tensor.insert_slice
        -:  691:      // uConflictingWrite = OpOperand 1 (%t) of tensor.insert_slice
        -:  692:      return true;
        -:  693:  }
        -:  694:
        -:  695:  // If uConflictingWrite is an InsertSliceOp...
        -:  696:  if (auto insertSliceOp = dyn_cast<OpTy>(conflictingWritingOp))
        -:  697:    // As an example, consider the following IR.
        -:  698:    //
        -:  699:    // %0 = tensor.extract_slice %t[%a, %b][%c, %d][1, 1] {inplace = [true] }
        -:  700:    // %1 = linalg.fill %cst, %0 {inplace= [true] }
        -:  701:    // %2 = tensor.insert_slice %1 into %t[%a, %b][%c, %d][1, 1]
        -:  702:    //     {inplace= [true] }
        -:  703:    // %3 = vector.transfer_read %1, %cst
        -:  704:    //
        -:  705:    // In the above example:
        -:  706:    // uRead             = OpOperand 0 (%1) of vector.transfer_read
        -:  707:    // uConflictingWrite = OpOperand 1 (%t) of tensor.insert_slice
        -:  708:    // lastWrite         = %1
        -:  709:    //
        -:  710:    // This is not a conflict because the InsertSliceOp overwrites the
        -:  711:    // memory segment of %1 with the exact same data. (Effectively, there
        -:  712:    // is no memory write here.)
        -:  713:    if (uConflictingWrite == &insertSliceOp->getOpOperand(1) /*dest*/ &&
        -:  714:        state.areEquivalentBufferizedValues(uRead->get(),
        -:  715:                                            insertSliceOp.getSource()) &&
        -:  716:        matchesInsertDestination(state, insertSliceOp.getSource(),
        -:  717:                                 insertSliceOp))
        -:  718:      return true;
        -:  719:
        -:  720:  return false;
        -:  721:}
        -:  722:
        -:  723:/// Bufferization of tensor.insert_slice. Replace with a memory copy. Under
        -:  724:/// certain circumstances, this op can also be a no-op.
        -:  725:///
        -:  726:/// Note: DstBufferizableOpInterfaceExternalModel provides many default method
        -:  727:/// implementations for DestinationStyle ops.
    20474:  728:struct InsertSliceOpInterface
call    0 returned 100%
        -:  729:    : public DstBufferizableOpInterfaceExternalModel<InsertSliceOpInterface,
        -:  730:                                                     tensor::InsertSliceOp> {
    #####:  731:  bool isNotConflicting(Operation *op, OpOperand *uRead,
        -:  732:                        OpOperand *uConflictingWrite,
        -:  733:                        const AnalysisState &state) const {
    #####:  734:    return isNotConflictingInsertSliceLikeOp<tensor::InsertSliceOp>(
call    0 never executed
        -:  735:        op, uRead, uConflictingWrite, state);
        -:  736:  }
        -:  737:
        -:  738:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  739:                          const BufferizationOptions &options) const {
        -:  740:    // insert_slice ops arise from tiling and bufferizing them out-of-place is
        -:  741:    // generally a deal breaker. When used with loops, this ends up cloning the
        -:  742:    // whole tensor on every single iteration and is a symptom of a
        -:  743:    // catastrophically bad scheduling decision.
        -:  744:    // TODO: be very loud about it or even consider failing the pass.
        -:  745:    auto insertSliceOp = cast<tensor::InsertSliceOp>(op);
        -:  746:    SmallVector<OpFoldResult> mixedOffsets = insertSliceOp.getMixedOffsets();
        -:  747:    SmallVector<OpFoldResult> mixedSizes = insertSliceOp.getMixedSizes();
        -:  748:    SmallVector<OpFoldResult> mixedStrides = insertSliceOp.getMixedStrides();
        -:  749:    Location loc = insertSliceOp.getLoc();
        -:  750:
        -:  751:    // Get destination buffer.
        -:  752:    FailureOr<Value> dstMemref =
        -:  753:        getBuffer(rewriter, insertSliceOp.getDest(), options);
        -:  754:    if (failed(dstMemref))
        -:  755:      return failure();
        -:  756:
        -:  757:    // Take a subview of the destination buffer.
        -:  758:    auto dstMemrefType = dstMemref->getType().cast<MemRefType>();
        -:  759:    auto subviewMemRefType =
        -:  760:        memref::SubViewOp::inferRankReducedResultType(
        -:  761:            insertSliceOp.getSourceType().getShape(), dstMemrefType,
        -:  762:            mixedOffsets, mixedSizes, mixedStrides)
        -:  763:            .cast<MemRefType>();
        -:  764:    Value subView = rewriter.create<memref::SubViewOp>(
        -:  765:        loc, subviewMemRefType, *dstMemref, mixedOffsets, mixedSizes,
        -:  766:        mixedStrides);
        -:  767:
        -:  768:    // Copy tensor. If this tensor.insert_slice has a matching
        -:  769:    // tensor.extract_slice, the copy operation will eventually fold away.
        -:  770:    FailureOr<Value> srcMemref =
        -:  771:        getBuffer(rewriter, insertSliceOp.getSource(), options);
        -:  772:    if (failed(srcMemref))
        -:  773:      return failure();
        -:  774:    if (failed(options.createMemCpy(rewriter, loc, *srcMemref, subView)))
        -:  775:      return failure();
        -:  776:
        -:  777:    replaceOpWithBufferizedValues(rewriter, op, *dstMemref);
        -:  778:    return success();
        -:  779:  }
        -:  780:};
        -:  781:
        -:  782:/// Bufferization of tensor.pad. Replace with bufferization.alloc_tensor +
        -:  783:/// linalg.map + insert_slice.
        -:  784:/// For best performance, vectorize before bufferization (better performance in
        -:  785:/// case of padding with a constant).
    20474:  786:struct PadOpInterface
call    0 returned 100%
        -:  787:    : public BufferizableOpInterface::ExternalModel<PadOpInterface,
        -:  788:                                                    tensor::PadOp> {
    #####:  789:  bool bufferizesToAllocation(Operation *op, OpResult opResult) const {
    #####:  790:    return true;
        -:  791:  }
        -:  792:
    #####:  793:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  794:                              const AnalysisState &state) const {
    #####:  795:    return true;
        -:  796:  }
        -:  797:
    #####:  798:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  799:                               const AnalysisState &state) const {
    #####:  800:    return false;
        -:  801:  }
        -:  802:
    #####:  803:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  804:                                            const AnalysisState &state) const {
    #####:  805:    return {};
        -:  806:  }
        -:  807:
        -:  808:  FailureOr<BaseMemRefType>
        -:  809:  getBufferType(Operation *op, Value value, const BufferizationOptions &options,
        -:  810:                const DenseMap<Value, BaseMemRefType> &fixedTypes) const {
        -:  811:    // Infer memory space from the source tensor.
        -:  812:    auto padOp = cast<tensor::PadOp>(op);
        -:  813:    auto maybeSrcBufferType =
        -:  814:        bufferization::getBufferType(padOp.getSource(), options, fixedTypes);
        -:  815:    if (failed(maybeSrcBufferType))
        -:  816:      return failure();
        -:  817:    MemRefLayoutAttrInterface layout;
        -:  818:    return MemRefType::get(padOp.getResultType().getShape(),
        -:  819:                           padOp.getResultType().getElementType(), layout,
        -:  820:                           maybeSrcBufferType->getMemorySpace());
        -:  821:  }
        -:  822:
        -:  823:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  824:                          const BufferizationOptions &options) const {
        -:  825:    auto padOp = cast<tensor::PadOp>(op);
        -:  826:    Location loc = padOp.getLoc();
        -:  827:    RankedTensorType resultType = padOp.getResultType();
        -:  828:    RankedTensorType srcType = padOp.getSourceType();
        -:  829:
function _ZZNK4mlir6tensor12_GLOBAL__N_114PadOpInterface9bufferizeEPNS_9OperationERNS_12RewriterBaseERKNS_13bufferization20BufferizationOptionsEENKUlNS_12OpFoldResultEE_clESB_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  830:    auto toValue = [&](OpFoldResult ofr) {
    #####:  831:      if (ofr.is<Value>())
branch  0 never executed
branch  1 never executed
    #####:  832:        return ofr.get<Value>();
call    0 never executed
    #####:  833:      return rewriter
    #####:  834:          .create<arith::ConstantIndexOp>(loc, *getConstantIntValue(ofr))
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  835:          .getResult();
call    0 never executed
        -:  836:    };
        -:  837:
        -:  838:    // Compute dynamic result dimensions.
        -:  839:    SmallVector<OpFoldResult> mixedLowPad = padOp.getMixedLowPad();
        -:  840:    SmallVector<OpFoldResult> mixedHighPad = padOp.getMixedHighPad();
        -:  841:    SmallVector<Value> dynamicSizes;
        -:  842:    for (int64_t i = 0; i < resultType.getRank(); ++i) {
        -:  843:      if (!resultType.isDynamicDim(i))
        -:  844:        continue;
        -:  845:      Value srcDim = rewriter.create<tensor::DimOp>(loc, padOp.getSource(), i);
        -:  846:      Value lowPad = toValue(mixedLowPad[i]);
        -:  847:      Value highPad = toValue(mixedHighPad[i]);
        -:  848:      AffineExpr s0, s1, s2;
        -:  849:      bindSymbols(op->getContext(), s0, s1, s2);
        -:  850:      AffineExpr sumExpr = s0 + s1 + s2;
        -:  851:      Value sum = rewriter.create<AffineApplyOp>(
        -:  852:          loc, sumExpr, ValueRange{srcDim, lowPad, highPad});
        -:  853:      dynamicSizes.push_back(sum);
        -:  854:    }
        -:  855:
        -:  856:    // Should the buffer be deallocated?
        -:  857:    bool dealloc =
        -:  858:        shouldDeallocateOpResult(padOp.getResult().cast<OpResult>(), options);
        -:  859:    // Allocate a buffer for the padded result.
        -:  860:    FailureOr<Value> tensorAlloc =
        -:  861:        allocateTensorForShapedValue(rewriter, loc, padOp.getResult(),
        -:  862:                                     /*escape=*/!dealloc, options,
        -:  863:                                     /*copy=*/false);
        -:  864:    if (failed(tensorAlloc))
        -:  865:      return failure();
        -:  866:
        -:  867:    // tensor::PadOp is like tensor::GenerateOp: The only difference is that
        -:  868:    // only a part of the generated tensor is needed. For simplicity, we reuse
        -:  869:    // the same functionality here.
        -:  870:    Value filledBuffer = lowerGenerateLikeOpBody(
        -:  871:        rewriter, loc, *tensorAlloc, dynamicSizes, padOp.getBodyRegion());
        -:  872:
        -:  873:    // Create tensor::InsertSliceOp.
        -:  874:    SmallVector<OpFoldResult> sliceSizes =
        -:  875:        getMixedSizes(rewriter, loc, padOp.getSource());
        -:  876:    SmallVector<OpFoldResult> sliceStrides(srcType.getRank(),
        -:  877:                                           rewriter.getIndexAttr(1));
        -:  878:    rewriter.replaceOpWithNewOp<tensor::InsertSliceOp>(
        -:  879:        padOp, padOp.getSource(), filledBuffer,
        -:  880:        /*offsets=*/padOp.getMixedLowPad(), sliceSizes, sliceStrides);
        -:  881:
        -:  882:    return success();
        -:  883:  }
        -:  884:};
        -:  885:
        -:  886:/// Bufferization of tensor.rank. Replace with memref.rank.
    20474:  887:struct RankOpInterface
call    0 returned 100%
        -:  888:    : public BufferizableOpInterface::ExternalModel<RankOpInterface,
        -:  889:                                                    tensor::RankOp> {
    #####:  890:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  891:                              const AnalysisState &state) const {
    #####:  892:    return true;
        -:  893:  }
        -:  894:
    #####:  895:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  896:                               const AnalysisState &state) const {
    #####:  897:    return false;
        -:  898:  }
        -:  899:
    #####:  900:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  901:                                            const AnalysisState &state) const {
    #####:  902:    return {};
        -:  903:  }
        -:  904:
        -:  905:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  906:                          const BufferizationOptions &options) const {
        -:  907:    auto rankOp = cast<tensor::RankOp>(op);
        -:  908:    FailureOr<Value> v = getBuffer(rewriter, rankOp.getTensor(), options);
        -:  909:    if (failed(v))
        -:  910:      return failure();
        -:  911:    replaceOpWithNewBufferizedOp<memref::RankOp>(rewriter, op, rankOp.getType(),
        -:  912:                                                 *v);
        -:  913:    return success();
        -:  914:  }
        -:  915:};
        -:  916:
        -:  917:/// Bufferization of tensor.reshape. Replace with memref.reshape.
    20474:  918:struct ReshapeOpInterface
call    0 returned 100%
        -:  919:    : public BufferizableOpInterface::ExternalModel<ReshapeOpInterface,
        -:  920:                                                    tensor::ReshapeOp> {
    #####:  921:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  922:                              const AnalysisState &state) const {
    #####:  923:    if (&opOperand == &op->getOpOperand(1) /* shape */)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  924:      return true;
        -:  925:    return false;
        -:  926:  }
        -:  927:
    #####:  928:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  929:                               const AnalysisState &state) const {
    #####:  930:    return false;
        -:  931:  }
        -:  932:
    #####:  933:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  934:                                            const AnalysisState &state) const {
    #####:  935:    return {op->getOpResult(0)};
call    0 never executed
        -:  936:  }
        -:  937:
    #####:  938:  BufferRelation bufferRelation(Operation *op, OpResult opResult,
        -:  939:                                const AnalysisState &state) const {
    #####:  940:    return BufferRelation::Equivalent;
        -:  941:  }
        -:  942:
        -:  943:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  944:                          const BufferizationOptions &options) const {
        -:  945:    auto reshapeOp = cast<tensor::ReshapeOp>(op);
        -:  946:    FailureOr<Value> srcBuffer =
        -:  947:        getBuffer(rewriter, reshapeOp.getSource(), options);
        -:  948:    FailureOr<Value> shapeBuffer =
        -:  949:        getBuffer(rewriter, reshapeOp.getShape(), options);
        -:  950:    if (failed(srcBuffer) || failed(shapeBuffer))
        -:  951:      return failure();
        -:  952:    auto resultMemRefType = getMemRefType(
        -:  953:        reshapeOp.getResult(), options, /*layout=*/{},
        -:  954:        srcBuffer->getType().cast<BaseMemRefType>().getMemorySpaceAsInt());
        -:  955:    replaceOpWithNewBufferizedOp<memref::ReshapeOp>(
        -:  956:        rewriter, op, resultMemRefType, *srcBuffer, *shapeBuffer);
        -:  957:    return success();
        -:  958:  }
        -:  959:};
        -:  960:
        -:  961:/// Analysis of ParallelInsertSliceOp.
    20474:  962:struct ParallelInsertSliceOpInterface
call    0 returned 100%
        -:  963:    : public BufferizableOpInterface::ExternalModel<
        -:  964:          ParallelInsertSliceOpInterface, ParallelInsertSliceOp> {
    #####:  965:  SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
        -:  966:                                            const AnalysisState &state) const {
    #####:  967:      return {};
        -:  968:  }
        -:  969:
    #####:  970:  bool bufferizesToMemoryRead(Operation *op, OpOperand &opOperand,
        -:  971:                              const AnalysisState &state) const {
    #####:  972:    return true;
        -:  973:  }
        -:  974:
    #####:  975:  bool bufferizesToMemoryWrite(Operation *op, OpOperand &opOperand,
        -:  976:                               const AnalysisState &state) const {
    #####:  977:    return &opOperand == &op->getOpOperand(1) /*dest*/;
call    0 never executed
        -:  978:  }
        -:  979:
        -:  980:  LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
        -:  981:                          const BufferizationOptions &options) const {
        -:  982:    OpBuilder::InsertionGuard g(rewriter);
        -:  983:    auto parallelInsertSliceOp = cast<ParallelInsertSliceOp>(op);
        -:  984:    ParallelCombiningOpInterface parallelCombiningParent =
        -:  985:        parallelInsertSliceOp.getParallelCombiningParent();
        -:  986:
        -:  987:    // Bufferize the op outside of the parallel combining terminator.
        -:  988:    rewriter.setInsertionPoint(parallelCombiningParent);
        -:  989:
        -:  990:    // Get source and destination buffers.
        -:  991:    FailureOr<Value> destBuffer =
        -:  992:        getBuffer(rewriter, parallelInsertSliceOp.getDest(), options);
        -:  993:    if (failed(destBuffer))
        -:  994:      return failure();
        -:  995:    FailureOr<Value> srcBuffer =
        -:  996:        getBuffer(rewriter, parallelInsertSliceOp.getSource(), options);
        -:  997:    if (failed(srcBuffer))
        -:  998:      return failure();
        -:  999:
        -: 1000:    // Take a subview of the destination buffer.
        -: 1001:    auto destBufferType = destBuffer->getType().cast<MemRefType>();
        -: 1002:    auto subviewMemRefType =
        -: 1003:        memref::SubViewOp::inferRankReducedResultType(
        -: 1004:            parallelInsertSliceOp.getSourceType().getShape(), destBufferType,
        -: 1005:            parallelInsertSliceOp.getMixedOffsets(),
        -: 1006:            parallelInsertSliceOp.getMixedSizes(),
        -: 1007:            parallelInsertSliceOp.getMixedStrides())
        -: 1008:            .cast<MemRefType>();
        -: 1009:    Value subview = rewriter.create<memref::SubViewOp>(
        -: 1010:        parallelInsertSliceOp.getLoc(), subviewMemRefType, *destBuffer,
        -: 1011:        parallelInsertSliceOp.getMixedOffsets(),
        -: 1012:        parallelInsertSliceOp.getMixedSizes(),
        -: 1013:        parallelInsertSliceOp.getMixedStrides());
        -: 1014:
        -: 1015:    // This memcpy will fold away if everything bufferizes in-place.
        -: 1016:    if (failed(options.createMemCpy(rewriter, parallelInsertSliceOp.getLoc(),
        -: 1017:                                    *srcBuffer, subview)))
        -: 1018:      return failure();
        -: 1019:
        -: 1020:    // Delete the op.
        -: 1021:    rewriter.eraseOp(op);
        -: 1022:    return success();
        -: 1023:  }
        -: 1024:
    #####: 1025:  bool isNotConflicting(Operation *op, OpOperand *uRead,
        -: 1026:                        OpOperand *uConflictingWrite,
        -: 1027:                        const AnalysisState &state) const {
    #####: 1028:    return isNotConflictingInsertSliceLikeOp<tensor::ParallelInsertSliceOp>(
call    0 never executed
        -: 1029:        op, uRead, uConflictingWrite, state);
        -: 1030:  }
        -: 1031:};
        -: 1032:
        -: 1033:} // namespace
        -: 1034:} // namespace tensor
        -: 1035:} // namespace mlir
        -: 1036:
function _ZN4mlir6tensor45registerBufferizableOpInterfaceExternalModelsERNS_15DialectRegistryE called 129191 returned 100% blocks executed 100%
   129191: 1037:void mlir::tensor::registerBufferizableOpInterfaceExternalModels(
        -: 1038:    DialectRegistry &registry) {
function _ZZN4mlir6tensor45registerBufferizableOpInterfaceExternalModelsERNS_15DialectRegistryEENKUlPNS_11MLIRContextEPNS0_13TensorDialectEE_clES4_S6_.isra.0 called 20474 returned 100% blocks executed 100%
   149665: 1039:  registry.addExtension(+[](MLIRContext *ctx, tensor::TensorDialect *dialect) {
call    0 returned 100%
    20474: 1040:    CastOp::attachInterface<CastOpInterface>(*ctx);
call    0 returned 100%
    20474: 1041:    CollapseShapeOp::attachInterface<CollapseShapeOpInterface>(*ctx);
call    0 returned 100%
    20474: 1042:    DimOp::attachInterface<DimOpInterface>(*ctx);
call    0 returned 100%
    20474: 1043:    ExpandShapeOp::attachInterface<ExpandShapeOpInterface>(*ctx);
call    0 returned 100%
    20474: 1044:    ExtractSliceOp::attachInterface<ExtractSliceOpInterface>(*ctx);
call    0 returned 100%
    20474: 1045:    ExtractOp::attachInterface<ExtractOpInterface>(*ctx);
call    0 returned 100%
    20474: 1046:    FromElementsOp::attachInterface<FromElementsOpInterface>(*ctx);
call    0 returned 100%
    20474: 1047:    GenerateOp::attachInterface<GenerateOpInterface>(*ctx);
call    0 returned 100%
    20474: 1048:    InsertOp::attachInterface<InsertOpInterface>(*ctx);
call    0 returned 100%
    20474: 1049:    InsertSliceOp::attachInterface<InsertSliceOpInterface>(*ctx);
call    0 returned 100%
    20474: 1050:    PadOp::attachInterface<PadOpInterface>(*ctx);
call    0 returned 100%
    20474: 1051:    ParallelInsertSliceOp::attachInterface<ParallelInsertSliceOpInterface>(
call    0 returned 100%
        -: 1052:        *ctx);
    20474: 1053:    RankOp::attachInterface<RankOpInterface>(*ctx);
call    0 returned 100%
    20474: 1054:    ReshapeOp::attachInterface<ReshapeOpInterface>(*ctx);
call    0 returned 100%
        -: 1055:
        -: 1056:    // Load additional dialects of which ops may get created.
    20474: 1057:    ctx->loadDialect<arith::ArithDialect, linalg::LinalgDialect>();
call    0 returned 100%
    20474: 1058:  });
   129191: 1059:}
