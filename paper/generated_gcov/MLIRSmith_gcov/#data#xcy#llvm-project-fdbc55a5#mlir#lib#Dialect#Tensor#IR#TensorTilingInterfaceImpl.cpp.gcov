        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Tensor/IR/TensorTilingInterfaceImpl.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Tensor/IR/CMakeFiles/obj.MLIRTensorTilingInterfaceImpl.dir/TensorTilingInterfaceImpl.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Tensor/IR/CMakeFiles/obj.MLIRTensorTilingInterfaceImpl.dir/TensorTilingInterfaceImpl.cpp.gcda
        -:    0:Runs:116161
        -:    1://===- TensorTilingInterface.cpp - Tiling Interface  models *- C++ ------*-===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8:
        -:    9:#include "mlir/Dialect/Tensor/IR/TensorTilingInterfaceImpl.h"
        -:   10:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   11:#include "mlir/Dialect/Arith/Utils/Utils.h"
        -:   12:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   13:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   14:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   15:#include "mlir/Interfaces/TilingInterface.h"
        -:   16:
        -:   17:using namespace mlir;
        -:   18:using namespace mlir::tensor;
        -:   19:
        -:   20:namespace {
        -:   21:
    92062:   22:struct PadOpTiling : public TilingInterface::ExternalModel<PadOpTiling, PadOp> {
call    0 returned 100%
        -:   23:
        -:   24:  SmallVector<utils::IteratorType> getLoopIteratorTypes(Operation *op) const {
        -:   25:    auto padOp = cast<PadOp>(op);
        -:   26:    SmallVector<utils::IteratorType> iteratorTypes(
        -:   27:        padOp.getResultType().getRank(), utils::IteratorType::parallel);
        -:   28:    return iteratorTypes;
        -:   29:  }
        -:   30:
        -:   31:  SmallVector<Range> getIterationDomain(Operation *op, OpBuilder &b) const {
        -:   32:    ReifiedRankedShapedTypeDims reifiedShapes;
        -:   33:    ReifyRankedShapedTypeOpInterface reifyShapedTypeInterface =
        -:   34:        dyn_cast<ReifyRankedShapedTypeOpInterface>(op);
        -:   35:    (void)reifyShapedTypeInterface.reifyResultShapes(b, reifiedShapes);
        -:   36:
        -:   37:    Location loc = op->getLoc();
        -:   38:    Value zero = b.create<arith::ConstantIndexOp>(loc, 0);
        -:   39:    Value one = b.create<arith::ConstantIndexOp>(loc, 1);
        -:   40:    // Initialize all the ranges to {zero, one, one}. All the `ub`s are
        -:   41:    // overwritten.
        -:   42:    SmallVector<Range> loopRanges(reifiedShapes[0].size(), {zero, one, one});
        -:   43:    for (const auto &ub : enumerate(reifiedShapes[0]))
        -:   44:      loopRanges[ub.index()].size = ub.value();
        -:   45:    return loopRanges;
        -:   46:  }
        -:   47:
        -:   48:  SmallVector<Operation *>
        -:   49:  getTiledImplementation(Operation *op, OpBuilder &b,
        -:   50:                         ArrayRef<OpFoldResult> offsets,
        -:   51:                         ArrayRef<OpFoldResult> sizes) const {
        -:   52:    Operation *result =
        -:   53:        tensor::bubbleUpPadSlice(b, cast<PadOp>(op), offsets, sizes);
        -:   54:    if (!result)
        -:   55:      return {};
        -:   56:    return {result};
        -:   57:  }
        -:   58:
        -:   59:  LogicalResult
        -:   60:  getResultTilePosition(Operation *op, OpBuilder &b, unsigned resultNumber,
        -:   61:                        ArrayRef<OpFoldResult> offsets,
        -:   62:                        ArrayRef<OpFoldResult> sizes,
        -:   63:                        SmallVector<OpFoldResult> &resultOffsets,
        -:   64:                        SmallVector<OpFoldResult> &resultSizes) const {
        -:   65:    resultOffsets.assign(offsets.begin(), offsets.end());
        -:   66:    resultSizes.assign(sizes.begin(), sizes.end());
        -:   67:    return success();
        -:   68:  }
        -:   69:};
        -:   70:
        -:   71:} // namespace
        -:   72:
function _ZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_b called 0 returned 0% blocks executed 0%
    #####:   73:Operation *tensor::bubbleUpPadSlice(OpBuilder &b, tensor::PadOp padOp,
        -:   74:                                    ArrayRef<OpFoldResult> offsets,
        -:   75:                                    ArrayRef<OpFoldResult> sizes,
        -:   76:                                    bool generateZeroSliceGuard) {
        -:   77:  // Only constant padding value supported.
    #####:   78:  Value padValue = padOp.getConstantPaddingValue();
call    0 never executed
    #####:   79:  if (!padValue)
branch  0 never executed
branch  1 never executed
        -:   80:    return nullptr;
        -:   81:
        -:   82:  // Helper variables and functions for various arithmetic operations. These
        -:   83:  // are used extensively for computing new offset/length and padding values.
    #####:   84:  Location loc = padOp->getLoc();
call    0 never executed
    #####:   85:  AffineExpr dim0, dim1;
    #####:   86:  bindDims(b.getContext(), dim0, dim1);
call    0 never executed
        -:   87:  // Add two integers.
    #####:   88:  auto addMap = AffineMap::get(2, 0, {dim0 + dim1});
call    0 never executed
call    1 never executed
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlNS_5ValueES8_E_clES8_S8_ called 0 returned 0% blocks executed 0%
    #####:   89:  auto add = [&](Value v1, Value v2) {
    #####:   90:    return b.createOrFold<AffineApplyOp>(loc, addMap, ValueRange{v1, v2});
call    0 never executed
call    1 never executed
    #####:   91:  };
        -:   92:  // Subtract two integers.
    #####:   93:  auto subMap = AffineMap::get(2, 0, {dim0 - dim1});
call    0 never executed
call    1 never executed
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlNS_5ValueES8_E0_clES8_S8_ called 0 returned 0% blocks executed 0%
    #####:   94:  auto sub = [&](Value v1, Value v2) {
    #####:   95:    return b.createOrFold<AffineApplyOp>(loc, subMap, ValueRange{v1, v2});
call    0 never executed
call    1 never executed
    #####:   96:  };
        -:   97:  // Take the minimum of two integers.
    #####:   98:  auto idMap = AffineMap::getMultiDimIdentityMap(2, b.getContext());
call    0 never executed
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlNS_5ValueES8_E1_clES8_S8_ called 0 returned 0% blocks executed 0%
    #####:   99:  auto min = [&](Value v1, Value v2) {
    #####:  100:    return b.createOrFold<AffineMinOp>(loc, idMap, ValueRange{v1, v2});
call    0 never executed
call    1 never executed
    #####:  101:  };
        -:  102:  // Take the maximum of two integers.
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlNS_5ValueES8_E2_clES8_S8_ called 0 returned 0% blocks executed 0%
    #####:  103:  auto max = [&](Value v1, Value v2) {
    #####:  104:    return b.createOrFold<AffineMaxOp>(loc, idMap, ValueRange{v1, v2});
call    0 never executed
call    1 never executed
    #####:  105:  };
        -:  106:  // Zero index-typed integer.
    #####:  107:  auto zero = b.create<arith::ConstantIndexOp>(loc, 0);
call    0 never executed
        -:  108:
        -:  109:  // Helper function for filling static/dynamic low/high padding indices
        -:  110:  // vectors of PadOp.
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlNS_5ValueERNS4_11SmallVectorIS8_Lj6EEERNS9_IlLj6EEEE3_clES8_SB_SD_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  111:  auto appendIndex = [&](Value val, SmallVector<Value> &dynIndices,
        -:  112:                         SmallVector<int64_t> &staticIndices) {
    #####:  113:    if (auto constInt = getConstantIntValue(val)) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  114:      staticIndices.push_back(*constInt);
call    0 never executed
        -:  115:    } else {
    #####:  116:      staticIndices.push_back(ShapedType::kDynamicSize);
call    0 never executed
    #####:  117:      dynIndices.push_back(val);
call    0 never executed
        -:  118:    }
    #####:  119:  };
        -:  120:
        -:  121:  // Compute new offsets, lengths, low padding, high padding.
    #####:  122:  SmallVector<OpFoldResult> newOffsets, newLengths, newStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  123:  SmallVector<Value> newLows, newHighs;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  124:  SmallVector<int64_t> staticNewLows, staticNewHighs;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  125:  // Set to true if the original data source is not read at all.
    #####:  126:  bool hasZeroLen = false;
        -:  127:  // Same as hasZeroLen, but for dynamic dimension sizes. This condition
        -:  128:  // is true if the original data source turns out to be unused at runtime.
    #####:  129:  Value dynHasZeroLenCond;
        -:  130:
    #####:  131:  int64_t rank = padOp.getSourceType().getRank();
call    0 never executed
call    1 never executed
    #####:  132:  for (unsigned dim = 0; dim < rank; ++dim) {
branch  0 never executed
branch  1 never executed
    #####:  133:    auto low =
    #####:  134:        getValueOrCreateConstantIndexOp(b, loc, padOp.getMixedLowPad()[dim]);
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  135:    bool hasLowPad = getConstantIntValue(low) != static_cast<int64_t>(0);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  136:    auto high =
    #####:  137:        getValueOrCreateConstantIndexOp(b, loc, padOp.getMixedHighPad()[dim]);
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  138:    bool hasHighPad = getConstantIntValue(high) != static_cast<int64_t>(0);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  139:    auto offset = getValueOrCreateConstantIndexOp(b, loc, offsets[dim]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  140:    auto length = getValueOrCreateConstantIndexOp(b, loc, sizes[dim]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  141:    auto srcSize = b.createOrFold<tensor::DimOp>(loc, padOp.getSource(), dim);
call    0 never executed
call    1 never executed
        -:  142:
        -:  143:    // The new amount of low padding is `low - offset`. Except for the case
        -:  144:    // where none of the low padding is read. In that case, the new amount of
        -:  145:    // low padding is zero.
        -:  146:    //
        -:  147:    // Optimization: If low = 0, then newLow = 0.
    #####:  148:    Value newLow = hasLowPad ? max(zero, sub(low, offset)) : zero;
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  149:    appendIndex(newLow, newLows, staticNewLows);
call    0 never executed
        -:  150:
        -:  151:    // Start reading the data from position `offset - low`. Since the original
        -:  152:    // read may have started in the low padding zone, this value could be
        -:  153:    // negative. Therefore, start reading from:
        -:  154:    //
        -:  155:    // max(offset - low, 0)
        -:  156:    //
        -:  157:    // The original read could also have started in the high padding zone.
        -:  158:    // In that case, set the offset to the end of source tensor. The new
        -:  159:    // ExtractSliceOp length will be zero in that case. (Effectively reading
        -:  160:    // no data from the source.)
        -:  161:    //
        -:  162:    // Optimization: If low = 0, then the formula can be simplified.
    #####:  163:    Value newOffset = hasLowPad ? min(max(sub(offset, low), zero), srcSize)
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  164:                                : min(offset, srcSize);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  165:    newOffsets.push_back(getAsOpFoldResult(newOffset));
call    0 never executed
call    1 never executed
        -:  166:
        -:  167:    // The original ExtractSliceOp was reading until position `offset +
        -:  168:    // length`. Therefore, the corresponding position within the source tensor
        -:  169:    // is:
        -:  170:    //
        -:  171:    // offset + length - low
        -:  172:    //
        -:  173:    // In case the original ExtractSliceOp stopped reading within the low
        -:  174:    // padding zone, this value can be negative. In that case, the end
        -:  175:    // position of the read should be zero. (Similar to newOffset.)
        -:  176:    //
        -:  177:    // The original read could also have stopped in the high padding zone.
        -:  178:    // In that case, set the end positition of the read should be the end of
        -:  179:    // the source tensor. (Similar to newOffset.)
        -:  180:    //
        -:  181:    // endLoc = min(max(offset - low + length, 0), srcSize)
        -:  182:    //
        -:  183:    // The new ExtractSliceOp length is `endLoc - newOffset`.
        -:  184:    //
        -:  185:    // Optimization: If low = 0, then the formula can be simplified.
    #####:  186:    Value endLoc = hasLowPad
    #####:  187:                       ? min(max(add(sub(offset, low), length), zero), srcSize)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  188:                       : min(add(offset, length), srcSize);
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  189:    Value newLength = sub(endLoc, newOffset);
call    0 never executed
    #####:  190:    newLengths.push_back(getAsOpFoldResult(newLength));
call    0 never executed
call    1 never executed
        -:  191:
        -:  192:    // Check if newLength is zero. In that case, no SubTensorOp should be
        -:  193:    // executed.
    #####:  194:    if (auto newLengthInt = getConstantIntValue(newLength)) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  195:      hasZeroLen |= *newLengthInt == 0;
        -:  196:    } else {
    #####:  197:      Value check = b.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq,
    #####:  198:                                            newLength, zero);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  199:      dynHasZeroLenCond =
        -:  200:          dynHasZeroLenCond
    #####:  201:              ? b.create<arith::OrIOp>(loc, check, dynHasZeroLenCond)
call    0 never executed
    #####:  202:              : check;
branch  0 never executed
branch  1 never executed
        -:  203:    }
        -:  204:
        -:  205:    // The amount of high padding is simply the number of elements remaining,
        -:  206:    // so that the result has the same length as the original ExtractSliceOp.
        -:  207:    // As an optimization, if the original high padding is zero, then the new
        -:  208:    // high padding must also be zero.
    #####:  209:    Value newHigh = hasHighPad ? sub(sub(length, newLength), newLow) : zero;
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  210:    appendIndex(newHigh, newHighs, staticNewHighs);
call    0 never executed
        -:  211:
        -:  212:    // Only unit stride supported.
    #####:  213:    newStrides.push_back(b.getIndexAttr(1));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  214:  }
        -:  215:
        -:  216:  // The shape of the result can be obtained from the sizes passed in.
    #####:  217:  SmallVector<Value> dynDims;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  218:  SmallVector<int64_t> shape;
branch  0 never executed
branch  1 never executed
    #####:  219:  dispatchIndexOpFoldResults(sizes, dynDims, shape, ShapedType::kDynamicSize);
call    0 never executed
    #####:  220:  RankedTensorType resultType =
    #####:  221:      RankedTensorType::get(shape, padOp.getResultType().getElementType());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  222:
        -:  223:  // Insert cast to ensure that types match. (May be folded away.)
    #####:  224:  auto castResult = [&](Value val) -> Operation * {
    #####:  225:    return b.create<tensor::CastOp>(loc, resultType, val);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  226:  };
        -:  227:
        -:  228:  // In cases where the original data source is unused: Emit a GenerateOp and
        -:  229:  // do not generate a SliceOp. (The result shape of the SliceOp would
        -:  230:  // have a dimension of size 0, the semantics of which is unclear.)
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlvE5_clEv called 0 returned 0% blocks executed 0%
    #####:  231:  auto createGenerateOp = [&]() {
        -:  232:    // Create GenerateOp.
    #####:  233:    auto generateOp = b.create<tensor::GenerateOp>(
    #####:  234:        loc, resultType, dynDims,
    #####:  235:        [&](OpBuilder &builder, Location gLoc, ValueRange indices) {
    #####:  236:          builder.create<tensor::YieldOp>(gLoc, padValue);
call    0 never executed
    #####:  237:        });
call    0 never executed
    #####:  238:    return castResult(generateOp);
call    0 never executed
    #####:  239:  };
        -:  240:
        -:  241:  // Emit a SliceOp and a PadOp. Should not be used in cases where
        -:  242:  // the result shape of the new SliceOp has a zero dimension.
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlvE6_clEv called 0 returned 0% blocks executed 0%
    #####:  243:  auto createPadOfExtractSlice = [&]() {
        -:  244:    // Create pad(extract_slice(x)).
    #####:  245:    auto newSliceOp = b.create<tensor::ExtractSliceOp>(
    #####:  246:        loc, padOp.getSource(), newOffsets, newLengths, newStrides);
call    0 never executed
call    1 never executed
    #####:  247:    auto newPadOp = b.create<PadOp>(loc, newSliceOp, staticNewLows,
    #####:  248:                                    staticNewHighs, newLows, newHighs);
call    0 never executed
        -:  249:
        -:  250:    // Copy region to new PadOp.
    #####:  251:    BlockAndValueMapping bvm;
call    0 never executed
    #####:  252:    padOp.getRegion().cloneInto(&newPadOp.getRegion(), bvm);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  253:
        -:  254:    // Cast result and return.
    #####:  255:    return castResult(newPadOp);
call    0 never executed
call    1 never executed
    #####:  256:  };
        -:  257:
        -:  258:  // Rewrite extract_slice(pad(x)) into a GenerateOp it is statically known that
        -:  259:  // the original data source x is not used.
    #####:  260:  if (hasZeroLen)
branch  0 never executed
branch  1 never executed
    #####:  261:    return createGenerateOp();
call    0 never executed
        -:  262:
        -:  263:  // If there are dynamic dimensions: Generate an scf.if check to avoid
        -:  264:  // creating SliceOps with result dimensions of size 0 at runtime.
    #####:  265:  if (generateZeroSliceGuard && dynHasZeroLenCond) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  266:    auto result = b.create<scf::IfOp>(
        -:  267:        loc, resultType, dynHasZeroLenCond,
        -:  268:        /*thenBuilder=*/
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlS2_NS_8LocationEE7_clES2_S8_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  269:        [&](OpBuilder &b, Location loc) {
    #####:  270:          b.create<scf::YieldOp>(loc, createGenerateOp()->getResult(0));
call    0 never executed
call    1 never executed
    #####:  271:        },
        -:  272:        /*elseBuilder=*/
function _ZZN4mlir6tensor16bubbleUpPadSliceERNS_9OpBuilderENS0_5PadOpEN4llvm8ArrayRefINS_12OpFoldResultEEES7_bENKUlS2_NS_8LocationEE8_clES2_S8_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  273:        [&](OpBuilder &b, Location loc) {
    #####:  274:          b.create<scf::YieldOp>(loc, createPadOfExtractSlice()->getResult(0));
call    0 never executed
call    1 never executed
    #####:  275:        });
call    0 never executed
    #####:  276:    return result;
        -:  277:  }
    #####:  278:  return createPadOfExtractSlice();
call    0 never executed
        -:  279:}
        -:  280:
function _ZN4mlir6tensor37registerTilingInterfaceExternalModelsERNS_15DialectRegistryE called 116630 returned 100% blocks executed 100%
   116630:  281:void mlir::tensor::registerTilingInterfaceExternalModels(
        -:  282:    DialectRegistry &registry) {
   116630:  283:  registry.addExtension(+[](MLIRContext *ctx, TensorDialect *dialect) {
call    0 returned 100%
        -:  284:    tensor::PadOp::attachInterface<PadOpTiling>(*ctx);
        -:  285:  });
   116630:  286:}
