        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Linalg/IR/LinalgOps.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Linalg/IR/CMakeFiles/obj.MLIRLinalgDialect.dir/LinalgOps.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Linalg/IR/CMakeFiles/obj.MLIRLinalgDialect.dir/LinalgOps.cpp.gcda
        -:    0:Runs:116158
        -:    1://===- LinalgOps.cpp - Implementation of the linalg operations ------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements the Linalg operations.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   14:
        -:   15:#include "mlir/AsmParser/AsmParser.h"
        -:   16:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   17:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   18:#include "mlir/Dialect/Arith/Utils/Utils.h"
        -:   19:#include "mlir/Dialect/Complex/IR/Complex.h"
        -:   20:#include "mlir/Dialect/Math/IR/Math.h"
        -:   21:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   22:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   23:#include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
        -:   24:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   25:#include "mlir/Dialect/Utils/ReshapeOpsUtils.h"
        -:   26:#include "mlir/Dialect/Utils/StaticValueUtils.h"
        -:   27:#include "mlir/IR/AffineExprVisitor.h"
        -:   28:#include "mlir/IR/AffineMap.h"
        -:   29:#include "mlir/IR/Matchers.h"
        -:   30:#include "mlir/IR/OpImplementation.h"
        -:   31:#include "mlir/IR/PatternMatch.h"
        -:   32:#include "mlir/Interfaces/InferTypeOpInterface.h"
        -:   33:
        -:   34:#include "llvm/ADT/DenseMap.h"
        -:   35:#include "llvm/ADT/SmallSet.h"
        -:   36:#include "llvm/ADT/StringSet.h"
        -:   37:#include "llvm/ADT/TypeSwitch.h"
        -:   38:#include "llvm/Support/FormatVariadic.h"
        -:   39:#include "llvm/Support/MathExtras.h"
        -:   40:#include "llvm/Support/raw_ostream.h"
        -:   41:
        -:   42:using namespace mlir;
        -:   43:using namespace mlir::linalg;
        -:   44:
        -:   45://===----------------------------------------------------------------------===//
        -:   46:// Support for named Linalg ops defined in ods-gen.
        -:   47://===----------------------------------------------------------------------===//
        -:   48:
        -:   49:using RegionBuilderFn = llvm::function_ref<void(ImplicitLocOpBuilder &, Block &,
        -:   50:                                                ArrayRef<NamedAttribute>)>;
        -:   51:
        -:   52:/// Fills the region of a structured operation using the provided
        -:   53:/// `regionBuilder`. The method is used by both named structured ops created by
        -:   54:/// ods-gen and by manually defined C++ ops. It is called by both builders and
        -:   55:/// parsers and creates a block with arguments corresponding to the elemental
        -:   56:/// types of `inputTypes` and `outputTypes`. All output types are asserted to be
        -:   57:/// ShapedType.
function _ZL22fillStructuredOpRegionRN4mlir9OpBuilderERNS_6RegionENS_9TypeRangeES4_N4llvm8ArrayRefINS_14NamedAttributeEEENS5_12function_refIFvRNS_20ImplicitLocOpBuilderERNS_5BlockES8_EEE called 1670212 returned 100% blocks executed 85%
  1670212:   58:static void fillStructuredOpRegion(OpBuilder &opBuilder, Region &region,
        -:   59:                                   TypeRange inputTypes, TypeRange outputTypes,
        -:   60:                                   ArrayRef<NamedAttribute> attrs,
        -:   61:                                   RegionBuilderFn regionBuilder) {
 1670212*:   62:  assert(llvm::all_of(outputTypes, [](Type t) { return t.isa<ShapedType>(); }));
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
        -:   63:
        -:   64:  // TODO: atm all operands go through getElementTypeOrSelf,
        -:   65:  // reconsider when we have evidence we need to.
  1670212:   66:  SmallVector<Type, 8> argTypes;
  1670212:   67:  SmallVector<Location, 8> argLocs;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
  5010636:   68:  for (auto containers : {inputTypes, outputTypes}) {
branch  0 taken 67% (fallthrough)
branch  1 taken 33%
  8167876:   69:    for (auto t : containers) {
branch  0 taken 41% (fallthrough)
branch  1 taken 59%
call    2 returned 100%
  4827452:   70:      argTypes.push_back(getElementTypeOrSelf(t));
call    0 returned 100%
call    1 returned 100%
        -:   71:
        -:   72:      // TODO: Pass in a proper location here.
  4827452:   73:      argLocs.push_back(opBuilder.getUnknownLoc());
call    0 returned 100%
call    1 returned 100%
        -:   74:    }
        -:   75:  }
        -:   76:
        -:   77:  // RAII.
  3340424:   78:  OpBuilder::InsertionGuard guard(opBuilder);
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
  1670212:   79:  Block *body =
call    0 returned 100%
  1670212:   80:      opBuilder.createBlock(&region, /*insertPt=*/{}, argTypes, argLocs);
call    0 returned 100%
call    1 returned 100%
        -:   81:
  1670212:   82:  opBuilder.setInsertionPointToStart(body);
call    0 returned 100%
  1670212:   83:  ImplicitLocOpBuilder b(opBuilder.getUnknownLoc(), opBuilder);
call    0 returned 100%
call    1 returned 100%
  1670212:   84:  regionBuilder(b, *body, attrs);
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -:   85:
        -:   86:  // indexing_maps is an auto-generated method.
        -:   87:
        -:   88:  // iterator_types is an auto-generated method.
  1670212:   89:}
        -:   90:
        -:   91:/// Creates a structured operation given `inputs`, `outputs`, and `attributes`.
        -:   92:/// The result types are derived automatically if `resultTensorTypes` is none.
        -:   93:/// The body of the operation is filled using `regionBuilder`. All ods-gen
        -:   94:/// created structured operations use the method to implement their builders.
        -:   95:static void buildStructuredOp(OpBuilder &b, OperationState &state,
        -:   96:                              llvm::Optional<TypeRange> resultTensorTypes,
        -:   97:                              ValueRange inputs, ValueRange outputs,
        -:   98:                              ArrayRef<NamedAttribute> attributes,
        -:   99:                              RegionBuilderFn regionBuilder) {
        -:  100:  // Derive the result types if needed.
        -:  101:  SmallVector<Type> derivedResultTypes =
        -:  102:      resultTensorTypes.value_or(TypeRange());
        -:  103:  if (!resultTensorTypes)
        -:  104:    copy_if(outputs.getTypes(), std::back_inserter(derivedResultTypes),
        -:  105:            [](Type type) { return type.isa<RankedTensorType>(); });
        -:  106:
        -:  107:  state.addOperands(inputs);
        -:  108:  state.addOperands(outputs);
        -:  109:  state.addTypes(derivedResultTypes);
        -:  110:  state.addAttributes(attributes);
        -:  111:  state.addAttribute(
        -:  112:      "operand_segment_sizes",
        -:  113:      b.getDenseI32ArrayAttr({static_cast<int32_t>(inputs.size()),
        -:  114:                              static_cast<int32_t>(outputs.size())}));
        -:  115:
        -:  116:  // Create and fill the region of the structured operation.
        -:  117:  Region &region = *state.addRegion();
        -:  118:  fillStructuredOpRegion(b, region, TypeRange(inputs), TypeRange(outputs),
        -:  119:                         state.attributes.getAttrs(), regionBuilder);
        -:  120:}
        -:  121:
        -:  122:/// Common parsing used for both named structured ops created by ods-gen and by
        -:  123:/// manually defined C++ ops. Does not handle regions.
        -:  124:static ParseResult
function _ZL28parseCommonStructuredOpPartsRN4mlir11OpAsmParserERNS_14OperationStateERN4llvm15SmallVectorImplINS_4TypeEEES8_b called 3269200 returned 100% blocks executed 85%
  3269200:  125:parseCommonStructuredOpParts(OpAsmParser &parser, OperationState &result,
        -:  126:                             SmallVectorImpl<Type> &inputTypes,
        -:  127:                             SmallVectorImpl<Type> &outputTypes,
        -:  128:                             bool addOperandSegmentSizes = true) {
  3269200:  129:  SMLoc inputsOperandsLoc, outputsOperandsLoc;
  3269200:  130:  SmallVector<OpAsmParser::UnresolvedOperand, 4> inputsOperands,
call    0 returned 100%
  3269200:  131:      outputsOperands;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  132:
  3269200:  133:  if (parser.parseOptionalAttrDict(result.attributes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  134:    return failure();
        -:  135:
  3269200:  136:  if (succeeded(parser.parseOptionalKeyword("ins"))) {
call    0 returned 100%
branch  1 taken 96% (fallthrough)
branch  2 taken 4%
  3144429:  137:    if (parser.parseLParen())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  138:      return failure();
        -:  139:
  3144429:  140:    inputsOperandsLoc = parser.getCurrentLocation();
call    0 returned 100%
 6288858*:  141:    if (parser.parseOperandList(inputsOperands) ||
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
 3144429*:  142:        parser.parseColonTypeList(inputTypes) || parser.parseRParen())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
call    5 returned 100%
branch  6 taken 0% (fallthrough)
branch  7 taken 100%
    #####:  143:      return failure();
        -:  144:  }
        -:  145:
  3269200:  146:  if (succeeded(parser.parseOptionalKeyword("outs"))) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
  3269200:  147:    outputsOperandsLoc = parser.getCurrentLocation();
call    0 returned 100%
 6538400*:  148:    if (parser.parseLParen() || parser.parseOperandList(outputsOperands) ||
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
branch  4 taken 100% (fallthrough)
branch  5 taken 0%
branch  6 taken 100% (fallthrough)
branch  7 taken 0%
 6538400*:  149:        parser.parseColonTypeList(outputTypes) || parser.parseRParen())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
call    5 returned 100%
branch  6 taken 100% (fallthrough)
branch  7 taken 0%
    #####:  150:      return failure();
        -:  151:  }
        -:  152:
 6538400*:  153:  if (parser.resolveOperands(inputsOperands, inputTypes, inputsOperandsLoc,
  6538400:  154:                             result.operands) ||
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
 3269200*:  155:      parser.resolveOperands(outputsOperands, outputTypes, outputsOperandsLoc,
  3269200:  156:                             result.operands))
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
    #####:  157:    return failure();
        -:  158:
  3269200:  159:  if (addOperandSegmentSizes) {
branch  0 taken 71%
branch  1 taken 29%
  2328902:  160:    result.addAttribute("operand_segment_sizes",
call    0 returned 100%
  2328902:  161:                        parser.getBuilder().getDenseI32ArrayAttr(
call    0 returned 100%
  2328902:  162:                            {static_cast<int32_t>(inputsOperands.size()),
call    0 returned 100%
  2328902:  163:                             static_cast<int32_t>(outputsOperands.size())}));
call    0 returned 100%
call    1 returned 100%
        -:  164:  }
  3269200:  165:  return success();
        -:  166:}
        -:  167:
function _ZL28printCommonStructuredOpPartsRN4mlir12OpAsmPrinterENS_10ValueRangeES2_ called 3259548 returned 100% blocks executed 100%
  3259548:  168:static void printCommonStructuredOpParts(OpAsmPrinter &p, ValueRange inputs,
        -:  169:                                         ValueRange outputs) {
  3259548:  170:  if (!inputs.empty())
branch  0 taken 95% (fallthrough)
branch  1 taken 5%
 12385684:  171:    p << " ins(" << inputs << " : " << inputs.getTypes() << ")";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
call    5 returned 100%
  3259548:  172:  if (!outputs.empty())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
 13038192:  173:    p << " outs(" << outputs << " : " << outputs.getTypes() << ")";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
call    5 returned 100%
  3259548:  174:}
        -:  175:
function _ZL39printCommonStructuredOpPartsWithNewLineRN4mlir12OpAsmPrinterENS_10ValueRangeES2_ called 1313162 returned 100% blocks executed 100%
  1313162:  176:static void printCommonStructuredOpPartsWithNewLine(OpAsmPrinter &p,
        -:  177:                                                    ValueRange inputs,
        -:  178:                                                    ValueRange outputs) {
  1313162:  179:  if (!inputs.empty()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 1%
  1309205:  180:    p.printNewline();
call    0 returned 100%
  5236820:  181:    p << "ins(" << inputs << " : " << inputs.getTypes() << ")";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
call    5 returned 100%
        -:  182:  }
  1313162:  183:  if (!outputs.empty()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
  1313162:  184:    p.printNewline();
call    0 returned 100%
  5252648:  185:    p << "outs(" << outputs << " : " << outputs.getTypes() << ")";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
call    5 returned 100%
        -:  186:  }
  1313162:  187:}
        -:  188://===----------------------------------------------------------------------===//
        -:  189:// Specific parsing and printing for named structured ops created by ods-gen.
        -:  190://===----------------------------------------------------------------------===//
        -:  191:
function _ZL28parseNamedStructuredOpRegionRN4mlir11OpAsmParserERNS_6RegionEjNS_9TypeRangeES4_N4llvm8ArrayRefINS_14NamedAttributeEEENS5_12function_refIFvRNS_20ImplicitLocOpBuilderERNS_5BlockES8_EEE called 1670212 returned 100% blocks executed 45%
  1670212:  192:static ParseResult parseNamedStructuredOpRegion(
        -:  193:    OpAsmParser &parser, Region &region, unsigned numRegionArgs,
        -:  194:    TypeRange inputTypes, TypeRange outputTypes, ArrayRef<NamedAttribute> attrs,
        -:  195:    RegionBuilderFn regionBuilder) {
  1670212:  196:  if (numRegionArgs != inputTypes.size() + outputTypes.size()) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  197:    return parser.emitError(
call    0 never executed
call    1 never executed
    #####:  198:        parser.getCurrentLocation(),
call    0 never executed
    #####:  199:        llvm::formatv("[parseNamedStructuredOpRegion] ods-gen generated "
call    0 never executed
        -:  200:                      "region expects {0} args, got {1}",
    #####:  201:                      numRegionArgs, inputTypes.size() + outputTypes.size()));
call    0 never executed
call    1 never executed
        -:  202:  }
        -:  203:
  1670212:  204:  OpBuilder opBuilder(parser.getContext());
call    0 returned 100%
call    1 returned 100%
  1670212:  205:  fillStructuredOpRegion(opBuilder, region, inputTypes, outputTypes, attrs,
call    0 returned 100%
        -:  206:                         regionBuilder);
  1670212:  207:  return success();
        -:  208:}
        -:  209:
        -:  210:static ParseResult
  2328902:  211:parseNamedStructuredOpResults(OpAsmParser &parser,
        -:  212:                              SmallVectorImpl<Type> &resultTypes) {
  2328902:  213:  if (parser.parseOptionalArrowTypeList(resultTypes))
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
  2328902:  214:    return failure();
  2328902:  215:  return success();
        -:  216:}
        -:  217:
function _ZL22parseNamedStructuredOpRN4mlir11OpAsmParserERNS_14OperationStateEjN4llvm12function_refIFvRNS_20ImplicitLocOpBuilderERNS_5BlockENS4_8ArrayRefINS_14NamedAttributeEEEEEE called 1670212 returned 100% blocks executed 79%
  1670212:  218:static ParseResult parseNamedStructuredOp(OpAsmParser &parser,
        -:  219:                                          OperationState &result,
        -:  220:                                          unsigned numRegionArgs,
        -:  221:                                          RegionBuilderFn regionBuilder) {
        -:  222:  // TODO: Enable when ods-gen supports captures.
  3340424:  223:  SmallVector<Type, 1> inputTypes, outputTypes;
call    0 returned 100%
branch  1 taken 89% (fallthrough)
branch  2 taken 11%
  1670212:  224:  if (parseCommonStructuredOpParts(parser, result, inputTypes, outputTypes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  225:    return failure();
        -:  226:
        -:  227:  // TODO: consider merging results parsing into region parsing.
        -:  228:  // Need to wait for declarative assembly resolution to decide.
  3340424:  229:  SmallVector<Type, 1> outputTensorsTypes;
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
  3340424:  230:  if (parseNamedStructuredOpResults(parser, outputTensorsTypes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  231:    return failure();
  1670212:  232:  result.addTypes(outputTensorsTypes);
call    0 returned 100%
        -:  233:
  3340424:  234:  std::unique_ptr<Region> region = std::make_unique<Region>();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
  1670212:  235:  if (parseNamedStructuredOpRegion(parser, *region, numRegionArgs, inputTypes,
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -:  236:                                   outputTypes, result.attributes.getAttrs(),
  1670212:  237:                                   regionBuilder))
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
    #####:  238:    return failure();
  1670212:  239:  result.addRegion(std::move(region));
call    0 returned 100%
        -:  240:
  1670212:  241:  return success();
        -:  242:}
        -:  243:
  3259548:  244:static void printNamedStructuredOpResults(OpAsmPrinter &p,
        -:  245:                                          TypeRange resultTypes) {
  3259548:  246:  if (resultTypes.empty())
        -:  247:    return;
  3258883:  248:  p.printOptionalArrowTypeList(resultTypes);
call    0 returned 100%
call    1 returned 100%
        -:  249:}
        -:  250:
function _ZL22printNamedStructuredOpRN4mlir12OpAsmPrinterEPNS_9OperationENS_10ValueRangeES4_ called 2215500 returned 100% blocks executed 100%
  2215500:  251:static void printNamedStructuredOp(OpAsmPrinter &p, Operation *op,
        -:  252:                                   ValueRange inputs, ValueRange outputs) {
  2215500:  253:  p.printOptionalAttrDict(
call    0 returned 100%
        -:  254:      op->getAttrs(),
        -:  255:      /*elidedAttrs=*/{"operand_segment_sizes",
        -:  256:                       // See generated code in mlir-linalg-yaml-gen.cpp
  2215500:  257:                       "linalg.memoized_indexing_maps"});
call    0 returned 100%
        -:  258:
        -:  259:  // Printing is shared with generic ops, except for the region and
        -:  260:  // attributes.
  2215500:  261:  printCommonStructuredOpParts(p, inputs, outputs);
call    0 returned 100%
        -:  262:
        -:  263:  // Results printing.
  4431000:  264:  printNamedStructuredOpResults(p, op->getResultTypes());
branch  0 taken 100% (fallthrough)
branch  1 taken 1%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 1%
        -:  265:
        -:  266:  // Region is elided.
  2215500:  267:}
        -:  268:
        -:  269://===----------------------------------------------------------------------===//
        -:  270:// Region builder helper.
        -:  271:// TODO: Move this to a utility library.
        -:  272:// The public methods on this class are referenced directly from generated code.
        -:  273:// Helper build the unary, binary, and type conversion functions defined by the
        -:  274:// DSL. See mlir-linalg-ods-yaml-gen.cpp for the code that uses this class.
        -:  275://
        -:  276:// Implementations of the math functions must be polymorphic over numeric types,
        -:  277:// internally performing necessary casts. If the function application makes no
        -:  278:// sense, then the only recourse is to assert and return nullptr. This can be
        -:  279:// extended later if it becomes possible to fail construction of the region. The
        -:  280:// invariant should be enforced at a higher level.
        -:  281://
        -:  282:// TODO: These helpers are currently type polymorphic over the class of integer
        -:  283:// and floating point types, but they will not internally cast within bit
        -:  284:// widths of a class (mixed precision such as i8->i32) or across classes
        -:  285:// (i.e. mixed float and integer). Many such combinations are ambiguous or need
        -:  286:// to be handled with care and work is being considered to extend the op
        -:  287:// language to make such cases explicit. In the mean-time, violating this will
        -:  288:// fail verification, which is deemed acceptable.
        -:  289://===----------------------------------------------------------------------===//
        -:  290:
        -:  291:namespace {
        -:  292:
        -:  293:class RegionBuilderHelper {
        -:  294:public:
 1670212*:  295:  RegionBuilderHelper(MLIRContext *context, Block &block)
 1670212*:  296:      : context(context), block(block) {}
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
call   13 never executed
call   14 never executed
call   15 never executed
call   16 never executed
call   17 never executed
call   18 never executed
call   19 never executed
call   20 never executed
call   21 never executed
call   22 never executed
call   23 never executed
call   24 never executed
call   25 never executed
call   26 never executed
call   27 never executed
call   28 never executed
call   29 never executed
call   30 never executed
call   31 never executed
call   32 returned 100%
call   33 never executed
call   34 never executed
call   35 never executed
call   36 never executed
call   37 never executed
call   38 never executed
call   39 never executed
call   40 never executed
call   41 never executed
call   42 returned 100%
call   43 never executed
call   44 never executed
call   45 returned 100%
        -:  297:
        -:  298:  // Build the unary functions defined by OpDSL.
function _ZN12_GLOBAL__N_119RegionBuilderHelper12buildUnaryFnEN4mlir6linalg7UnaryFnENS1_5ValueE called 0 returned 0% blocks executed 0%
    #####:  299:  Value buildUnaryFn(UnaryFn unaryFn, Value arg) {
    #####:  300:    if (!isFloatingPoint(arg))
    #####:  301:      llvm_unreachable("unsupported non numeric type");
call    0 never executed
    #####:  302:    OpBuilder builder = getBuilder();
    #####:  303:    switch (unaryFn) {
    #####:  304:    case UnaryFn::exp:
    #####:  305:      return builder.create<math::ExpOp>(arg.getLoc(), arg);
call    0 never executed
call    1 never executed
    #####:  306:    case UnaryFn::log:
    #####:  307:      return builder.create<math::LogOp>(arg.getLoc(), arg);
call    0 never executed
call    1 never executed
    #####:  308:    case UnaryFn::abs:
    #####:  309:      return builder.create<math::AbsFOp>(arg.getLoc(), arg);
call    0 never executed
call    1 never executed
    #####:  310:    case UnaryFn::ceil:
    #####:  311:      return builder.create<math::CeilOp>(arg.getLoc(), arg);
call    0 never executed
call    1 never executed
    #####:  312:    case UnaryFn::floor:
    #####:  313:      return builder.create<math::FloorOp>(arg.getLoc(), arg);
call    0 never executed
call    1 never executed
    #####:  314:    case UnaryFn::negf:
    #####:  315:      return builder.create<arith::NegFOp>(arg.getLoc(), arg);
call    0 never executed
call    1 never executed
        -:  316:    }
    #####:  317:    llvm_unreachable("unsupported unary function");
call    0 never executed
        -:  318:  }
        -:  319:
        -:  320:  // Build the binary functions defined by OpDSL.
function _ZN12_GLOBAL__N_119RegionBuilderHelper13buildBinaryFnEN4mlir6linalg8BinaryFnENS1_5ValueES4_ called 2974056 returned 100% blocks executed 40%
  2974056:  321:  Value buildBinaryFn(BinaryFn binaryFn, Value arg0, Value arg1) {
 8922168*:  322:    bool allComplex = isComplex(arg0) && isComplex(arg1);
 10580392:  323:    bool allFloatingPoint = isFloatingPoint(arg0) && isFloatingPoint(arg1);
 15357000:  324:    bool allInteger = isInteger(arg0) && isInteger(arg1);
  2710446:  325:    bool allBool = allInteger && arg0.getType().getIntOrFloatBitWidth() == 1 &&
call    0 returned 100%
branch  1 taken 26% (fallthrough)
branch  2 taken 74%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
   565502:  326:                   arg1.getType().getIntOrFloatBitWidth() == 1;
call    0 returned 100%
  2974056:  327:    if (!allComplex && !allFloatingPoint && !allInteger)
branch  0 taken 72% (fallthrough)
branch  1 taken 28%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####:  328:      llvm_unreachable("unsupported non numeric type");
call    0 never executed
  2974056:  329:    OpBuilder builder = getBuilder();
  2974056:  330:    switch (binaryFn) {
  1487028:  331:    case BinaryFn::add:
  1487028:  332:      if (allComplex)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  333:        return builder.create<complex::AddOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
  1487028:  334:      if (allFloatingPoint)
branch  0 taken 28% (fallthrough)
branch  1 taken 72%
   414556:  335:        return builder.create<arith::AddFOp>(arg0.getLoc(), arg0, arg1);
call    0 returned 100%
call    1 returned 100%
  1072472:  336:      if (allBool)
branch  0 taken 26% (fallthrough)
branch  1 taken 74%
   282751:  337:        return builder.create<arith::OrIOp>(arg0.getLoc(), arg0, arg1);
call    0 returned 100%
call    1 returned 100%
   789721:  338:      return builder.create<arith::AddIOp>(arg0.getLoc(), arg0, arg1);
call    0 returned 100%
call    1 returned 100%
    #####:  339:    case BinaryFn::sub:
    #####:  340:      if (allComplex)
branch  0 never executed
branch  1 never executed
    #####:  341:        return builder.create<complex::SubOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  342:      if (allFloatingPoint)
branch  0 never executed
branch  1 never executed
    #####:  343:        return builder.create<arith::SubFOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  344:      if (allBool)
branch  0 never executed
branch  1 never executed
    #####:  345:        llvm_unreachable("unsupported operation: sub with bools");
call    0 never executed
    #####:  346:      return builder.create<arith::SubIOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
  1487028:  347:    case BinaryFn::mul:
  1487028:  348:      if (allComplex)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  349:        return builder.create<complex::MulOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
  1487028:  350:      if (allFloatingPoint)
branch  0 taken 28% (fallthrough)
branch  1 taken 72%
   414556:  351:        return builder.create<arith::MulFOp>(arg0.getLoc(), arg0, arg1);
call    0 returned 100%
call    1 returned 100%
  1072472:  352:      if (allBool)
branch  0 taken 26% (fallthrough)
branch  1 taken 74%
   282751:  353:        return builder.create<arith::AndIOp>(arg0.getLoc(), arg0, arg1);
call    0 returned 100%
call    1 returned 100%
   789721:  354:      return builder.create<arith::MulIOp>(arg0.getLoc(), arg0, arg1);
call    0 returned 100%
call    1 returned 100%
    #####:  355:    case BinaryFn::max_signed:
    #####:  356:      assert(!allComplex);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  357:      if (allFloatingPoint)
branch  0 never executed
branch  1 never executed
    #####:  358:        return builder.create<arith::MaxFOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  359:      return builder.create<arith::MaxSIOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  360:    case BinaryFn::min_signed:
    #####:  361:      assert(!allComplex);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  362:      if (allFloatingPoint)
branch  0 never executed
branch  1 never executed
    #####:  363:        return builder.create<arith::MinFOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  364:      return builder.create<arith::MinSIOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  365:    case BinaryFn::max_unsigned:
    #####:  366:      assert(!allComplex);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  367:      if (allFloatingPoint)
branch  0 never executed
branch  1 never executed
    #####:  368:        return builder.create<arith::MaxFOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  369:      return builder.create<arith::MaxUIOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  370:    case BinaryFn::min_unsigned:
    #####:  371:      assert(!allComplex);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  372:      if (allFloatingPoint)
branch  0 never executed
branch  1 never executed
    #####:  373:        return builder.create<arith::MinFOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
    #####:  374:      return builder.create<arith::MinUIOp>(arg0.getLoc(), arg0, arg1);
call    0 never executed
call    1 never executed
        -:  375:    }
    #####:  376:    llvm_unreachable("unsupported binary function");
call    0 never executed
        -:  377:  }
        -:  378:
        -:  379:  // Build the type functions defined by OpDSL.
function _ZN12_GLOBAL__N_119RegionBuilderHelper11buildTypeFnEN4mlir6linalg6TypeFnENS1_4TypeENS1_5ValueE called 3157240 returned 100% blocks executed 60%
  3157240:  380:  Value buildTypeFn(TypeFn typeFn, Type toType, Value operand) {
  3157240:  381:    switch (typeFn) {
branch  0 taken 100%
branch  1 taken 0%
branch  2 taken 0%
  3157240:  382:    case TypeFn::cast_signed:
  3157240:  383:      return cast(toType, operand, false);
call    0 returned 100%
    #####:  384:    case TypeFn::cast_unsigned:
    #####:  385:      return cast(toType, operand, true);
call    0 never executed
        -:  386:    }
    #####:  387:    llvm_unreachable("unsupported type conversion function");
call    0 never executed
        -:  388:  }
        -:  389:
function _ZN12_GLOBAL__N_119RegionBuilderHelper12yieldOutputsEN4mlir10ValueRangeE called 1670212 returned 100% blocks executed 100%
  1670212:  390:  void yieldOutputs(ValueRange values) {
  1670212:  391:    OpBuilder builder = getBuilder();
  1670212:  392:    Location loc = builder.getUnknownLoc();
  1670212:  393:    builder.create<YieldOp>(loc, values);
call    0 returned 100%
  1670212:  394:  }
        -:  395:
function _ZN12_GLOBAL__N_119RegionBuilderHelper8constantERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE called 0 returned 0% blocks executed 0%
    #####:  396:  Value constant(const std::string &value) {
    #####:  397:    OpBuilder builder = getBuilder();
    #####:  398:    Location loc = builder.getUnknownLoc();
    #####:  399:    Attribute valueAttr = parseAttribute(value, builder.getContext());
call    0 never executed
    #####:  400:    Type type = NoneType::get(builder.getContext());
call    0 never executed
    #####:  401:    if (auto typedAttr = valueAttr.dyn_cast<TypedAttr>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  402:      type = typedAttr.getType();
call    0 never executed
    #####:  403:    return builder.create<arith::ConstantOp>(loc, type, valueAttr);
call    0 never executed
        -:  404:  }
        -:  405:
function _ZN12_GLOBAL__N_119RegionBuilderHelper5indexEl called 0 returned 0% blocks executed 0%
    #####:  406:  Value index(int64_t dim) {
    #####:  407:    OpBuilder builder = getBuilder();
    #####:  408:    return builder.create<IndexOp>(builder.getUnknownLoc(), dim);
call    0 never executed
        -:  409:  }
        -:  410:
    #####:  411:  Type getIntegerType(unsigned width) {
    #####:  412:    return IntegerType::get(context, width);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
        -:  413:  }
        -:  414:
        -:  415:  Type getFloat32Type() { return Float32Type::get(context); }
    #####:  416:  Type getFloat64Type() { return Float64Type::get(context); }
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
        -:  417:
        -:  418:private:
        -:  419:  // Generates operations to cast the given operand to a specified type.
        -:  420:  // If the cast cannot be performed, a warning will be issued and the
        -:  421:  // operand returned as-is (which will presumably yield a verification
        -:  422:  // issue downstream).
function _ZN12_GLOBAL__N_119RegionBuilderHelper4castEN4mlir4TypeENS1_5ValueEb called 3157240 returned 100% blocks executed 6%
  3157240:  423:  Value cast(Type toType, Value operand, bool isUnsignedCast) {
  3157240:  424:    OpBuilder builder = getBuilder();
  3157240:  425:    auto loc = operand.getLoc();
        -:  426:
  3157240:  427:    if (operand.getType() == toType)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
  3157240:  428:      return operand;
    #####:  429:    if (auto toIntType = toType.dyn_cast<IntegerType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  430:      // If operand is floating point, cast directly to the int type.
    #####:  431:      if (operand.getType().isa<FloatType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  432:        if (isUnsignedCast)
branch  0 never executed
branch  1 never executed
    #####:  433:          return builder.create<arith::FPToUIOp>(loc, toType, operand);
call    0 never executed
    #####:  434:        return builder.create<arith::FPToSIOp>(loc, toType, operand);
call    0 never executed
        -:  435:      }
        -:  436:      // Cast index operands directly to the int type.
    #####:  437:      if (operand.getType().isIndex())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  438:        return builder.create<arith::IndexCastOp>(loc, toType, operand);
call    0 never executed
    #####:  439:      if (auto fromIntType = operand.getType().dyn_cast<IntegerType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  440:        // Either extend or truncate.
    #####:  441:        if (toIntType.getWidth() > fromIntType.getWidth()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  442:          if (isUnsignedCast)
branch  0 never executed
branch  1 never executed
    #####:  443:            return builder.create<arith::ExtUIOp>(loc, toType, operand);
call    0 never executed
    #####:  444:          return builder.create<arith::ExtSIOp>(loc, toType, operand);
call    0 never executed
        -:  445:        }
    #####:  446:        if (toIntType.getWidth() < fromIntType.getWidth())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  447:          return builder.create<arith::TruncIOp>(loc, toType, operand);
call    0 never executed
        -:  448:      }
    #####:  449:    } else if (auto toFloatType = toType.dyn_cast<FloatType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  450:      // If operand is integer, cast directly to the float type.
        -:  451:      // Note that it is unclear how to cast from BF16<->FP16.
    #####:  452:      if (operand.getType().isa<IntegerType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  453:        if (isUnsignedCast)
branch  0 never executed
branch  1 never executed
    #####:  454:          return builder.create<arith::UIToFPOp>(loc, toFloatType, operand);
call    0 never executed
    #####:  455:        return builder.create<arith::SIToFPOp>(loc, toFloatType, operand);
call    0 never executed
        -:  456:      }
    #####:  457:      if (auto fromFloatType = operand.getType().dyn_cast<FloatType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  458:        if (toFloatType.getWidth() > fromFloatType.getWidth())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  459:          return builder.create<arith::ExtFOp>(loc, toFloatType, operand);
call    0 never executed
    #####:  460:        if (toFloatType.getWidth() < fromFloatType.getWidth())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  461:          return builder.create<arith::TruncFOp>(loc, toFloatType, operand);
call    0 never executed
        -:  462:      }
        -:  463:    }
        -:  464:
    #####:  465:    emitWarning(operand.getLoc()) << "could not cast operand of type "
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  466:                                  << operand.getType() << " to " << toType;
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  467:    return operand;
        -:  468:  }
        -:  469:
 2974056*:  470:  bool isComplex(Value value) { return value.getType().isa<ComplexType>(); }
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
branch  4 never executed
branch  5 never executed
 2974056*:  471:  bool isFloatingPoint(Value value) { return value.getType().isa<FloatType>(); }
call    0 returned 100%
branch  1 taken 28% (fallthrough)
branch  2 taken 72%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
call    6 never executed
branch  7 never executed
branch  8 never executed
  2974056:  472:  bool isInteger(Value value) { return value.getType().isa<IntegerType>(); }
call    0 returned 100%
branch  1 taken 72% (fallthrough)
branch  2 taken 28%
call    3 returned 100%
branch  4 taken 100% (fallthrough)
branch  5 taken 0%
        -:  473:
 7801508*:  474:  OpBuilder getBuilder() {
 7801508*:  475:    OpBuilder builder(context);
call    0 returned 100%
call    1 never executed
call    2 never executed
call    3 returned 100%
branch  4 taken 50%
branch  5 taken 0%
branch  6 taken 50%
branch  7 taken 0%
branch  8 taken 0%
branch  9 taken 0%
branch 10 taken 0%
branch 11 taken 0%
branch 12 never executed
branch 13 never executed
branch 14 never executed
branch 15 never executed
branch 16 never executed
branch 17 never executed
branch 18 never executed
 7801508*:  476:    builder.setInsertionPointToEnd(&block);
call    0 returned 100%
call    1 never executed
call    2 never executed
call    3 returned 100%
branch  4 taken 50%
branch  5 taken 0%
branch  6 taken 50%
branch  7 taken 0%
branch  8 taken 0%
branch  9 taken 0%
branch 10 taken 0%
branch 11 taken 0%
branch 12 never executed
branch 13 never executed
branch 14 never executed
branch 15 never executed
branch 16 never executed
branch 17 never executed
branch 18 never executed
 7801508*:  477:    return builder;
call    0 returned 100%
call    1 never executed
call    2 never executed
call    3 returned 100%
branch  4 taken 50%
branch  5 taken 0%
branch  6 taken 50%
branch  7 taken 0%
branch  8 taken 0%
branch  9 taken 0%
branch 10 taken 0%
branch 11 taken 0%
branch 12 never executed
branch 13 never executed
branch 14 never executed
branch 15 never executed
branch 16 never executed
branch 17 never executed
branch 18 never executed
        -:  478:  }
        -:  479:
        -:  480:  MLIRContext *context;
        -:  481:  Block &block;
        -:  482:};
        -:  483:
        -:  484:} // namespace
        -:  485:
        -:  486://===----------------------------------------------------------------------===//
        -:  487:// FillOp
        -:  488://===----------------------------------------------------------------------===//
        -:  489:
        -:  490:namespace {
        -:  491:
        -:  492:/// Fold linalg.fill -> tensor.expand/collapse_shape chain.
        -:  493:///
        -:  494:/// For such op chains, we can create new linalg.fill ops with the result
        -:  495:/// type of the tensor.expand/collapse_shape op.
        -:  496:template <typename TensorReshapeOp>
        -:  497:struct FoldFillWithTensorReshape : OpRewritePattern<TensorReshapeOp> {
        -:  498:  using OpRewritePattern<TensorReshapeOp>::OpRewritePattern;
     2602:  499:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -:  500:                                PatternRewriter &rewriter) const override {
     2602:  501:    auto oldFill = reshapeOp.getSrc().template getDefiningOp<FillOp>();
     2602:  502:    if (!oldFill)
     2602:  503:      return failure();
        -:  504:
    #####:  505:    Location loc = oldFill.getLoc();
    #####:  506:    auto newInit = rewriter.create<TensorReshapeOp>(
        -:  507:        loc, reshapeOp.getResultType(), oldFill.output(),
        -:  508:        reshapeOp.getReassociation());
    #####:  509:    rewriter.replaceOpWithNewOp<FillOp>(reshapeOp, ValueRange{oldFill.value()},
        -:  510:                                        ValueRange{newInit});
        -:  511:
     2602:  512:    return success();
        -:  513:  }
------------------
_ZNK12_GLOBAL__N_125FoldFillWithTensorReshapeIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_125FoldFillWithTensorReshapeIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 1206 returned 100% blocks executed 36%
     1206:  499:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -:  500:                                PatternRewriter &rewriter) const override {
     1206:  501:    auto oldFill = reshapeOp.getSrc().template getDefiningOp<FillOp>();
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
     1206:  502:    if (!oldFill)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     1206:  503:      return failure();
        -:  504:
    #####:  505:    Location loc = oldFill.getLoc();
call    0 never executed
    #####:  506:    auto newInit = rewriter.create<TensorReshapeOp>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  507:        loc, reshapeOp.getResultType(), oldFill.output(),
        -:  508:        reshapeOp.getReassociation());
    #####:  509:    rewriter.replaceOpWithNewOp<FillOp>(reshapeOp, ValueRange{oldFill.value()},
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  510:                                        ValueRange{newInit});
        -:  511:
     1206:  512:    return success();
        -:  513:  }
------------------
_ZNK12_GLOBAL__N_125FoldFillWithTensorReshapeIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_125FoldFillWithTensorReshapeIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 1396 returned 100% blocks executed 36%
     1396:  499:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -:  500:                                PatternRewriter &rewriter) const override {
     1396:  501:    auto oldFill = reshapeOp.getSrc().template getDefiningOp<FillOp>();
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
     1396:  502:    if (!oldFill)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     1396:  503:      return failure();
        -:  504:
    #####:  505:    Location loc = oldFill.getLoc();
call    0 never executed
    #####:  506:    auto newInit = rewriter.create<TensorReshapeOp>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  507:        loc, reshapeOp.getResultType(), oldFill.output(),
        -:  508:        reshapeOp.getReassociation());
    #####:  509:    rewriter.replaceOpWithNewOp<FillOp>(reshapeOp, ValueRange{oldFill.value()},
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  510:                                        ValueRange{newInit});
        -:  511:
     1396:  512:    return success();
        -:  513:  }
------------------
        -:  514:};
        -:  515:
        -:  516:/// Fold tensor.pad(linalg.fill) into linalg.fill if the padding value and the
        -:  517:/// filling value are the same.
        -:  518:struct FoldFillWithPad final : public OpRewritePattern<tensor::PadOp> {
        -:  519:  using OpRewritePattern::OpRewritePattern;
        -:  520:
function _ZNK12_GLOBAL__N_115FoldFillWithPad15matchAndRewriteEN4mlir6tensor5PadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  521:  LogicalResult matchAndRewrite(tensor::PadOp padOp,
        -:  522:                                PatternRewriter &rewriter) const override {
    #####:  523:    auto fillOp = padOp.getSource().getDefiningOp<linalg::FillOp>();
call    0 never executed
call    1 never executed
    #####:  524:    if (!fillOp)
branch  0 never executed
branch  1 never executed
    #####:  525:      return failure();
        -:  526:
        -:  527:    // We can only fold if the padding value is the same as the original
        -:  528:    // filling value.
    #####:  529:    Value padValue = padOp.getConstantPaddingValue();
call    0 never executed
    #####:  530:    if (!padValue || fillOp.value() != padValue)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  531:      return failure();
        -:  532:
    #####:  533:    ReifiedRankedShapedTypeDims reifiedShape;
call    0 never executed
    #####:  534:    ReifyRankedShapedTypeOpInterface interface =
    #####:  535:        cast<ReifyRankedShapedTypeOpInterface>(padOp.getOperation());
call    0 never executed
    #####:  536:    if (failed(interface.reifyResultShapes(rewriter, reifiedShape)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  537:      return rewriter.notifyMatchFailure(
    #####:  538:          padOp, "failed to reify tensor.pad op result shape");
call    0 never executed
        -:  539:
    #####:  540:    auto oldResultType = padOp.getResultType();
call    0 never executed
    #####:  541:    SmallVector<int64_t, 4> staticShape(oldResultType.getRank(),
call    0 never executed
    #####:  542:                                        ShapedType::kDynamicSize);
call    0 never executed
call    1 never executed
    #####:  543:    auto emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  544:        padOp.getLoc(), staticShape, oldResultType.getElementType(),
    #####:  545:        reifiedShape.front());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  546:    auto newFillOp = rewriter.create<FillOp>(
    #####:  547:        fillOp.getLoc(), ValueRange{padValue}, ValueRange{emptyTensor});
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  548:    rewriter.replaceOpWithNewOp<tensor::CastOp>(padOp, oldResultType,
    #####:  549:                                                newFillOp.result());
call    0 never executed
call    1 never executed
        -:  550:
    #####:  551:    return success();
branch  0 never executed
branch  1 never executed
        -:  552:  }
        -:  553:};
        -:  554:
        -:  555:/// Fold tensor.insert_slice(tensor.pad(<input>), linalg.fill) into
        -:  556:/// tensor.insert_slice(<input>, linalg.fill) if the padding value and the
        -:  557:/// filling value are the same.
        -:  558:struct FoldInsertPadIntoFill : public OpRewritePattern<tensor::InsertSliceOp> {
        -:  559:  using OpRewritePattern::OpRewritePattern;
        -:  560:
function _ZNK12_GLOBAL__N_121FoldInsertPadIntoFill15matchAndRewriteEN4mlir6tensor13InsertSliceOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  561:  LogicalResult matchAndRewrite(tensor::InsertSliceOp insertOp,
        -:  562:                                PatternRewriter &rewriter) const override {
    #####:  563:    auto srcPadOp = insertOp.getSource().getDefiningOp<tensor::PadOp>();
call    0 never executed
call    1 never executed
    #####:  564:    if (!srcPadOp)
branch  0 never executed
branch  1 never executed
    #####:  565:      return failure();
        -:  566:
    #####:  567:    if (insertOp.getType().getRank() != insertOp.getSourceType().getRank())
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  568:      return failure();
        -:  569:
        -:  570:    // Walk back the tensor.insert_slice chain and find the first destination
        -:  571:    // value at the start of the chain.
    #####:  572:    Value firstDest = insertOp.getDest();
call    0 never executed
    #####:  573:    while (auto prevOp = firstDest.getDefiningOp<tensor::InsertSliceOp>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  574:      if (prevOp.getType().getRank() != prevOp.getSourceType().getRank())
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  575:        return failure();
        -:  576:
        -:  577:      // Make sure the range of values accessed are disjoint. Without this, we
        -:  578:      // cannot fold tensor.pad away.
    #####:  579:      bool disjoint = false;
    #####:  580:      for (int i = 0, e = prevOp.getType().getRank(); i < e; ++i) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  581:        // If the dimension has dynamic offset/size, we cannot guarantee
        -:  582:        // disjoint. So just skip it.
    #####:  583:        if (insertOp.isDynamicOffset(i) || insertOp.isDynamicSize(i) ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  584:            insertOp.isDynamicStride(i) || prevOp.isDynamicOffset(i) ||
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  585:            prevOp.isDynamicSize(i) || prevOp.isDynamicStride(i))
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  586:          continue;
        -:  587:
        -:  588:        // Get the range start and end, inclusively for both.
    #####:  589:        int64_t prevStart = prevOp.getStaticOffset(i);
call    0 never executed
    #####:  590:        int64_t prevEnd = prevStart + (prevOp.getStaticSize(i) - 1) *
call    0 never executed
    #####:  591:                                          prevOp.getStaticStride(i);
call    0 never executed
    #####:  592:        int64_t nextStart = insertOp.getStaticOffset(i);
call    0 never executed
    #####:  593:        int64_t nextEnd = nextStart + (insertOp.getStaticSize(i) - 1) *
call    0 never executed
    #####:  594:                                          insertOp.getStaticStride(i);
call    0 never executed
    #####:  595:        if (prevEnd < nextStart || nextEnd < prevStart) {
branch  0 never executed
branch  1 never executed
        -:  596:          disjoint = true;
        -:  597:          break;
        -:  598:        }
        -:  599:      }
        -:  600:
    #####:  601:      if (!disjoint)
branch  0 never executed
branch  1 never executed
        -:  602:        break;
    #####:  603:      firstDest = prevOp.getDest();
call    0 never executed
    #####:  604:    }
        -:  605:
        -:  606:    // Check whether the first destination is a fill op. For overlapped cases,
        -:  607:    // this also cannot be true.
    #####:  608:    auto dstFillOp = firstDest.getDefiningOp<linalg::FillOp>();
call    0 never executed
    #####:  609:    if (!dstFillOp)
branch  0 never executed
branch  1 never executed
    #####:  610:      return failure();
        -:  611:
        -:  612:    // We can only fold if the padding value is the same as the original
        -:  613:    // filling value.
    #####:  614:    Value padValue = srcPadOp.getConstantPaddingValue();
call    0 never executed
    #####:  615:    if (!padValue || dstFillOp.value() != padValue)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  616:      return failure();
        -:  617:
    #####:  618:    SmallVector<OpFoldResult> lowPads = srcPadOp.getMixedLowPad();
call    0 never executed
    #####:  619:    SmallVector<OpFoldResult> oldOffsets = insertOp.getMixedOffsets();
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
        -:  620:
    #####:  621:    Location loc = insertOp.getLoc();
call    0 never executed
    #####:  622:    MLIRContext *context = getContext();
call    0 never executed
        -:  623:
    #####:  624:    AffineExpr sym0, sym1;
    #####:  625:    bindSymbols(context, sym0, sym1);
call    0 never executed
    #####:  626:    auto addMap = AffineMap::get(0, 2, {sym0 + sym1}, context);
call    0 never executed
call    1 never executed
        -:  627:
        -:  628:    // Calculate the new offsets for the insert. It should be the old offsets
        -:  629:    // plus low padding sizes.
    #####:  630:    SmallVector<OpFoldResult, 4> newOffsets;
branch  0 never executed
branch  1 never executed
    #####:  631:    for (const auto &p : llvm::zip(lowPads, oldOffsets)) {
branch  0 never executed
branch  1 never executed
    #####:  632:      newOffsets.push_back(makeComposedFoldedAffineApply(
call    0 never executed
    #####:  633:          rewriter, loc, addMap, {std::get<0>(p), std::get<1>(p)}));
call    0 never executed
        -:  634:    }
        -:  635:
    #####:  636:    SmallVector<OpFoldResult, 4> newSizes;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  637:    for (int i = 0, e = srcPadOp.getSourceType().getRank(); i < e; ++i) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  638:      newSizes.push_back(
call    0 never executed
    #####:  639:          rewriter.create<tensor::DimOp>(loc, srcPadOp.getSource(), i)
call    0 never executed
call    1 never executed
    #####:  640:              .getResult());
call    0 never executed
call    1 never executed
        -:  641:    }
        -:  642:
    #####:  643:    rewriter.replaceOpWithNewOp<tensor::InsertSliceOp>(
    #####:  644:        insertOp, srcPadOp.getSource(), insertOp.getDest(), newOffsets,
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  645:        newSizes, insertOp.getMixedStrides());
call    0 never executed
call    1 never executed
    #####:  646:    return success();
branch  0 never executed
branch  1 never executed
        -:  647:  }
        -:  648:};
        -:  649:
        -:  650:} // namespace
        -:  651:
function _ZN4mlir6linalg6FillOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1670 returned 100% blocks executed 100%
     1670:  652:void FillOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  653:                                         MLIRContext *context) {
     1670:  654:  results
        -:  655:      .add<FoldFillWithPad, FoldFillWithTensorReshape<tensor::CollapseShapeOp>,
        -:  656:           FoldFillWithTensorReshape<tensor::ExpandShapeOp>,
     1670:  657:           FoldInsertPadIntoFill>(context);
call    0 returned 100%
     1670:  658:}
        -:  659:
        -:  660://===----------------------------------------------------------------------===//
        -:  661:// GenericOp
        -:  662://===----------------------------------------------------------------------===//
        -:  663:
function _ZL18buildGenericRegionRN4mlir9OpBuilderERNS_14OperationStateENS_10ValueRangeES4_N4llvm12function_refIFvS1_NS_8LocationES4_EEE called 278994 returned 100% blocks executed 88%
   278994:  664:static void buildGenericRegion(
        -:  665:    OpBuilder &builder, OperationState &result, ValueRange inputs,
        -:  666:    ValueRange outputs,
        -:  667:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild) {
   278994:  668:  SmallVector<Type, 4> blockArgTypes;
   278994:  669:  SmallVector<Location, 4> blockArgLocs;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   836982:  670:  for (ValueRange container : {inputs, outputs}) {
branch  0 taken 67% (fallthrough)
branch  1 taken 33%
  2279454:  671:    for (Value v : container) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
   581739:  672:      blockArgTypes.push_back(getElementTypeOrSelf(v));
call    0 returned 100%
call    1 returned 100%
   581739:  673:      blockArgLocs.push_back(v.getLoc());
call    0 returned 100%
call    1 returned 100%
        -:  674:    }
        -:  675:  }
        -:  676:
   557988:  677:  OpBuilder::InsertionGuard guard(builder);
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
   278994:  678:  auto &region = *result.regions.front();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
   278994:  679:  Block *bodyBlock =
call    0 returned 100%
   278994:  680:      builder.createBlock(&region, region.end(), blockArgTypes, blockArgLocs);
call    0 returned 100%
call    1 returned 100%
   278994:  681:  bodyBuild(builder, result.location, bodyBlock->getArguments());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 34% (fallthrough)
branch  3 taken 66%
   278994:  682:}
        -:  683:
function _ZN4mlir6linalg9GenericOp24getAsmBlockArgumentNamesERNS_6RegionEN4llvm12function_refIFvNS_5ValueENS4_9StringRefEEEE called 29396370 returned 100% blocks executed 100%
 29396370:  684:void GenericOp::getAsmBlockArgumentNames(Region &region,
        -:  685:                                         OpAsmSetValueNameFn setNameFn) {
 80213507:  686:  for (Value v : getRegionInputArgs())
call    0 returned 100%
branch  1 taken 63% (fallthrough)
branch  2 taken 37%
 50817137:  687:    setNameFn(v, "in");
call    0 returned 100%
 58792740:  688:  for (Value v : getRegionOutputArgs())
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
 29396370:  689:    setNameFn(v, "out");
call    0 returned 100%
 29396370:  690:}
        -:  691:
function _ZN4mlir6linalg9GenericOp5buildERNS_9OpBuilderERNS_14OperationStateENS_9TypeRangeENS_10ValueRangeES7_NS_9ArrayAttrES8_NS_10StringAttrES9_N4llvm12function_refIFvS3_NS_8LocationES7_EEENSA_8ArrayRefINS_14NamedAttributeEEE called 96812 returned 100% blocks executed 100%
    96812:  692:void GenericOp::build(
        -:  693:    OpBuilder &builder, OperationState &result, TypeRange resultTensorTypes,
        -:  694:    ValueRange inputs, ValueRange outputs, ArrayAttr indexingMaps,
        -:  695:    ArrayAttr iteratorTypes, StringAttr doc, StringAttr libraryCall,
        -:  696:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -:  697:    ArrayRef<NamedAttribute> attributes) {
    96812:  698:  build(builder, result, resultTensorTypes, inputs, outputs, indexingMaps,
call    0 returned 100%
        -:  699:        iteratorTypes, doc, libraryCall);
    96812:  700:  result.addAttributes(attributes);
call    0 returned 100%
    96812:  701:  if (bodyBuild)
branch  0 taken 99% (fallthrough)
branch  1 taken 1%
    95810:  702:    buildGenericRegion(builder, result, inputs, outputs, bodyBuild);
call    0 returned 100%
    96812:  703:}
        -:  704:
function _ZN4mlir6linalg9GenericOp5buildERNS_9OpBuilderERNS_14OperationStateENS_9TypeRangeENS_10ValueRangeES7_N4llvm8ArrayRefINS_9AffineMapEEENS9_INS8_9StringRefEEESC_SC_NS8_12function_refIFvS3_NS_8LocationES7_EEENS9_INS_14NamedAttributeEEE called 96812 returned 100% blocks executed 67%
    96812:  705:void GenericOp::build(
        -:  706:    OpBuilder &builder, OperationState &result, TypeRange resultTensorTypes,
        -:  707:    ValueRange inputs, ValueRange outputs, ArrayRef<AffineMap> indexingMaps,
        -:  708:    ArrayRef<StringRef> iteratorTypes, StringRef doc, StringRef libraryCall,
        -:  709:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -:  710:    ArrayRef<NamedAttribute> attributes) {
   193624:  711:  build(builder, result, resultTensorTypes, inputs, outputs,
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -:  712:        builder.getAffineMapArrayAttr(indexingMaps),
        -:  713:        builder.getStrArrayAttr(iteratorTypes),
   96812*:  714:        doc.empty() ? StringAttr() : builder.getStringAttr(doc),
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 never executed
call    3 never executed
   96812*:  715:        libraryCall.empty() ? StringAttr() : builder.getStringAttr(libraryCall),
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 never executed
call    3 never executed
        -:  716:        bodyBuild, attributes);
    96812:  717:}
        -:  718:
function _ZN4mlir6linalg9GenericOp5buildERNS_9OpBuilderERNS_14OperationStateENS_10ValueRangeES6_N4llvm8ArrayRefINS_9AffineMapEEENS8_INS7_9StringRefEEESB_SB_NS7_12function_refIFvS3_NS_8LocationES6_EEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####:  719:void GenericOp::build(
        -:  720:    OpBuilder &builder, OperationState &result, ValueRange inputs,
        -:  721:    ValueRange outputs, ArrayRef<AffineMap> indexingMaps,
        -:  722:    ArrayRef<StringRef> iteratorTypes, StringRef doc, StringRef libraryCall,
        -:  723:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -:  724:    ArrayRef<NamedAttribute> attributes) {
    #####:  725:  build(builder, result, TypeRange{}, inputs, outputs, indexingMaps,
call    0 never executed
call    1 never executed
        -:  726:        iteratorTypes, doc, libraryCall, bodyBuild, attributes);
    #####:  727:}
        -:  728:
function _ZN4mlir6linalg9GenericOp5buildERNS_9OpBuilderERNS_14OperationStateENS_10ValueRangeES6_N4llvm8ArrayRefINS_9AffineMapEEENS8_INS7_9StringRefEEENS7_12function_refIFvS3_NS_8LocationES6_EEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####:  729:void GenericOp::build(
        -:  730:    OpBuilder &builder, OperationState &result, ValueRange inputs,
        -:  731:    ValueRange outputs, ArrayRef<AffineMap> indexingMaps,
        -:  732:    ArrayRef<StringRef> iteratorTypes,
        -:  733:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -:  734:    ArrayRef<NamedAttribute> attributes) {
    #####:  735:  build(builder, result, inputs, outputs, indexingMaps, iteratorTypes,
call    0 never executed
        -:  736:        /*doc=*/"",
        -:  737:        /*libraryCall=*/"", bodyBuild, attributes);
    #####:  738:}
        -:  739:
function _ZN4mlir6linalg9GenericOp5buildERNS_9OpBuilderERNS_14OperationStateENS_9TypeRangeENS_10ValueRangeES7_N4llvm8ArrayRefINS_9AffineMapEEENS9_INS8_9StringRefEEENS8_12function_refIFvS3_NS_8LocationES7_EEENS9_INS_14NamedAttributeEEE called 96812 returned 100% blocks executed 100%
    96812:  740:void GenericOp::build(
        -:  741:    OpBuilder &builder, OperationState &result, TypeRange resultTensorTypes,
        -:  742:    ValueRange inputs, ValueRange outputs, ArrayRef<AffineMap> indexingMaps,
        -:  743:    ArrayRef<StringRef> iteratorTypes,
        -:  744:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -:  745:    ArrayRef<NamedAttribute> attributes) {
    96812:  746:  build(builder, result, resultTensorTypes, inputs, outputs, indexingMaps,
call    0 returned 100%
        -:  747:        iteratorTypes,
        -:  748:        /*doc=*/"",
        -:  749:        /*libraryCall=*/"", bodyBuild, attributes);
    96812:  750:}
        -:  751:
function _ZN4mlir6linalg9GenericOp5printERNS_12OpAsmPrinterE called 1044048 returned 100% blocks executed 85%
  1044048:  752:void GenericOp::print(OpAsmPrinter &p) {
  1044048:  753:  p << " ";
call    0 returned 100%
        -:  754:
        -:  755:  // Print extra attributes.
  1044048:  756:  auto genericAttrNames = linalgTraitAttrNames();
call    0 returned 100%
        -:  757:
  2088096:  758:  llvm::StringSet<> genericAttrNamesSet;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
  1044048:  759:  genericAttrNamesSet.insert(genericAttrNames.begin(), genericAttrNames.end());
  2088096:  760:  SmallVector<NamedAttribute, 8> genericAttrs;
call    0 returned 100%
call    1 returned 100%
  4176192:  761:  for (auto attr : (*this)->getAttrs())
call    0 returned 100%
branch  1 taken 75% (fallthrough)
branch  2 taken 25%
  4176192:  762:    if (genericAttrNamesSet.count(attr.getName().strref()) > 0)
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
  2088096:  763:      genericAttrs.push_back(attr);
call    0 returned 100%
  1044048:  764:  if (!genericAttrs.empty()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
  1044048:  765:    auto genericDictAttr = DictionaryAttr::get(getContext(), genericAttrs);
call    0 returned 100%
call    1 returned 100%
  1044048:  766:    p << genericDictAttr;
call    0 returned 100%
        -:  767:  }
        -:  768:
        -:  769:  // Printing is shared with named ops, except for the region and attributes
  1044048:  770:  printCommonStructuredOpParts(p, SmallVector<Value>(getDpsInputOperands()),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
branch  6 taken 0% (fallthrough)
branch  7 taken 100%
branch  8 taken 0% (fallthrough)
branch  9 taken 100%
  2088096:  771:                               SmallVector<Value>(getDpsInitOperands()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
        -:  772:
  1044048:  773:  genericAttrNames.push_back("operand_segment_sizes");
call    0 returned 100%
  1044048:  774:  genericAttrNamesSet.insert(genericAttrNames.back());
call    0 returned 100%
call    1 returned 100%
        -:  775:
  1044048:  776:  bool hasExtraAttrs = false;
  4176192:  777:  for (NamedAttribute n : (*this)->getAttrs()) {
call    0 returned 100%
branch  1 taken 75% (fallthrough)
branch  2 taken 25%
  3132144:  778:    if ((hasExtraAttrs = !genericAttrNamesSet.contains(n.getName().strref())))
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
        -:  779:      break;
        -:  780:  }
  1044048:  781:  if (hasExtraAttrs) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  782:    p << " attrs = ";
call    0 never executed
    #####:  783:    p.printOptionalAttrDict((*this)->getAttrs(),
call    0 never executed
    #####:  784:                            /*elidedAttrs=*/genericAttrNames);
call    0 never executed
        -:  785:  }
        -:  786:
        -:  787:  // Print region.
  1044048:  788:  if (!getRegion().empty()) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
  1044048:  789:    p << ' ';
call    0 returned 100%
  1044048:  790:    p.printRegion(getRegion());
call    0 returned 100%
call    1 returned 100%
        -:  791:  }
        -:  792:
        -:  793:  // Print results.
  2087436:  794:  printNamedStructuredOpResults(p, getResultTensors().getTypes());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 1%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
  1044048:  795:}
        -:  796:
function _ZN4mlir6linalg9GenericOp5parseERNS_11OpAsmParserERNS_14OperationStateE called 658690 returned 100% blocks executed 68%
   658690:  797:ParseResult GenericOp::parse(OpAsmParser &parser, OperationState &result) {
   658690:  798:  DictionaryAttr dictAttr;
        -:  799:  // Parse the core linalg traits that must check into a dictAttr.
        -:  800:  // The name is unimportant as we will overwrite result.attributes.
        -:  801:  // The core linalg traits must contain the information necessary to pass the
        -:  802:  // verifier.
   658690:  803:  if (parser.parseAttribute(dictAttr, "_", result.attributes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  804:    return failure();
   658690:  805:  result.attributes.assign(dictAttr.getValue().begin(),
call    0 returned 100%
call    1 returned 100%
   658690:  806:                           dictAttr.getValue().end());
call    0 returned 100%
        -:  807:
        -:  808:  // Parsing is shared with named ops, except for the region.
  1317380:  809:  SmallVector<Type, 1> inputTypes, outputTypes;
call    0 returned 100%
branch  1 taken 57% (fallthrough)
branch  2 taken 43%
   658690:  810:  if (parseCommonStructuredOpParts(parser, result, inputTypes, outputTypes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  811:    return failure();
        -:  812:
        -:  813:  // Optional attributes may be added.
   658690:  814:  if (succeeded(parser.parseOptionalKeyword("attrs")))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  815:    if (failed(parser.parseEqual()) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  816:        failed(parser.parseOptionalAttrDict(result.attributes)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  817:      return failure();
        -:  818:
  1317380:  819:  std::unique_ptr<Region> region = std::make_unique<Region>();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
   658690:  820:  if (parser.parseRegion(*region, {}))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  821:    return failure();
   658690:  822:  result.addRegion(std::move(region));
call    0 returned 100%
        -:  823:
        -:  824:  // Generic ops may specify that a subset of its outputs are tensors. Such
        -:  825:  // outputs are specified in the result type.
        -:  826:  // TODO: may need to move output parsing before region parsing.
        -:  827:  // Need to wait for declarative assembly resolution to decide.
  1317380:  828:  SmallVector<Type, 1> outputTensorsTypes;
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
  1317380:  829:  if (parseNamedStructuredOpResults(parser, outputTensorsTypes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  830:    return failure();
   658690:  831:  result.addTypes(outputTensorsTypes);
call    0 returned 100%
        -:  832:
   658690:  833:  return success();
        -:  834:}
        -:  835:
        -:  836:static void getGenericEffectsImpl(
        -:  837:    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
        -:  838:        &effects,
        -:  839:    ValueRange results, OpOperandVector inputOperands,
        -:  840:    OpOperandVector outputOperands) {
        -:  841:  for (auto *operand : inputOperands) {
        -:  842:    if (!operand->get().getType().isa<MemRefType>())
        -:  843:      continue;
        -:  844:    effects.emplace_back(MemoryEffects::Read::get(), operand->get(),
        -:  845:                         SideEffects::DefaultResource::get());
        -:  846:  }
        -:  847:  for (auto *operand : outputOperands) {
        -:  848:    if (!operand->get().getType().isa<MemRefType>())
        -:  849:      continue;
        -:  850:    effects.emplace_back(MemoryEffects::Read::get(), operand->get(),
        -:  851:                         SideEffects::DefaultResource::get());
        -:  852:    effects.emplace_back(MemoryEffects::Write::get(), operand->get(),
        -:  853:                         SideEffects::DefaultResource::get());
        -:  854:  }
        -:  855:}
        -:  856:
function _ZN4mlir6linalg9GenericOp10getEffectsERN4llvm15SmallVectorImplINS_11SideEffects14EffectInstanceINS_13MemoryEffects6EffectEEEEE called 142828 returned 100% blocks executed 82%
   142828:  857:void GenericOp::getEffects(
        -:  858:    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
        -:  859:        &effects) {
   281766:  860:  getGenericEffectsImpl(effects, getOperation()->getResults(),
branch  0 taken 97% (fallthrough)
branch  1 taken 3%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
   285656:  861:                        getDpsInputOperands(), getDpsInitOperands());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
   142828:  862:}
        -:  863:
function _ZL17isResultValueDeadN4mlir6linalg9GenericOpENS_8OpResultE called 1 returned 100% blocks executed 9%
        1:  864:static bool isResultValueDead(linalg::GenericOp genericOp, OpResult result) {
        1:  865:  if (!result.use_empty())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  866:    return false;
        -:  867:  // If out operand not used in payload, we can drop it.
    #####:  868:  OpOperand *outputOpOperand =
call    0 never executed
    #####:  869:      genericOp.getDpsInitOperand(result.getResultNumber());
call    0 never executed
    #####:  870:  if (!genericOp.payloadUsesValueFromOperand(outputOpOperand))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  871:    return true;
        -:  872:
        -:  873:  // The out operand that is part of a payload can be dropped if
        -:  874:  // these conditions are met:
        -:  875:  // - Result from out operand is dead.
        -:  876:  // - User of arg is yield.
        -:  877:  // - outArg data is not being used by other outArgs.
        -:  878:
        -:  879:  // Check block arg and cycle from out operand has a single use.
    #####:  880:  BlockArgument outputArg =
    #####:  881:      genericOp.getRegionOutputArgs()[result.getResultNumber()];
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  882:  if (!outputArg.hasOneUse())
branch  0 never executed
branch  1 never executed
        -:  883:    return false;
    #####:  884:  Operation *argUserOp = *outputArg.user_begin();
branch  0 never executed
branch  1 never executed
        -:  885:
        -:  886:  // Check argUser has no other use.
    #####:  887:  if (!argUserOp->use_empty())
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  888:    return false;
        -:  889:
        -:  890:  // Check that argUser is a yield.
    #####:  891:  auto yieldOp = dyn_cast<linalg::YieldOp>(argUserOp);
call    0 never executed
    #####:  892:  if (!yieldOp)
branch  0 never executed
branch  1 never executed
        -:  893:    return false;
        -:  894:
        -:  895:  // Check outArg data is not being used by other outArgs.
    #####:  896:  if (yieldOp.getOperand(result.getResultNumber()) != outputArg)
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  897:    return false;
        -:  898:
        -:  899:  return true;
        -:  900:}
        -:  901:
function _ZN4mlir6linalg9GenericOp6verifyEv called 30724932 returned 100% blocks executed 100%
    #####:  902:LogicalResult GenericOp::verify() { return success(); }
        -:  903:
        -:  904:namespace {
        -:  905:
        -:  906:struct DeduplicateAndRemoveDeadOperandsAndResults
        -:  907:    : public OpRewritePattern<GenericOp> {
        -:  908:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -:  909:
function _ZNK12_GLOBAL__N_142DeduplicateAndRemoveDeadOperandsAndResults15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 3 returned 100% blocks executed 21%
        3:  910:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -:  911:                                PatternRewriter &rewriter) const override {
        -:  912:    // Create a map from argument position in the original op to the argument
        -:  913:    // position in the new op. If the argument is dropped it wont have an entry.
        3:  914:    SmallVector<OpOperand *> droppedOpOperands;
call    0 returned 100%
        -:  915:
        -:  916:    // Information needed to build the new op.
        3:  917:    SmallVector<Value> newInputOperands, newOutputOperands;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        3:  918:    SmallVector<AffineMap> newIndexingMaps;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  919:
        -:  920:    // Gather information about duplicate input operands.
        3:  921:    llvm::SmallDenseMap<unsigned, unsigned> origInsToNewInsPos =
        -:  922:        deduplicateInputOperands(genericOp, droppedOpOperands, newInputOperands,
        6:  923:                                 newIndexingMaps);
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -:  924:
        -:  925:    // Gather information about the dropped outputs.
        3:  926:    llvm::SmallDenseMap<unsigned, unsigned> origOutsToNewOutsPos =
        -:  927:        deduplicateOutputOperands(genericOp, droppedOpOperands,
        6:  928:                                  newOutputOperands, newIndexingMaps);
call    0 returned 100%
call    1 returned 100%
        -:  929:
        -:  930:    // Check if there is any change to operands.
        3:  931:    if (newInputOperands.size() + newOutputOperands.size() ==
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        6:  932:        genericOp->getNumOperands())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        3:  933:      return failure();
        -:  934:
        -:  935:    // Create the new op with the body being empty.
    #####:  936:    Location loc = genericOp.getLoc();
call    0 never executed
       3*:  937:    SmallVector<Type> newResultTypes;
call    0 never executed
call    1 returned 100%
    #####:  938:    if (genericOp.hasTensorSemantics()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  939:      newResultTypes = llvm::to_vector(llvm::map_range(
call    0 never executed
    #####:  940:          newOutputOperands, [](Value v) { return v.getType(); }));
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  941:    }
    #####:  942:    auto newOp = rewriter.create<GenericOp>(
        -:  943:        loc, newResultTypes, newInputOperands, newOutputOperands,
    #####:  944:        rewriter.getAffineMapArrayAttr(newIndexingMaps),
    #####:  945:        genericOp.getIteratorTypes(), genericOp.getDocAttr(),
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  946:        genericOp.getLibraryCallAttr(),
    #####:  947:        [](OpBuilder & /*builder*/, Location /*loc*/, ValueRange /*args*/) {
    #####:  948:          return;
    #####:  949:        });
call    0 never executed
call    1 never executed
        -:  950:    // Copy over unknown attributes. They might be load bearing for some flow.
    #####:  951:    ArrayRef<StringRef> odsAttrs = genericOp.getAttributeNames();
call    0 never executed
    #####:  952:    for (NamedAttribute kv : genericOp->getAttrs())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  953:      if (!llvm::is_contained(odsAttrs, kv.getName().getValue()))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  954:        newOp->setAttr(kv.getName(), kv.getValue());
call    0 never executed
call    1 never executed
        -:  955:
        -:  956:    // Fix up the payload of the canonicalized operation.
    #####:  957:    populateOpPayload(genericOp, newOp, origInsToNewInsPos,
call    0 never executed
        -:  958:                      origOutsToNewOutsPos, rewriter);
        -:  959:
        -:  960:    // Replace all live uses of the op.
    #####:  961:    SmallVector<Value> replacementsVals(genericOp->getNumResults(), nullptr);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  962:    for (const auto &result : llvm::enumerate(genericOp.getResults())) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
call    5 never executed
    #####:  963:      auto it = origOutsToNewOutsPos.find(result.index());
call    0 never executed
    #####:  964:      if (it == origOutsToNewOutsPos.end())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  965:        continue;
    #####:  966:      replacementsVals[result.index()] = newOp.getResult(it->second);
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -:  967:    }
    #####:  968:    rewriter.replaceOp(genericOp, replacementsVals);
call    0 never executed
call    1 never executed
    #####:  969:    return success();
branch  0 never executed
branch  1 never executed
        -:  970:  }
        -:  971:
        -:  972:private:
        -:  973:  // Deduplicate input operands, and return the
        -:  974:  // - Mapping from operand position in the original op, to operand position in
        -:  975:  // the canonicalized op.
        -:  976:  // - The preserved input operands list (by reference).
        -:  977:  llvm::SmallDenseMap<unsigned, unsigned>
        -:  978:  deduplicateInputOperands(GenericOp genericOp,
        -:  979:                           SmallVector<OpOperand *> &droppedOpOperands,
        -:  980:                           SmallVector<Value> &newInputOperands,
        -:  981:                           SmallVector<AffineMap> &newIndexingMaps) const {
        -:  982:    llvm::SmallDenseMap<unsigned, unsigned> origToNewPos;
        -:  983:    llvm::SmallDenseMap<std::pair<Value, AffineMap>, unsigned> dedupedInputs;
        -:  984:    for (const auto &en : llvm::enumerate(genericOp.getDpsInputOperands())) {
        -:  985:      OpOperand *inputOpOperand = en.value();
        -:  986:      // Check if operand is dead and if dropping the indexing map makes the
        -:  987:      // loops to shape computation invalid.
        -:  988:      if (!genericOp.payloadUsesValueFromOperand(inputOpOperand)) {
        -:  989:        // Add the current operands to the list of potentially droppable
        -:  990:        // operands. If it cannot be dropped, this needs to be popped back.
        -:  991:        droppedOpOperands.push_back(inputOpOperand);
        -:  992:        if (genericOp.canOpOperandsBeDropped(droppedOpOperands))
        -:  993:          continue;
        -:  994:        droppedOpOperands.pop_back();
        -:  995:      }
        -:  996:
        -:  997:      // Check if this operand is a duplicate.
        -:  998:      AffineMap indexingMap = genericOp.getMatchingIndexingMap(inputOpOperand);
        -:  999:      auto it = dedupedInputs.find(
        -: 1000:          std::make_pair(inputOpOperand->get(), indexingMap));
        -: 1001:      if (it != dedupedInputs.end()) {
        -: 1002:        origToNewPos[en.index()] = it->second;
        -: 1003:        droppedOpOperands.push_back(inputOpOperand);
        -: 1004:        continue;
        -: 1005:      }
        -: 1006:
        -: 1007:      // This is a preserved argument.
        -: 1008:      origToNewPos[en.index()] = newInputOperands.size();
        -: 1009:      dedupedInputs[{inputOpOperand->get(), indexingMap}] =
        -: 1010:          newInputOperands.size();
        -: 1011:      newInputOperands.push_back(inputOpOperand->get());
        -: 1012:      newIndexingMaps.push_back(indexingMap);
        -: 1013:    }
        -: 1014:    return origToNewPos;
        -: 1015:  }
        -: 1016:
        -: 1017:  // Deduplicate output operands, and return the
        -: 1018:  // - Mapping from operand position in the original op, to operand position in
        -: 1019:  // the canonicalized op.
        -: 1020:  // - The preserved output operands list (by reference).
        -: 1021:  llvm::SmallDenseMap<unsigned, unsigned>
        -: 1022:  deduplicateOutputOperands(GenericOp genericOp,
        -: 1023:                            SmallVector<OpOperand *> &droppedOpOperands,
        -: 1024:                            SmallVector<Value> &newOutputOperands,
        -: 1025:                            SmallVector<AffineMap> &newIndexingMaps) const {
        -: 1026:    llvm::SmallDenseMap<unsigned, unsigned> origToNewPos;
        -: 1027:    llvm::SmallDenseMap<std::tuple<Value, AffineMap, Value>, unsigned>
        -: 1028:        dedupedOutpts;
        -: 1029:    // If the op doesnt have tensor semantics, keep all the outputs as
        -: 1030:    // preserved.
        -: 1031:    if (!genericOp.hasTensorSemantics()) {
        -: 1032:      for (const auto &en : llvm::enumerate(genericOp.getDpsInitOperands())) {
        -: 1033:        origToNewPos[en.index()] = newOutputOperands.size();
        -: 1034:        newOutputOperands.push_back(en.value()->get());
        -: 1035:        newIndexingMaps.push_back(genericOp.getMatchingIndexingMap(en.value()));
        -: 1036:      }
        -: 1037:      return origToNewPos;
        -: 1038:    }
        -: 1039:    // Output argument can be dropped if the result has
        -: 1040:    // - no users, and
        -: 1041:    // - it is not used in the payload, and
        -: 1042:    // - the corresponding indexing maps are not needed for loop bound
        -: 1043:    //   computation.
        -: 1044:    auto yieldOp = cast<YieldOp>(genericOp.getBody()->getTerminator());
        -: 1045:    for (const auto &outputOpOperand :
        -: 1046:         llvm::enumerate(genericOp.getDpsInitOperands())) {
        -: 1047:      OpResult result = genericOp.getTiedOpResult(outputOpOperand.value());
        -: 1048:      AffineMap indexingMap =
        -: 1049:          genericOp.getMatchingIndexingMap(outputOpOperand.value());
        -: 1050:      auto key = std::make_tuple(outputOpOperand.value()->get(), indexingMap,
        -: 1051:                                 yieldOp->getOperand(outputOpOperand.index()));
        -: 1052:      if (isResultValueDead(genericOp, result)) {
        -: 1053:        // Check if the opoperand can be dropped without affecting loop
        -: 1054:        // bound computation. Add the operand to the list of dropped op
        -: 1055:        // operand for checking. If it cannot be dropped, need to pop the
        -: 1056:        // value back.
        -: 1057:        droppedOpOperands.push_back(outputOpOperand.value());
        -: 1058:        if (genericOp.canOpOperandsBeDropped(droppedOpOperands)) {
        -: 1059:          continue;
        -: 1060:        }
        -: 1061:        droppedOpOperands.pop_back();
        -: 1062:      }
        -: 1063:
        -: 1064:      if (!genericOp.payloadUsesValueFromOperand(outputOpOperand.value())) {
        -: 1065:        // The out operand can also be dropped if it is computed redundantly
        -: 1066:        // by another result, the conditions for that are
        -: 1067:        // - The same operand is used as the out operand
        -: 1068:        // - The same indexing map is used
        -: 1069:        // - The same yield value is used.
        -: 1070:        auto it = dedupedOutpts.find(key);
        -: 1071:        if (it != dedupedOutpts.end()) {
        -: 1072:          origToNewPos[outputOpOperand.index()] = it->second;
        -: 1073:          droppedOpOperands.push_back(outputOpOperand.value());
        -: 1074:          continue;
        -: 1075:        }
        -: 1076:      }
        -: 1077:
        -: 1078:      origToNewPos[outputOpOperand.index()] = newOutputOperands.size();
        -: 1079:      dedupedOutpts[key] = newOutputOperands.size();
        -: 1080:      newOutputOperands.push_back(outputOpOperand.value()->get());
        -: 1081:      newIndexingMaps.push_back(
        -: 1082:          genericOp.getMatchingIndexingMap(outputOpOperand.value()));
        -: 1083:    }
        -: 1084:    return origToNewPos;
        -: 1085:  }
        -: 1086:
        -: 1087:  // Populate the body of the canonicalized operation.
        -: 1088:  void populateOpPayload(
        -: 1089:      GenericOp genericOp, GenericOp newOp,
        -: 1090:      const llvm::SmallDenseMap<unsigned, unsigned> &origInsToNewInsPos,
        -: 1091:      const llvm::SmallDenseMap<unsigned, unsigned> &origOutsToNewOutsPos,
        -: 1092:      PatternRewriter &rewriter) const {
        -: 1093:    // Merge the body of the original op with the new op.
        -: 1094:    Block *newOpBlock = &newOp.getRegion().front();
        -: 1095:    assert(newOpBlock->empty() && "expected new op to have an empty payload");
        -: 1096:    Block *origOpBlock = &genericOp.getRegion().front();
        -: 1097:    SmallVector<Value> replacements(origOpBlock->getNumArguments(), nullptr);
        -: 1098:
        -: 1099:    // Replace all arguments in the original op, with arguments from the
        -: 1100:    // canonicalized op.
        -: 1101:    auto updateReplacements =
function _ZZNK12_GLOBAL__N_142DeduplicateAndRemoveDeadOperandsAndResults17populateOpPayloadEN4mlir6linalg9GenericOpES3_RKN4llvm13SmallDenseMapIjjLj4ENS4_12DenseMapInfoIjvEENS4_6detail12DenseMapPairIjjEEEESD_RNS1_15PatternRewriterEENKUlRNS1_15OpOperandVectorESH_SD_E_clESH_SH_SD_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1102:        [&](OpOperandVector &origOperands, OpOperandVector &newOperands,
        -: 1103:            const llvm::SmallDenseMap<unsigned, unsigned> &map) {
    #####: 1104:          for (const auto &origOperand : llvm::enumerate(origOperands)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1105:            auto it = map.find(origOperand.index());
call    0 never executed
    #####: 1106:            if (it == map.end())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1107:              continue;
    #####: 1108:            OpOperand *newOperand = newOperands[it->second];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1109:            replacements[origOperand.value()->getOperandNumber()] =
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1110:                newOpBlock->getArgument(newOperand->getOperandNumber());
call    0 never executed
call    1 never executed
        -: 1111:          }
    #####: 1112:        };
        -: 1113:
        -: 1114:    OpOperandVector origInputOperands = genericOp.getDpsInputOperands();
        -: 1115:    OpOperandVector newInputOperands = newOp.getDpsInputOperands();
        -: 1116:    updateReplacements(origInputOperands, newInputOperands, origInsToNewInsPos);
        -: 1117:
        -: 1118:    OpOperandVector origOutputOperands = genericOp.getDpsInitOperands();
        -: 1119:    OpOperandVector newOutputOperands = newOp.getDpsInitOperands();
        -: 1120:    updateReplacements(origOutputOperands, newOutputOperands,
        -: 1121:                       origOutsToNewOutsPos);
        -: 1122:
        -: 1123:    // Drop the unused yield args.
        -: 1124:    if (newOp.getNumDpsInits() != genericOp.getNumDpsInits()) {
        -: 1125:      OpBuilder::InsertionGuard g(rewriter);
        -: 1126:      YieldOp origYieldOp = cast<YieldOp>(origOpBlock->getTerminator());
        -: 1127:      rewriter.setInsertionPoint(origYieldOp);
        -: 1128:
        -: 1129:      SmallVector<Value> newYieldVals(newOp.getNumDpsInits(), nullptr);
        -: 1130:      for (const auto &yieldOpOperands :
        -: 1131:           llvm::enumerate(origYieldOp.getValues())) {
        -: 1132:        auto it = origOutsToNewOutsPos.find(yieldOpOperands.index());
        -: 1133:        if (it == origOutsToNewOutsPos.end())
        -: 1134:          continue;
        -: 1135:        newYieldVals[it->second] = yieldOpOperands.value();
        -: 1136:      }
        -: 1137:      rewriter.replaceOpWithNewOp<YieldOp>(origYieldOp, newYieldVals);
        -: 1138:    }
        -: 1139:
        -: 1140:    rewriter.mergeBlocks(origOpBlock, newOpBlock, replacements);
        -: 1141:  }
        -: 1142:};
        -: 1143:
        -: 1144:/// Remove generic operations (on tensors) that are just copying
        -: 1145:/// the values from inputs to the results. Requirements are
        -: 1146:/// 1) All iterator types are parallel
        -: 1147:/// 2) The body contains just a yield operation with the yielded values being
        -: 1148:///    the arguments corresponding to the operands.
        -: 1149:struct EraseIdentityGenericOp : public OpRewritePattern<GenericOp> {
        -: 1150:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -: 1151:
function _ZNK12_GLOBAL__N_122EraseIdentityGenericOp15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 3 returned 100% blocks executed 53%
        3: 1152:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -: 1153:                                PatternRewriter &rewriter) const override {
        -: 1154:    // Check all indexing maps are identity.
        3: 1155:    if (llvm::any_of(genericOp.getIndexingMapsArray(),
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
branch  4 taken 67% (fallthrough)
branch  5 taken 33%
        -: 1156:                     [](AffineMap map) { return !map.isIdentity(); }))
        2: 1157:      return failure();
        -: 1158:
        -: 1159:    // Check that the body of the linalg operation is just a linalg.yield
        -: 1160:    // operation.
        1: 1161:    Block &body = genericOp.getRegion().front();
call    0 returned 100%
call    1 returned 100%
       1*: 1162:    if (!llvm::hasSingleElement(body))
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    #####: 1163:      return failure();
        1: 1164:    auto yieldOp = dyn_cast<linalg::YieldOp>(body.getTerminator());
call    0 returned 100%
call    1 returned 100%
        1: 1165:    if (!yieldOp)
branch  0 taken 0%
branch  1 taken 100%
    #####: 1166:      return failure();
        -: 1167:
        -: 1168:    // In the buffer case, we need to check exact buffer equality.
        1: 1169:    if (genericOp.hasBufferSemantics()) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1170:      if (genericOp.getNumDpsInputs() == 1 && genericOp.getNumDpsInits() == 1 &&
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 1171:          genericOp.getDpsInputOperand(0)->get() ==
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1172:              genericOp.getDpsInitOperand(0)->get()) {
call    0 never executed
call    1 never executed
    #####: 1173:        rewriter.eraseOp(genericOp);
call    0 never executed
    #####: 1174:        return success();
        -: 1175:      }
    #####: 1176:      return failure();
        -: 1177:    }
        -: 1178:
        -: 1179:    // Mixed semantics is not supported yet.
        1: 1180:    if (!genericOp.hasTensorSemantics())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1181:      return failure();
        -: 1182:
        -: 1183:    // Get the argument number of the returned values. That is the operand
        -: 1184:    // number to use for replacing uses of this operation.
        1: 1185:    SmallVector<Value> returnedArgs;
call    0 returned 100%
        3: 1186:    for (const auto &yieldVal : llvm::enumerate(yieldOp.getValues())) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
call    4 returned 100%
        1: 1187:      auto yieldArg = yieldVal.value().dyn_cast<BlockArgument>();
call    0 returned 100%
        1: 1188:      if (!yieldArg || yieldArg.getOwner() != &body)
branch  0 taken 100%
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
    #####: 1189:        return failure();
        1: 1190:      unsigned argumentNumber = yieldArg.getArgNumber();
call    0 returned 100%
        1: 1191:      Value returnedArg = genericOp->getOperand(argumentNumber);
call    0 returned 100%
        2: 1192:      Type resultType = genericOp->getResult(yieldVal.index()).getType();
branch  0 taken 100%
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -: 1193:      // The input can have a different type than the result, e.g. a dynamic
        -: 1194:      // input dimension can be turned into a static output dimension.
        1: 1195:      Type returnType = returnedArg.getType();
        1: 1196:      if (returnType != resultType) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1197:        // Distinguish between sparse conversion or dense tensor casting.
        -: 1198:        // TODO: unify the two ops?
    #####: 1199:        if (sparse_tensor::getSparseTensorEncoding(returnType) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1200:            sparse_tensor::getSparseTensorEncoding(resultType))
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1201:          returnedArg = rewriter.create<sparse_tensor::ConvertOp>(
    #####: 1202:              genericOp.getLoc(), resultType, returnedArg);
call    0 never executed
        -: 1203:        else {
    #####: 1204:          if (!tensor::CastOp::areCastCompatible(returnedArg.getType(),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -: 1205:                                                 resultType))
    #####: 1206:            return failure();
    #####: 1207:          returnedArg = rewriter.create<tensor::CastOp>(
    #####: 1208:              genericOp.getLoc(), resultType, returnedArg);
call    0 never executed
        -: 1209:        }
        -: 1210:      }
        1: 1211:      returnedArgs.push_back(returnedArg);
call    0 returned 100%
        -: 1212:    }
        -: 1213:
        1: 1214:    if (returnedArgs.size() != genericOp->getNumResults())
branch  0 taken 0%
branch  1 taken 100%
    #####: 1215:      return failure();
        1: 1216:    rewriter.replaceOp(genericOp, returnedArgs);
call    0 returned 100%
call    1 returned 100%
        1: 1217:    return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1218:  }
        -: 1219:};
        -: 1220:
        -: 1221:/// Remove unused cycles.
        -: 1222:/// We can remove unused cycle within a payload of generic region
        -: 1223:/// if these conditions are met:
        -: 1224:/// - Result from out operand is dead.
        -: 1225:/// - Block arg from out operand has a single use in the %cycle
        -: 1226:/// instruction.
        -: 1227:/// - Cycle has a single use and it is in yield.
        -: 1228:struct RemoveUnusedCycleInGenericOp : public OpRewritePattern<GenericOp> {
        -: 1229:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -: 1230:
function _ZNK12_GLOBAL__N_128RemoveUnusedCycleInGenericOp15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 2 returned 100% blocks executed 9%
        2: 1231:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -: 1232:                                PatternRewriter &rewriter) const override {
        -: 1233:
        -: 1234:    // If the op doesnt have tensor semantics, preserve the outputs as is.
        2: 1235:    if (!genericOp.hasTensorSemantics())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
        2: 1236:      return failure();
        -: 1237:
    #####: 1238:    bool hasRemovedCycles = false;
        -: 1239:    // Iterate over output operands and remove any unused cycles.
    #####: 1240:    for (const auto &outputOpOperand :
    #####: 1241:         llvm::enumerate(genericOp.getDpsInitOperands())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
call    7 never executed
        -: 1242:
        -: 1243:      // Check that result from out operand is dead.
    #####: 1244:      Value result = genericOp.getResult(outputOpOperand.index());
branch  0 never executed
branch  1 never executed
    #####: 1245:      if (!result.use_empty())
branch  0 never executed
branch  1 never executed
    #####: 1246:        continue;
        -: 1247:
        -: 1248:      // Check that outputArg has one use in cycle.
    #####: 1249:      BlockArgument outputArg =
    #####: 1250:          genericOp.getRegionOutputArgs()[outputOpOperand.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1251:      if (!outputArg.hasOneUse())
branch  0 never executed
branch  1 never executed
    #####: 1252:        continue;
        -: 1253:
        -: 1254:      // Check cycle has at most one use.
    #####: 1255:      Operation *cycleOp = *outputArg.user_begin();
call    0 never executed
    #####: 1256:      if (!cycleOp->hasOneUse())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1257:        continue;
        -: 1258:
        -: 1259:      // Check that the cycleUser is a yield.
    #####: 1260:      Operation *cycleUserOp = *cycleOp->user_begin();
call    0 never executed
call    1 never executed
    #####: 1261:      if (!isa<linalg::YieldOp>(cycleUserOp))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1262:        continue;
        -: 1263:
        -: 1264:      // Check that argIndex matches yieldIndex, else data is being used.
    #####: 1265:      if (cycleUserOp->getOperand(outputOpOperand.index()) !=
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1266:          cycleOp->getResult(0))
call    0 never executed
    #####: 1267:        continue;
        -: 1268:
        -: 1269:      // Directly replace the cycle with the blockArg such that
        -: 1270:      // Deduplicate pattern can eliminate it along with unused yield.
    #####: 1271:      rewriter.replaceOp(cycleOp, outputArg);
call    0 never executed
call    1 never executed
    #####: 1272:      rewriter.updateRootInPlace(genericOp, [] {});
call    0 never executed
call    1 never executed
    #####: 1273:      hasRemovedCycles = true;
        -: 1274:    }
        -: 1275:
    #####: 1276:    if (hasRemovedCycles) {
branch  0 never executed
branch  1 never executed
    #####: 1277:      return success();
        -: 1278:    }
        -: 1279:
    #####: 1280:    return failure();
        -: 1281:  }
        -: 1282:};
        -: 1283:} // namespace
        -: 1284:
function _ZN4mlir6linalg9GenericOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1673 returned 100% blocks executed 100%
     1673: 1285:void GenericOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 1286:                                            MLIRContext *context) {
     1673: 1287:  results.add<DeduplicateAndRemoveDeadOperandsAndResults,
     1673: 1288:              EraseIdentityGenericOp, RemoveUnusedCycleInGenericOp>(context);
call    0 returned 100%
     1673: 1289:}
        -: 1290:
function _ZN4mlir6linalg9GenericOp4foldEN4llvm8ArrayRefINS_9AttributeEEERNS2_15SmallVectorImplINS_12OpFoldResultEEE called 125329 returned 100% blocks executed 100%
   125329: 1291:LogicalResult GenericOp::fold(ArrayRef<Attribute>,
        -: 1292:                              SmallVectorImpl<OpFoldResult> &) {
   125329: 1293:  return memref::foldMemRefCast(*this);
call    0 returned 100%
        -: 1294:}
        -: 1295:
        -: 1296://===----------------------------------------------------------------------===//
        -: 1297:// MapOp
        -: 1298://===----------------------------------------------------------------------===//
        -: 1299:
function _ZL15parseDstStyleOpRN4mlir11OpAsmParserERNS_14OperationStateEN4llvm12function_refIFNS_11ParseResultES1_RNS_13NamedAttrListEEEE called 940298 returned 100% blocks executed 77%
   940298: 1300:static ParseResult parseDstStyleOp(
        -: 1301:    OpAsmParser &parser, OperationState &result,
        -: 1302:    function_ref<ParseResult(OpAsmParser &, NamedAttrList &)> parseAttrsFn =
        -: 1303:        nullptr) {
        -: 1304:  // Parse `ins` and `outs`.
  1880596: 1305:  SmallVector<Type, 4> inputTypes, outputTypes;
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
   940298: 1306:  if (parseCommonStructuredOpParts(parser, result, inputTypes, outputTypes,
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   940298: 1307:                                   /*addOperandSegmentSizes=*/false))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1308:    return failure();
        -: 1309:
        -: 1310:  // Add result types.
  1880596: 1311:  for (Type outputType : outputTypes) {
branch  0 taken 50% (fallthrough)
branch  1 taken 50%
   940298: 1312:    if (outputType.isa<RankedTensorType>())
call    0 returned 100%
branch  1 taken 82% (fallthrough)
branch  2 taken 18%
   771283: 1313:      result.addTypes(outputType);
call    0 returned 100%
        -: 1314:  }
        -: 1315:
        -: 1316:  // Parse required attributes.
   940298: 1317:  if (parseAttrsFn && failed(parseAttrsFn(parser, result.attributes)))
branch  0 taken 39% (fallthrough)
branch  1 taken 61%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####: 1318:    return failure();
        -: 1319:
        -: 1320:  // Parse optional attributes.
   940298: 1321:  if (parser.parseOptionalAttrDict(result.attributes))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1322:    return failure();
   940298: 1323:  return success();
        -: 1324:}
        -: 1325:
function _ZN4mlir6linalg5MapOp24getAsmBlockArgumentNamesERNS_6RegionEN4llvm12function_refIFvNS_5ValueENS4_9StringRefEEEE called 21527542 returned 100% blocks executed 100%
 21527542: 1326:void MapOp::getAsmBlockArgumentNames(Region &region,
        -: 1327:                                     OpAsmSetValueNameFn setNameFn) {
 65057378: 1328:  for (Value v : getRegionInputArgs())
call    0 returned 100%
branch  1 taken 67% (fallthrough)
branch  2 taken 33%
 43529836: 1329:    setNameFn(v, "in");
call    0 returned 100%
 21527542: 1330:}
        -: 1331:
function _ZN4mlir6linalg5MapOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 21527542 returned 100% blocks executed 100%
 21527542: 1332:void MapOp::getAsmResultNames(function_ref<void(Value, StringRef)> setNameFn) {
 43055084: 1333:  if (!getResults().empty())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
 21527542: 1334:    setNameFn(getResults().front(), "mapped");
call    0 returned 100%
call    1 returned 100%
 21527542: 1335:}
        -: 1336:
function _ZN4mlir6linalg5MapOp5buildERNS_9OpBuilderERNS_14OperationStateENS_10ValueRangeENS_5ValueEN4llvm12function_refIFvS3_NS_8LocationES6_EEENS8_8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1337:void MapOp::build(
        -: 1338:    OpBuilder &builder, OperationState &result, ValueRange inputs, Value init,
        -: 1339:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -: 1340:    ArrayRef<NamedAttribute> attributes) {
    #####: 1341:  build(builder, result, TypeRange{}, inputs, init);
call    0 never executed
call    1 never executed
    #####: 1342:  result.addAttributes(attributes);
call    0 never executed
        -: 1343:
        -: 1344:  // Add output types for `RankedTensorType` output arguments.
    #####: 1345:  Type initType = init.getType();
call    0 never executed
    #####: 1346:  if (initType.isa<RankedTensorType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1347:    result.addTypes(initType);
call    0 never executed
        -: 1348:
    #####: 1349:  if (bodyBuild)
branch  0 never executed
branch  1 never executed
    #####: 1350:    buildGenericRegion(builder, result, inputs, /*outputs=*/{}, bodyBuild);
call    0 never executed
call    1 never executed
    #####: 1351:}
        -: 1352:
function _ZN4mlir6linalg5MapOp5parseERNS_11OpAsmParserERNS_14OperationStateE called 573930 returned 100% blocks executed 80%
   573930: 1353:ParseResult MapOp::parse(OpAsmParser &parser, OperationState &result) {
   573930: 1354:  if (parseDstStyleOp(parser, result))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1355:    return failure();
        -: 1356:
   573930: 1357:  SmallVector<OpAsmParser::Argument> regionArgs;
call    0 returned 100%
   573930: 1358:  if (parser.parseArgumentList(regionArgs, OpAsmParser::Delimiter::Paren,
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   573930: 1359:                               /*allowType=*/true, /*allowAttrs=*/true)) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1360:    return failure();
        -: 1361:  }
        -: 1362:
   573930: 1363:  Region *body = result.addRegion();
call    0 returned 100%
   573930: 1364:  if (parser.parseRegion(*body, regionArgs))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1365:    return failure();
        -: 1366:
   573930: 1367:  return success();
        -: 1368:}
        -: 1369:
function _ZN4mlir6linalg5MapOp5printERNS_12OpAsmPrinterE called 754692 returned 100% blocks executed 87%
   754692: 1370:void MapOp::print(OpAsmPrinter &p) {
   754692: 1371:  p.increaseIndent();
call    0 returned 100%
   754692: 1372:  printCommonStructuredOpPartsWithNewLine(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
  1509384: 1373:      p, SmallVector<Value>(getDpsInputOperands()),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
  1509384: 1374:      SmallVector<Value>(getDpsInitOperands()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
   754692: 1375:  p.printOptionalAttrDict((*this)->getAttrs());
call    0 returned 100%
call    1 returned 100%
        -: 1376:
   754692: 1377:  p.printNewline();
call    0 returned 100%
   754692: 1378:  p << "(";
call    0 returned 100%
   754692: 1379:  llvm::interleaveComma(getMapper().getArguments(), p,
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
  1537635: 1380:                        [&](auto arg) { p.printRegionArgument(arg); });
call    0 returned 100%
call    1 returned 100%
   754692: 1381:  p << ") ";
call    0 returned 100%
        -: 1382:
   754692: 1383:  p.printRegion(getMapper(), /*printEntryBlockArgs=*/false);
call    0 returned 100%
call    1 returned 100%
   754692: 1384:  p.decreaseIndent();
call    0 returned 100%
   754692: 1385:}
        -: 1386:
function _ZN4mlir6linalg5MapOp6verifyEv called 22585677 returned 100% blocks executed 47%
 22585677: 1387:LogicalResult MapOp::verify() {
 22585677: 1388:  auto *bodyBlock = getBody();
call    0 returned 100%
 22585677: 1389:  auto blockArgs = bodyBlock->getArguments();
call    0 returned 100%
        -: 1390:
        -: 1391:  // Checks if the number of `inputs` match the arity of the `mapper` region.
 22585677: 1392:  if (getInputs().size() != blockArgs.size())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1393:    return emitOpError() << "expects number of operands to match the arity of "
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1394:                            "mapper, but got: "
call    0 never executed
    #####: 1395:                         << getInputs().size() << " and " << blockArgs.size();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -: 1396:
        -: 1397:  // The parameters of mapper should all match the element type // of inputs.
 68281669: 1398:  for (const auto &[bbArgType, inputArg] :
call    0 returned 100%
 68281669: 1399:       llvm::zip(bodyBlock->getArgumentTypes(), getInputs())) {
call    0 returned 100%
branch  1 taken 67% (fallthrough)
branch  2 taken 33%
 45695992: 1400:    auto inputElemType = inputArg.getType().cast<ShapedType>().getElementType();
call    0 returned 100%
call    1 returned 100%
 45695992: 1401:    if (bbArgType != inputElemType) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1402:      return emitOpError() << "expected element type of input " << inputElemType
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1403:                           << " to match bbArg type " << bbArgType;
call    0 never executed
call    1 never executed
        -: 1404:    }
        -: 1405:  }
        -: 1406:
        -: 1407:  // The shape of each input must match the shape of the output.
 22585677: 1408:  auto outputShape = getInit().getType().cast<ShapedType>().getShape();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
 68281669: 1409:  for (Type inputArgType : TypeRange{getInputs()}) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 33% (fallthrough)
branch  3 taken 67%
call    4 returned 100%
 45695992: 1410:    auto inputElemShape = inputArgType.cast<ShapedType>().getShape();
call    0 returned 100%
call    1 returned 100%
 45695992: 1411:    if (inputElemShape != outputShape) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1412:      return emitOpError() << "expected shape of input (" << inputElemShape
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1413:                           << ") to match shape of output (" << outputShape
call    0 never executed
call    1 never executed
    #####: 1414:                           << ")";
call    0 never executed
        -: 1415:    }
        -: 1416:  }
        -: 1417:
 22585677: 1418:  return success();
        -: 1419:}
        -: 1420:
function _ZN4mlir6linalg5MapOp21getIteratorTypesArrayEv called 113577726 returned 100% blocks executed 100%
113577726: 1421:SmallVector<StringRef> MapOp::getIteratorTypesArray() {
113577726: 1422:  int64_t rank = getInit().getType().getRank();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
113577726: 1423:  return SmallVector<StringRef>(rank, getParallelIteratorTypeName());
call    0 returned 100%
        -: 1424:}
        -: 1425:
function _ZN4mlir6linalg5MapOp15getIndexingMapsEv called 204199525 returned 100% blocks executed 91%
204199525: 1426:ArrayAttr MapOp::getIndexingMaps() {
204199525: 1427:  Builder builder(getContext());
call    0 returned 100%
call    1 returned 100%
204199525: 1428:  int64_t rank = getInit().getType().getRank();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
204199525: 1429:  int64_t numIndexingMaps = getOperands().size();
call    0 returned 100%
call    1 returned 100%
204199525: 1430:  return builder.getAffineMapArrayAttr(SmallVector<AffineMap>(
call    0 returned 100%
call    1 returned 100%
408399050: 1431:      numIndexingMaps, builder.getMultiDimIdentityMap(rank)));
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -: 1432:}
        -: 1433:
function _ZN4mlir6linalg5MapOp10getEffectsERN4llvm15SmallVectorImplINS_11SideEffects14EffectInstanceINS_13MemoryEffects6EffectEEEEE called 121984 returned 100% blocks executed 82%
   121984: 1434:void MapOp::getEffects(
        -: 1435:    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
        -: 1436:        &effects) {
   243968: 1437:  getGenericEffectsImpl(effects, getOperation()->getResults(),
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
   243968: 1438:                        getDpsInputOperands(), getDpsInitOperands());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
   121984: 1439:}
        -: 1440:
        -: 1441://===----------------------------------------------------------------------===//
        -: 1442:// ReduceOp
        -: 1443://===----------------------------------------------------------------------===//
        -: 1444:
function _ZN4mlir6linalg8ReduceOp24getAsmBlockArgumentNamesERNS_6RegionEN4llvm12function_refIFvNS_5ValueENS4_9StringRefEEEE called 6337793 returned 100% blocks executed 100%
  6337793: 1445:void ReduceOp::getAsmBlockArgumentNames(Region &region,
        -: 1446:                                        OpAsmSetValueNameFn setNameFn) {
 12675586: 1447:  for (Value v : getRegionInputArgs())
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
  6337793: 1448:    setNameFn(v, "in");
call    0 returned 100%
 12675586: 1449:  for (Value v : getRegionOutputArgs())
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
  6337793: 1450:    setNameFn(v, "init");
call    0 returned 100%
  6337793: 1451:}
        -: 1452:
function _ZN4mlir6linalg8ReduceOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 6337793 returned 100% blocks executed 100%
  6337793: 1453:void ReduceOp::getAsmResultNames(
        -: 1454:    function_ref<void(Value, StringRef)> setNameFn) {
  9796402: 1455:  if (!getResults().empty())
branch  0 taken 55% (fallthrough)
branch  1 taken 45%
branch  2 taken 55% (fallthrough)
branch  3 taken 45%
  3458609: 1456:    setNameFn(getResults().front(), "reduced");
call    0 returned 100%
call    1 returned 100%
  6337793: 1457:}
        -: 1458:
function _ZN4mlir6linalg8ReduceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_10ValueRangeES6_N4llvm8ArrayRefIlEENS7_12function_refIFvS3_NS_8LocationES6_EEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1459:void ReduceOp::build(
        -: 1460:    OpBuilder &builder, OperationState &result, ValueRange inputs,
        -: 1461:    ValueRange inits, ArrayRef<int64_t> dimensions,
        -: 1462:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuild,
        -: 1463:    ArrayRef<NamedAttribute> attributes) {
    #####: 1464:  build(builder, result, TypeRange{}, inputs, inits, dimensions);
call    0 never executed
call    1 never executed
    #####: 1465:  result.addAttributes(attributes);
call    0 never executed
        -: 1466:
        -: 1467:  // Add output types for `RankedTensorType` output arguments.
    #####: 1468:  for (Value init : inits) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1469:    Type initType = init.getType();
call    0 never executed
    #####: 1470:    if (initType.isa<RankedTensorType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1471:      result.addTypes(initType);
call    0 never executed
        -: 1472:  }
        -: 1473:
    #####: 1474:  if (bodyBuild)
branch  0 never executed
branch  1 never executed
    #####: 1475:    buildGenericRegion(builder, result, inputs, inits, bodyBuild);
call    0 never executed
    #####: 1476:}
        -: 1477:
function _ZN4mlir6linalg8ReduceOp21getIteratorTypesArrayEv called 26867020 returned 100% blocks executed 86%
 26867020: 1478:SmallVector<StringRef> ReduceOp::getIteratorTypesArray() {
 26867020: 1479:  int64_t inputRank = getInputs()[0].getType().cast<ShapedType>().getRank();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
call    4 returned 100%
 26867020: 1480:  SmallVector<StringRef> iteratorTypes(inputRank,
call    0 returned 100%
 26867020: 1481:                                       getParallelIteratorTypeName());
call    0 returned 100%
 92513268: 1482:  for (int64_t reductionDim : getDimensions())
call    0 returned 100%
branch  1 taken 59% (fallthrough)
branch  2 taken 41%
 38779228: 1483:    iteratorTypes[reductionDim] = getReductionIteratorTypeName();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
 26867020: 1484:  return iteratorTypes;
        -: 1485:}
        -: 1486:
function _ZN4mlir6linalg8ReduceOp15getIndexingMapsEv called 47020089 returned 100% blocks executed 92%
 47020089: 1487:ArrayAttr ReduceOp::getIndexingMaps() {
 47020089: 1488:  int64_t inputRank = getInputs()[0].getType().cast<ShapedType>().getRank();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
call    4 returned 100%
 47020089: 1489:  SmallVector<AffineMap> affineMaps(
 47020089: 1490:      getNumDpsInputs(),
call    0 returned 100%
 94040178: 1491:      AffineMap::getMultiDimIdentityMap(inputRank, getContext()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
 47020089: 1492:  AffineMap resultMap =
 47020089: 1493:      AffineMap::getMultiDimIdentityMap(inputRank, getContext())
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
 47020089: 1494:          .dropResults(getDimensions());
call    0 returned 100%
 94040178: 1495:  for (int64_t i = 0, e = getNumDpsInits(); i < e; ++i)
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
 47020089: 1496:    affineMaps.push_back(resultMap);
call    0 returned 100%
 47020089: 1497:  return Builder(getContext()).getAffineMapArrayAttr(affineMaps);
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -: 1498:}
        -: 1499:
function _ZN4mlir6linalg8ReduceOp10getEffectsERN4llvm15SmallVectorImplINS_11SideEffects14EffectInstanceINS_13MemoryEffects6EffectEEEEE called 115868 returned 100% blocks executed 82%
   115868: 1500:void ReduceOp::getEffects(
        -: 1501:    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
        -: 1502:        &effects) {
   146835: 1503:  getGenericEffectsImpl(effects, getOperation()->getResults(),
branch  0 taken 27% (fallthrough)
branch  1 taken 73%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
   231736: 1504:                        getDpsInputOperands(), getDpsInitOperands());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
   115868: 1505:}
        -: 1506:
function _ZL22parseDenseI64ArrayAttrRN4mlir11OpAsmParserERNS_13NamedAttrListEN4llvm9StringRefE called 366368 returned 100% blocks executed 89%
   366368: 1507:static ParseResult parseDenseI64ArrayAttr(OpAsmParser &parser,
        -: 1508:                                          NamedAttrList &attributes,
        -: 1509:                                          StringRef attributeName) {
  366368*: 1510:  if (parser.parseKeyword(attributeName) || parser.parseEqual())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
    #####: 1511:    return failure();
        -: 1512:
   366368: 1513:  attributes.set(attributeName, DenseI64ArrayAttr::parse(parser, Type{}));
call    0 returned 100%
call    1 returned 100%
   366368: 1514:  return success();
        -: 1515:}
        -: 1516:
function _ZN4mlir6linalg8ReduceOp5parseERNS_11OpAsmParserERNS_14OperationStateE called 183184 returned 100% blocks executed 80%
   183184: 1517:ParseResult ReduceOp::parse(OpAsmParser &parser, OperationState &result) {
   183184: 1518:  if (parseDstStyleOp(
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   183184: 1519:          parser, result, [&](OpAsmParser &parser, NamedAttrList &attributes) {
   183184: 1520:            return parseDenseI64ArrayAttr(parser, attributes, "dimensions");
call    0 returned 100%
   183184: 1521:          }))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1522:    return failure();
        -: 1523:
   183184: 1524:  SmallVector<OpAsmParser::Argument> regionArgs;
call    0 returned 100%
   183184: 1525:  if (parser.parseArgumentList(regionArgs, OpAsmParser::Delimiter::Paren,
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   183184: 1526:                               /*allowType=*/true, /*allowAttrs=*/true)) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1527:    return failure();
        -: 1528:  }
        -: 1529:
   183184: 1530:  Region *body = result.addRegion();
call    0 returned 100%
   183184: 1531:  if (parser.parseRegion(*body, regionArgs))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1532:    return failure();
        -: 1533:
   183184: 1534:  return success();
        -: 1535:}
        -: 1536:
function _ZL22printDenseI64ArrayAttrRN4mlir12OpAsmPrinterEN4llvm9StringRefENS2_8ArrayRefIlEE called 558470 returned 100% blocks executed 100%
   558470: 1537:static void printDenseI64ArrayAttr(OpAsmPrinter &p, StringRef attributeName,
        -: 1538:                                   ArrayRef<int64_t> attributeValue) {
  1675410: 1539:  p << attributeName << " = [" << attributeValue << "] ";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
   558470: 1540:}
        -: 1541:
function _ZN4mlir6linalg8ReduceOp5printERNS_12OpAsmPrinterE called 281821 returned 100% blocks executed 89%
   281821: 1542:void ReduceOp::print(OpAsmPrinter &p) {
   281821: 1543:  p.increaseIndent();
call    0 returned 100%
   281821: 1544:  printCommonStructuredOpPartsWithNewLine(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
   563642: 1545:      p, SmallVector<Value>(getDpsInputOperands()),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
   563642: 1546:      SmallVector<Value>(getDpsInitOperands()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
   281821: 1547:  p.printNewline();
call    0 returned 100%
        -: 1548:
   563642: 1549:  printDenseI64ArrayAttr(p, getDimensionsAttrName(), getDimensions());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
   281821: 1550:  p.printOptionalAttrDict((*this)->getAttrs(), {getDimensionsAttrName()});
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -: 1551:
   281821: 1552:  p.printNewline();
call    0 returned 100%
   281821: 1553:  p << "(";
call    0 returned 100%
   281821: 1554:  llvm::interleaveComma(getCombiner().getArguments(), p,
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
   563642: 1555:                        [&](auto arg) { p.printRegionArgument(arg); });
call    0 returned 100%
call    1 returned 100%
   281821: 1556:  p << ") ";
call    0 returned 100%
        -: 1557:
   281821: 1558:  p.printRegion(getCombiner(), /*printEntryBlockArgs=*/false);
call    0 returned 100%
call    1 returned 100%
   281821: 1559:  p.decreaseIndent();
call    0 returned 100%
   281821: 1560:}
        -: 1561:
function _ZN4mlir6linalg8ReduceOp6verifyEv called 6718111 returned 100% blocks executed 39%
  6718111: 1562:LogicalResult ReduceOp::verify() {
  6718111: 1563:  ArrayRef<int64_t> dimensionsRef = getDimensions();
call    0 returned 100%
        -: 1564:
 6718111*: 1565:  for (int64_t i = 1; i < getNumDpsInputs(); ++i) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1566:    if (getInputs()[i].getType().cast<ShapedType>().getShape() !=
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1567:        getInputs()[0].getType().cast<ShapedType>().getShape()) {
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1568:      return emitOpError() << "expects all inputs to have the same shapes. "
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 1569:                              "Shape at input-index "
call    0 never executed
    #####: 1570:                           << i
call    0 never executed
    #####: 1571:                           << " is not equal to the shape at input-index 0.";
call    0 never executed
        -: 1572:    }
        -: 1573:  }
 6718111*: 1574:  for (int64_t i = 1; i < getNumDpsInits(); ++i) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1575:    if (getInits()[i].getType().cast<ShapedType>().getShape() !=
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1576:        getInits()[0].getType().cast<ShapedType>().getShape()) {
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1577:      return emitOpError() << "expects all outputs to have the same shapes. "
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 1578:                              "Shape at output-index "
call    0 never executed
    #####: 1579:                           << i
call    0 never executed
    #####: 1580:                           << " is not equal to the shape at output-index 0.";
call    0 never executed
        -: 1581:    }
        -: 1582:  }
  6718111: 1583:  auto inputType = getInputs()[0].getType().cast<ShapedType>();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
  6718111: 1584:  auto initType = getInits()[0].getType().cast<ShapedType>();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
        -: 1585:
  6718111: 1586:  DenseSet<int64_t> dimensionsToReduce;
call    0 returned 100%
 16414671: 1587:  for (int64_t dimension : dimensionsRef) {
branch  0 taken 59% (fallthrough)
branch  1 taken 41%
  9696560: 1588:    if (dimension < 0 || dimension >= inputType.getRank()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####: 1589:      return emitOpError()
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1590:             << "dimensions for reduction should be in the range [0, "
    #####: 1591:             << inputType.getRank() - 1 << "].";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -: 1592:    }
  9696560: 1593:    dimensionsToReduce.insert(dimension);
call    0 returned 100%
        -: 1594:  }
        -: 1595:
  6718111: 1596:  auto inputDims = inputType.getShape();
call    0 returned 100%
  6718111: 1597:  auto initDims = initType.getShape();
call    0 returned 100%
        -: 1598:
        -: 1599:  // Input dimensions that will be left after the reduction.
 13436222: 1600:  SmallVector<int64_t> reducedInputDims;
call    0 returned 100%
 19820462: 1601:  for (const auto &en : llvm::enumerate(inputDims)) {
branch  0 taken 66% (fallthrough)
branch  1 taken 34%
call    2 returned 100%
 22798911: 1602:    if (!dimensionsToReduce.count(en.index()))
call    0 returned 100%
  3405791: 1603:      reducedInputDims.push_back(en.value());
call    0 returned 100%
        -: 1604:  }
        -: 1605:
  6718111: 1606:  if (reducedInputDims.size() != static_cast<size_t>(initType.getRank())) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1607:    return emitOpError() << "number of dimensions after reduction "
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1608:                         << reducedInputDims.size()
call    0 never executed
call    1 never executed
    #####: 1609:                         << " doesn't match the init rank "
    #####: 1610:                         << initType.getRank();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1611:  }
        -: 1612:
  6718111: 1613:  if (reducedInputDims != initDims)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1614:    return emitOpError() << "init dimensions [" << initDims
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1615:                         << "] doesn't match input dimensions after reduction ["
call    0 never executed
    #####: 1616:                         << reducedInputDims << "]";
call    0 never executed
call    1 never executed
        -: 1617:
  6718111: 1618:  Block *block = getBody();
call    0 returned 100%
  6718111: 1619:  if (block->getNumArguments() != this->getNumOperands())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1620:    return emitOpError()
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 1621:           << "mismatching number of operands and block arguments";
call    0 never executed
        -: 1622:
        -: 1623:  // Check that the first block arguments match the element type of the inputs.
 13436222: 1624:  for (auto [input, bbArg] : llvm::zip(getInputs(), block->getArguments())) {
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
call    3 returned 100%
  6718111: 1625:    Type inputElementType = input.getType().cast<ShapedType>().getElementType();
call    0 returned 100%
call    1 returned 100%
  6718111: 1626:    if (inputElementType != bbArg.getType())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1627:      return emitOpError()
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1628:             << "input element type " << inputElementType
call    0 never executed
call    1 never executed
    #####: 1629:             << " does not match corresponding block argument type "
    #####: 1630:             << bbArg.getType();
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1631:  }
        -: 1632:
        -: 1633:  // Check that the last block arguments match the element type of the outputs.
 20154333: 1634:  for (auto [output, bbArg] :
call    0 returned 100%
call    1 returned 100%
 13436222: 1635:       llvm::zip(getDpsInitOperands(),
 26872444: 1636:                 block->getArguments().take_back(getNumDpsInits()))) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
branch  4 taken 100% (fallthrough)
branch  5 taken 0%
branch  6 taken 0% (fallthrough)
branch  7 taken 100%
branch  8 taken 50% (fallthrough)
branch  9 taken 50%
call   10 returned 100%
  6718111: 1637:    auto outputElementType =
  6718111: 1638:        output->get().getType().cast<ShapedType>().getElementType();
call    0 returned 100%
call    1 returned 100%
  6718111: 1639:    if (outputElementType != bbArg.getType())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1640:      return emitOpError()
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1641:             << "output element type " << outputElementType
call    0 never executed
call    1 never executed
    #####: 1642:             << " does not match corresponding block argument type "
    #####: 1643:             << bbArg.getType();
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1644:  }
  6718111: 1645:  return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1646:}
        -: 1647:
        -: 1648://===----------------------------------------------------------------------===//
        -: 1649:// TransposeOp
        -: 1650://===----------------------------------------------------------------------===//
        -: 1651:
        -: 1652:std::function<void(mlir::ImplicitLocOpBuilder &, mlir::Block &,
        -: 1653:                   mlir::ArrayRef<mlir::NamedAttribute>)>
function _ZN4mlir6linalg11TransposeOp16getRegionBuilderEv called 657 returned 100% blocks executed 100%
      657: 1654:TransposeOp::getRegionBuilder() {
function _ZZN4mlir6linalg11TransposeOp16getRegionBuilderEvENKUlRNS_20ImplicitLocOpBuilderERNS_5BlockEN4llvm8ArrayRefINS_14NamedAttributeEEEE_clES3_S5_S9_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1655:  return [](mlir::ImplicitLocOpBuilder &b, mlir::Block &block,
        -: 1656:            mlir::ArrayRef<mlir::NamedAttribute>) {
    #####: 1657:    b.create<linalg::YieldOp>(block.getArguments().front());
branch  0 never executed
branch  1 never executed
call    2 never executed
     657*: 1658:  };
        -: 1659:}
        -: 1660:
function _ZN4mlir6linalg11TransposeOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_NS_6detail18DenseArrayAttrImplIlEEN4llvm8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1661:void TransposeOp::build(::mlir::OpBuilder &builder,
        -: 1662:                        ::mlir::OperationState &result, Value input, Value init,
        -: 1663:                        DenseI64ArrayAttr permutation,
        -: 1664:                        ArrayRef<NamedAttribute> attributes) {
    #####: 1665:  result.addOperands(input);
call    0 never executed
call    1 never executed
    #####: 1666:  result.addOperands(init);
call    0 never executed
call    1 never executed
    #####: 1667:  result.addAttribute(getPermutationAttrName(result.name), permutation);
call    0 never executed
call    1 never executed
    #####: 1668:  result.addAttributes(attributes);
call    0 never executed
        -: 1669:
        -: 1670:  // Add output types for `RankedTensorType` output arguments.
    #####: 1671:  Type initType = init.getType();
call    0 never executed
    #####: 1672:  if (initType.isa<RankedTensorType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1673:    result.addTypes(initType);
call    0 never executed
        -: 1674:
    #####: 1675:  (void)result.addRegion();
call    0 never executed
    #####: 1676:  buildGenericRegion(builder, result, input, init,
call    0 never executed
call    1 never executed
call    2 never executed
function _ZZN4mlir6linalg11TransposeOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_NS_6detail18DenseArrayAttrImplIlEEN4llvm8ArrayRefINS_14NamedAttributeEEEENKUlS3_NS_8LocationENS_10ValueRangeEE_clES3_SE_SF_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1677:                     [&](OpBuilder &b, Location loc, ValueRange args) {
    #####: 1678:                       b.create<linalg::YieldOp>(loc, args[0]);
call    0 never executed
call    1 never executed
    #####: 1679:                     });
    #####: 1680:}
        -: 1681:
function _ZN4mlir6linalg11TransposeOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_N4llvm8ArrayRefIlEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1682:void TransposeOp::build(::mlir::OpBuilder &builder,
        -: 1683:                        ::mlir::OperationState &result, Value input, Value init,
        -: 1684:                        ArrayRef<int64_t> permutation,
        -: 1685:                        ArrayRef<NamedAttribute> attributes) {
    #####: 1686:  build(builder, result, input, init, builder.getDenseI64ArrayAttr(permutation),
call    0 never executed
call    1 never executed
        -: 1687:        attributes);
    #####: 1688:}
        -: 1689:
function _ZN4mlir6linalg11TransposeOp5parseERNS_11OpAsmParserERNS_14OperationStateE called 183184 returned 100% blocks executed 90%
   183184: 1690:ParseResult TransposeOp::parse(OpAsmParser &parser, OperationState &result) {
   366368: 1691:  if (failed(parseDstStyleOp(
   183184: 1692:          parser, result, [&](OpAsmParser &parser, NamedAttrList &attributes) {
   183184: 1693:            return parseDenseI64ArrayAttr(parser, attributes, "permutation");
call    0 returned 100%
   183184: 1694:          })))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1695:    return failure();
        -: 1696:
   183184: 1697:  (void)result.addRegion();
call    0 returned 100%
   183184: 1698:  OpBuilder builder(parser.getContext());
call    0 returned 100%
call    1 returned 100%
   183184: 1699:  buildGenericRegion(builder, result, /*inputs=*/result.operands,
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1700:                     /*outputs=*/{},
function _ZZN4mlir6linalg11TransposeOp5parseERNS_11OpAsmParserERNS_14OperationStateEENKUlRNS_9OpBuilderENS_8LocationENS_10ValueRangeEE0_clES7_S8_S9_.isra.0 called 183184 returned 100% blocks executed 100%
   183184: 1701:                     [&](OpBuilder &b, Location loc, ValueRange args) {
   183184: 1702:                       b.create<linalg::YieldOp>(loc, args[0]);
call    0 returned 100%
call    1 returned 100%
   183184: 1703:                     });
   183184: 1704:  return success();
        -: 1705:}
        -: 1706:
function _ZN4mlir6linalg11TransposeOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 6312351 returned 100% blocks executed 100%
  6312351: 1707:void TransposeOp::getAsmResultNames(
        -: 1708:    function_ref<void(Value, StringRef)> setNameFn) {
  9731550: 1709:  if (!getResults().empty())
branch  0 taken 54% (fallthrough)
branch  1 taken 46%
branch  2 taken 54% (fallthrough)
branch  3 taken 46%
  3419199: 1710:    setNameFn(getResults().front(), "transposed");
call    0 returned 100%
call    1 returned 100%
  6312351: 1711:}
        -: 1712:
function _ZN4mlir6linalg11TransposeOp5printERNS_12OpAsmPrinterE called 276649 returned 100% blocks executed 86%
   276649: 1713:void TransposeOp::print(OpAsmPrinter &p) {
   276649: 1714:  p.increaseIndent();
call    0 returned 100%
   276649: 1715:  printCommonStructuredOpPartsWithNewLine(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
   553298: 1716:      p, SmallVector<Value>(getDpsInputOperands()),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
   553298: 1717:      SmallVector<Value>(getDpsInitOperands()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
   276649: 1718:  p.printNewline();
call    0 returned 100%
        -: 1719:
   553298: 1720:  printDenseI64ArrayAttr(p, getPermutationAttrName(), getPermutation());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
   276649: 1721:  p.printOptionalAttrDict((*this)->getAttrs(), {getPermutationAttrName()});
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
   276649: 1722:  p.decreaseIndent();
call    0 returned 100%
   276649: 1723:}
        -: 1724:
function _ZN4mlir6linalg11TransposeOp6verifyEv called 6685877 returned 100% blocks executed 37%
  6685877: 1725:LogicalResult TransposeOp::verify() {
  6685877: 1726:  ArrayRef<int64_t> permutationRef = getPermutation();
call    0 returned 100%
        -: 1727:
  6685875: 1728:  if (!isPermutation(permutationRef))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1729:    return emitOpError("permutation is not valid");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1730:
  6685876: 1731:  auto inputType = getInput().getType();
call    0 returned 100%
call    1 returned 100%
  6685876: 1732:  auto initType = getInit().getType();
call    0 returned 100%
call    1 returned 100%
        -: 1733:
  6685877: 1734:  int64_t rank = inputType.getRank();
call    0 returned 100%
        -: 1735:
  6685877: 1736:  if (rank != initType.getRank())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1737:    return emitOpError() << "input rank " << rank
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1738:                         << " does not match init rank " << initType.getRank();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1739:
  6685875: 1740:  if (rank != static_cast<int64_t>(permutationRef.size()))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1741:    return emitOpError() << "size of permutation " << permutationRef.size()
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1742:                         << " does not match the argument rank " << rank;
call    0 never executed
call    1 never executed
        -: 1743:
  6685875: 1744:  auto inputDims = inputType.getShape();
call    0 returned 100%
  6685875: 1745:  auto initDims = initType.getShape();
call    0 returned 100%
        -: 1746:
 19832052: 1747:  for (int64_t i = 0; i < rank; ++i) {
branch  0 taken 66% (fallthrough)
branch  1 taken 34%
 13146177: 1748:    int64_t inputDim = inputDims[permutationRef[i]];
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
 13146177: 1749:    int64_t initDim = initDims[i];
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1750:
 13146177: 1751:    if (inputDim != initDim) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1752:      return emitOpError() << "dim(result, " << i << ") = " << initDim
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
    #####: 1753:                           << " doesn't match dim(input, permutation[" << i
call    0 never executed
call    1 never executed
    #####: 1754:                           << "]) = " << inputDim;
call    0 never executed
call    1 never executed
        -: 1755:    }
        -: 1756:  }
        -: 1757:
  6685875: 1758:  return success();
        -: 1759:}
        -: 1760:
function _ZN4mlir6linalg11TransposeOp21getIteratorTypesArrayEv called 26744072 returned 100% blocks executed 100%
 26744072: 1761:SmallVector<StringRef> TransposeOp::getIteratorTypesArray() {
 26744072: 1762:  int64_t rank = getInit().getType().getRank();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
 26744072: 1763:  return SmallVector<StringRef>(rank, getParallelIteratorTypeName());
call    0 returned 100%
        -: 1764:}
        -: 1765:
function _ZN4mlir6linalg11TransposeOp15getIndexingMapsEv called 46504461 returned 100% blocks executed 93%
 46504461: 1766:ArrayAttr TransposeOp::getIndexingMaps() {
 46504461: 1767:  Builder builder(getContext());
call    0 returned 100%
call    1 returned 100%
 46504461: 1768:  int64_t rank = getInit().getType().getRank();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
 46504461: 1769:  return builder.getAffineMapArrayAttr(
 46504461: 1770:      {builder.getMultiDimIdentityMap(rank),
call    0 returned 100%
        -: 1771:       AffineMap::getPermutationMap(
 93008910: 1772:           llvm::to_vector_of<unsigned>(getPermutation()), getContext())});
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
        -: 1773:}
        -: 1774:
function _ZN4mlir6linalg11TransposeOp10getEffectsERN4llvm15SmallVectorImplINS_11SideEffects14EffectInstanceINS_13MemoryEffects6EffectEEEEE called 110329 returned 100% blocks executed 82%
   110329: 1775:void TransposeOp::getEffects(
        -: 1776:    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
        -: 1777:        &effects) {
   136335: 1778:  getGenericEffectsImpl(effects, getOperation()->getResults(),
branch  0 taken 24% (fallthrough)
branch  1 taken 76%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
   220658: 1779:                        getDpsInputOperands(), getDpsInitOperands());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
   110329: 1780:}
        -: 1781:
        -: 1782://===----------------------------------------------------------------------===//
        -: 1783:// YieldOp
        -: 1784://===----------------------------------------------------------------------===//
        -: 1785:
function _ZN4mlir6linalg7YieldOp5printERNS_12OpAsmPrinterE called 2128052 returned 100% blocks executed 100%
  2128052: 1786:void linalg::YieldOp::print(OpAsmPrinter &p) {
  2128052: 1787:  if (getNumOperands() > 0)
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
  2128052: 1788:    p << ' ' << getOperands();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
  2128052: 1789:  p.printOptionalAttrDict((*this)->getAttrs());
call    0 returned 100%
call    1 returned 100%
  2128052: 1790:  if (getNumOperands() > 0)
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
  4256104: 1791:    p << " : " << getOperandTypes();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
  2128052: 1792:}
        -: 1793:
function _ZN4mlir6linalg7YieldOp5parseERNS_11OpAsmParserERNS_14OperationStateE called 1415804 returned 100% blocks executed 81%
  1415804: 1794:ParseResult YieldOp::parse(OpAsmParser &parser, OperationState &result) {
  1415804: 1795:  SmallVector<OpAsmParser::UnresolvedOperand, 2> opInfo;
call    0 returned 100%
  1415804: 1796:  SmallVector<Type, 2> types;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
  1415804: 1797:  SMLoc loc = parser.getCurrentLocation();
call    0 returned 100%
  2831608: 1798:  return failure(parser.parseOperandList(opInfo) ||
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
  1415804: 1799:                 parser.parseOptionalAttrDict(result.attributes) ||
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
  4247412: 1800:                 (!opInfo.empty() && parser.parseColonTypeList(types)) ||
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
call    4 returned 100%
branch  5 taken 100% (fallthrough)
branch  6 taken 0%
branch  7 taken 0% (fallthrough)
branch  8 taken 100%
branch  9 taken 0% (fallthrough)
branch 10 taken 100%
  1415804: 1801:                 parser.resolveOperands(opInfo, types, loc, result.operands));
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
        -: 1802:}
        -: 1803:
        -: 1804:// Check the operand number and types must match the element types of the
        -: 1805:// LinalgOp interface's shaped operands.
        -: 1806:static LogicalResult verifyYield(linalg::YieldOp op, LinalgOp linalgOp) {
        -: 1807:  if (op.getNumOperands() != linalgOp.getNumDpsInits())
        -: 1808:    return op.emitOpError("expected number of yield values (")
        -: 1809:           << linalgOp.getNumDpsInits()
        -: 1810:           << ") to match the number of operands of the enclosing "
        -: 1811:           << "LinalgOp (" << op.getNumOperands() << ")";
        -: 1812:
        -: 1813:  for (OpOperand &opOperand : op->getOpOperands()) {
        -: 1814:    OpOperand *outputOperand =
        -: 1815:        linalgOp.getDpsInitOperand(opOperand.getOperandNumber());
        -: 1816:    Type elementType = getElementTypeOrSelf(outputOperand->get().getType());
        -: 1817:    if (opOperand.get().getType() != elementType)
        -: 1818:      return op.emitOpError("type of yield operand ")
        -: 1819:             << (opOperand.getOperandNumber() + 1) << " ("
        -: 1820:             << opOperand.get().getType() << ") doesn't match "
        -: 1821:             << "the element type of the enclosing linalg.generic op ("
        -: 1822:             << elementType << ")";
        -: 1823:  }
        -: 1824:  return success();
        -: 1825:}
        -: 1826:
function _ZN4mlir6linalg7YieldOp6verifyEv called 132631619 returned 100% blocks executed 50%
132631619: 1827:LogicalResult linalg::YieldOp::verify() {
132631619: 1828:  auto *parentOp = (*this)->getParentOp();
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
132631619: 1829:  if (parentOp->getNumRegions() != 1 || parentOp->getRegion(0).empty())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####: 1830:    return emitOpError("expected single non-empty parent region");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1831:
132631619: 1832:  if (auto linalgOp = dyn_cast<LinalgOp>(parentOp))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
132631619: 1833:    return verifyYield(*this, linalgOp);
call    0 returned 100%
        -: 1834:
    #####: 1835:  return emitOpError("expected parent op with LinalgOp interface");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1836:}
        -: 1837:
        -: 1838://===----------------------------------------------------------------------===//
        -: 1839:// IndexOp
        -: 1840://===----------------------------------------------------------------------===//
        -: 1841:
function _ZN4mlir6linalg7IndexOp6verifyEv called 137046 returned 100% blocks executed 36%
   137046: 1842:LogicalResult IndexOp::verify() {
   274092: 1843:  auto linalgOp = dyn_cast<LinalgOp>((*this)->getParentOp());
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
   137046: 1844:  if (!linalgOp)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1845:    return emitOpError("expected parent op with LinalgOp interface");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
   137046: 1846:  if (linalgOp.getNumLoops() <= getDim())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####: 1847:    return emitOpError("expected dim (")
call    0 never executed
call    1 never executed
    #####: 1848:           << getDim() << ") to be lower than the number of loops ("
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1849:           << linalgOp.getNumLoops() << ") of the enclosing LinalgOp";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
   137046: 1850:  return success();
        -: 1851:}
        -: 1852:
        -: 1853://///// Operations corresponding to library calls defined with Tablegen ////////
        -: 1854:
        -: 1855:#include "mlir/Dialect/Linalg/IR/LinalgNamedStructuredOps.yamlgen.cpp.inc"
        -: 1856:
        -: 1857:#define GET_OP_CLASSES
        -: 1858:#include "mlir/Dialect/Linalg/IR/LinalgOps.cpp.inc"
        -: 1859:
        -: 1860:#define GET_OP_CLASSES
        -: 1861:#include "mlir/Dialect/Linalg/IR/LinalgStructuredOps.cpp.inc"
        -: 1862:
function _ZN4mlir6linalg20extractOrIdentityMapEN4llvm8OptionalINS_9AffineMapEEEjPNS_11MLIRContextE called 0 returned 0% blocks executed 0%
    #####: 1863:AffineMap mlir::linalg::extractOrIdentityMap(Optional<AffineMap> maybeMap,
        -: 1864:                                             unsigned rank,
        -: 1865:                                             MLIRContext *context) {
    #####: 1866:  if (maybeMap)
branch  0 never executed
branch  1 never executed
    #####: 1867:    return *maybeMap;
    #####: 1868:  if (rank == 0)
branch  0 never executed
branch  1 never executed
    #####: 1869:    return AffineMap::get(context);
call    0 never executed
    #####: 1870:  return AffineMap::getMultiDimIdentityMap(rank, context);
call    0 never executed
        -: 1871:}
        -: 1872:
        -: 1873:SmallVector<AffineExpr, 4>
function _ZN4mlir6linalg18makeAffineDimExprsEjRjPNS_11MLIRContextE called 0 returned 0% blocks executed 0%
    #####: 1874:mlir::linalg::makeAffineDimExprs(unsigned num, unsigned &startIdx,
        -: 1875:                                 MLIRContext *context) {
    #####: 1876:  SmallVector<AffineExpr, 4> res;
branch  0 never executed
branch  1 never executed
    #####: 1877:  res.reserve(num);
branch  0 never executed
branch  1 never executed
    #####: 1878:  for (unsigned i = 0; i < num; ++i)
branch  0 never executed
branch  1 never executed
    #####: 1879:    res.push_back(getAffineDimExpr(startIdx++, context));
call    0 never executed
call    1 never executed
    #####: 1880:  return res;
        -: 1881:}
        -: 1882:
function _ZN4mlir6linalg6concatEN4llvm8ArrayRefINS_10AffineExprEEES4_ called 0 returned 0% blocks executed 0%
    #####: 1883:SmallVector<AffineExpr, 4> mlir::linalg::concat(ArrayRef<AffineExpr> a,
        -: 1884:                                                ArrayRef<AffineExpr> b) {
    #####: 1885:  auto rangeA = llvm::make_range(a.begin(), a.end());
call    0 never executed
    #####: 1886:  auto rangeB = llvm::make_range(b.begin(), b.end());
    #####: 1887:  auto concatRanges = llvm::concat<const AffineExpr>(rangeA, rangeB);
call    0 never executed
    #####: 1888:  return llvm::to_vector<4>(concatRanges);
call    0 never executed
        -: 1889:}
        -: 1890:
function _ZN4mlir6linalg13isPermutationEN4llvm8ArrayRefIlEE called 6685875 returned 100% blocks executed 88%
  6685875: 1891:bool mlir::linalg::isPermutation(ArrayRef<int64_t> permutation) {
        -: 1892:  // Count the number of appearances for all indices.
  6685875: 1893:  SmallVector<int64_t> indexCounts(permutation.size(), 0);
call    0 returned 100%
 19832055: 1894:  for (auto index : permutation) {
branch  0 taken 66% (fallthrough)
branch  1 taken 34%
        -: 1895:    // Exit if the index is out-of-range.
 13146179: 1896:    if (index < 0 || index >= static_cast<int64_t>(permutation.size()))
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -: 1897:      return false;
 13146179: 1898:    ++indexCounts[index];
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1899:  }
        -: 1900:  // Return true if all indices appear once.
 13371752: 1901:  return count(indexCounts, 1) == static_cast<int64_t>(permutation.size());
        -: 1902:}
        -: 1903:
function _ZL17appendMangledTypeRN4llvm18raw_string_ostreamEN4mlir4TypeE called 0 returned 0% blocks executed 0%
    #####: 1904:static void appendMangledType(llvm::raw_string_ostream &ss, Type t) {
    #####: 1905:  if (auto memref = t.dyn_cast<MemRefType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1906:    ss << "view";
call    0 never executed
    #####: 1907:    for (auto size : memref.getShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1908:      if (size < 0)
branch  0 never executed
branch  1 never executed
    #####: 1909:        ss << "sx";
call    0 never executed
        -: 1910:      else
    #####: 1911:        ss << size << "x";
call    0 never executed
call    1 never executed
    #####: 1912:    appendMangledType(ss, memref.getElementType());
call    0 never executed
call    1 never executed
    #####: 1913:  } else if (auto vec = t.dyn_cast<VectorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1914:    ss << "vector";
call    0 never executed
    #####: 1915:    llvm::interleave(
    #####: 1916:        vec.getShape(), [&](int64_t i) { ss << i; }, [&]() { ss << "x"; });
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1917:    appendMangledType(ss, vec.getElementType());
call    0 never executed
call    1 never executed
    #####: 1918:  } else if (t.isSignlessIntOrIndexOrFloat()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1919:    ss << t;
call    0 never executed
        -: 1920:  } else {
    #####: 1921:    llvm_unreachable("Invalid type for linalg library name mangling");
call    0 never executed
        -: 1922:  }
    #####: 1923:}
        -: 1924:
function _ZN4mlir6linalg23generateLibraryCallNameB5cxx11EPNS_9OperationE called 0 returned 0% blocks executed 0%
    #####: 1925:std::string mlir::linalg::generateLibraryCallName(Operation *op) {
    #####: 1926:  assert(isa<LinalgOp>(op));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 1927:  std::string name(op->getName().getStringRef().str());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1928:  name.reserve(128);
call    0 never executed
    #####: 1929:  std::replace(name.begin(), name.end(), '.', '_');
    #####: 1930:  llvm::raw_string_ostream ss(name);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1931:  ss << "_";
call    0 never executed
    #####: 1932:  auto types = op->getOperandTypes();
call    0 never executed
    #####: 1933:  llvm::interleave(
call    0 never executed
    #####: 1934:      types.begin(), types.end(), [&](Type t) { appendMangledType(ss, t); },
call    0 never executed
call    1 never executed
    #####: 1935:      [&]() { ss << "_"; });
call    0 never executed
    #####: 1936:  return ss.str();
call    0 never executed
call    1 never executed
        -: 1937:}
        -: 1938:
        -: 1939://===----------------------------------------------------------------------===//
        -: 1940:// Canonicalizers and Folders.
        -: 1941://===----------------------------------------------------------------------===//
        -: 1942:
        -: 1943:namespace {
        -: 1944:struct EraseDeadLinalgOp : public OpInterfaceRewritePattern<LinalgOp> {
        -: 1945:  using OpInterfaceRewritePattern<LinalgOp>::OpInterfaceRewritePattern;
        -: 1946:
function _ZNK12_GLOBAL__N_117EraseDeadLinalgOp15matchAndRewriteEN4mlir6linalg8LinalgOpERNS1_15PatternRewriterE called 10707 returned 100% blocks executed 85%
    10707: 1947:  LogicalResult matchAndRewrite(LinalgOp op,
        -: 1948:                                PatternRewriter &rewriter) const override {
    33489: 1949:    for (OpOperand &opOperand : op->getOpOperands()) {
call    0 returned 100%
branch  1 taken 68% (fallthrough)
branch  2 taken 32%
        -: 1950:      // Linalg "inputs" may be either tensor or memref type.
        -: 1951:      // tensor<0xelt_type> is a convention that may not always mean
        -: 1952:      // "0 iterations". Only erase in cases we see memref<...x0x...>.
    22782: 1953:      auto mt = opOperand.get().getType().dyn_cast<MemRefType>();
call    0 returned 100%
    22782: 1954:      if (!mt)
branch  0 taken 61% (fallthrough)
branch  1 taken 39%
    13983: 1955:        continue;
     8799: 1956:      if (llvm::is_contained(op.getShape(&opOperand), 0)) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 1957:        rewriter.eraseOp(op);
call    0 never executed
    #####: 1958:        return success();
        -: 1959:      }
        -: 1960:    }
    10707: 1961:    return failure();
        -: 1962:  }
        -: 1963:};
        -: 1964:
        -: 1965:struct FoldTensorCastProducerOp : public OpInterfaceRewritePattern<LinalgOp> {
        -: 1966:  using OpInterfaceRewritePattern<LinalgOp>::OpInterfaceRewritePattern;
        -: 1967:
function _ZNK12_GLOBAL__N_124FoldTensorCastProducerOp15matchAndRewriteEN4mlir6linalg8LinalgOpERNS1_15PatternRewriterE called 10707 returned 100% blocks executed 69%
    10707: 1968:  LogicalResult matchAndRewrite(LinalgOp op,
        -: 1969:                                PatternRewriter &rewriter) const override {
        -: 1970:    // If no operand comes from a tensor::CastOp and can be folded then fail.
    10707: 1971:    bool hasTensorCastOperand =
function _ZZNK12_GLOBAL__N_124FoldTensorCastProducerOp15matchAndRewriteEN4mlir6linalg8LinalgOpERNS1_15PatternRewriterEENKUlRNS1_9OpOperandEE_clES7_.isra.0 called 22769 returned 100% blocks executed 88%
    10707: 1972:        llvm::any_of(op->getOpOperands(), [&](OpOperand &opOperand) {
call    0 returned 100%
call    1 returned 100%
    22769: 1973:          if (opOperand.get().isa<BlockArgument>())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
        -: 1974:            return false;
    22769: 1975:          auto castOp = opOperand.get().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
    22769: 1976:          return castOp && canFoldIntoConsumerOp(castOp);
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
        -: 1977:        });
    10707: 1978:    if (!hasTensorCastOperand)
branch  0 taken 100% (fallthrough)
branch  1 taken 1%
    10694: 1979:      return failure();
        -: 1980:
       13: 1981:    SmallVector<Type, 4> newResultTypes;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
       13: 1982:    newResultTypes.reserve(op->getNumResults());
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
       26: 1983:    SmallVector<Value, 4> newOperands;
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
       26: 1984:    newOperands.reserve(op->getNumOperands());
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -: 1985:    // Inputs may fold.
       39: 1986:    for (auto *input : op.getDpsInputOperands()) {
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
       13: 1987:      auto tensorCastOp = input->get().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
       26: 1988:      newOperands.push_back(canFoldIntoConsumerOp(tensorCastOp)
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
       26: 1989:                                ? tensorCastOp.getSource()
call    0 returned 100%
    #####: 1990:                                : input->get());
        -: 1991:    }
        -: 1992:    // Init tensors may fold, in which case the resultType must also change.
       39: 1993:    for (auto *output : op.getDpsInitOperands()) {
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
       13: 1994:      auto tensorCastOp = output->get().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
       13: 1995:      bool fold = canFoldIntoConsumerOp(tensorCastOp);
call    0 returned 100%
       13: 1996:      newOperands.push_back(fold ? tensorCastOp.getOperand() : output->get());
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
       13: 1997:      newResultTypes.push_back(newOperands.back().getType());
call    0 returned 100%
call    1 returned 100%
        -: 1998:    }
        -: 1999:    // Clone op.
       13: 2000:    Operation *newOp =
call    0 returned 100%
       13: 2001:        op.clone(rewriter, op->getLoc(), newResultTypes, newOperands);
call    0 returned 100%
call    1 returned 100%
       26: 2002:    SmallVector<Value, 4> replacements;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
       13: 2003:    replacements.reserve(newOp->getNumResults());
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
      26*: 2004:    for (auto result : llvm::zip(op->getResults(), newOp->getResults())) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
    #####: 2005:      Value oldResult = std::get<0>(result);
branch  0 never executed
branch  1 never executed
    #####: 2006:      Value newResult = std::get<1>(result);
branch  0 never executed
branch  1 never executed
    #####: 2007:      if (newResult.getType() != oldResult.getType()) {
branch  0 never executed
branch  1 never executed
    #####: 2008:        replacements.push_back(rewriter.create<tensor::CastOp>(
call    0 never executed
    #####: 2009:            op->getLoc(), oldResult.getType(), newResult));
call    0 never executed
call    1 never executed
        -: 2010:      } else {
    #####: 2011:        replacements.push_back(newResult);
call    0 never executed
        -: 2012:      }
        -: 2013:    }
       13: 2014:    rewriter.replaceOp(op, replacements);
call    0 returned 100%
call    1 returned 100%
        -: 2015:
       13: 2016:    return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 2017:  }
        -: 2018:};
        -: 2019:
        -: 2020:/// Fold LinalgOps with `tensor.cast` consumer if the `tensor.cast` has
        -: 2021:/// result that is more static than the linalg op.
        -: 2022:struct FoldTensorCastConsumerOp : public OpRewritePattern<tensor::CastOp> {
        -: 2023:  using OpRewritePattern<tensor::CastOp>::OpRewritePattern;
        -: 2024:
function _ZNK12_GLOBAL__N_124FoldTensorCastConsumerOp15matchAndRewriteEN4mlir6tensor6CastOpERNS1_15PatternRewriterE called 2341 returned 100% blocks executed 6%
     2341: 2025:  LogicalResult matchAndRewrite(tensor::CastOp castOp,
        -: 2026:                                PatternRewriter &rewriter) const override {
     2341: 2027:    if (!tensor::canFoldIntoProducerOp(castOp))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
     2341: 2028:      return failure();
        -: 2029:
    #####: 2030:    auto linalgOp = castOp.getSource().getDefiningOp<LinalgOp>();
call    0 never executed
call    1 never executed
    #####: 2031:    if (!linalgOp)
branch  0 never executed
branch  1 never executed
    #####: 2032:      return failure();
        -: 2033:
        -: 2034:    // Cast can be in conditionally reachable region, if which case folding will
        -: 2035:    // generate invalid code. Only conservatively fold ops in same block for
        -: 2036:    // now.
    #####: 2037:    if (castOp->getBlock() != linalgOp->getBlock())
branch  0 never executed
branch  1 never executed
    #####: 2038:      return failure();
        -: 2039:
    #####: 2040:    OpBuilder::InsertionGuard guard(rewriter);
call    0 never executed
    #####: 2041:    rewriter.setInsertionPoint(linalgOp);
call    0 never executed
        -: 2042:
    #####: 2043:    Location loc = linalgOp.getLoc();
call    0 never executed
    #####: 2044:    OpResult resultValue = castOp.getSource().cast<OpResult>();
call    0 never executed
call    1 never executed
    #####: 2045:    unsigned resultNumber = resultValue.getResultNumber();
call    0 never executed
    #####: 2046:    auto resultType = castOp->getResult(0).getType().cast<RankedTensorType>();
call    0 never executed
        -: 2047:    // Replace the `outs` for the result with a `tensor.cast`. This cast is now
        -: 2048:    // going from a more dynamic shape to a less dynamic shape. If the producer
        -: 2049:    // for this cast, i.e. producer of the out operand, is also an operation
        -: 2050:    // that folds with tensor.cast consumer (like this pattern), the cast will
        -: 2051:    // continue to propagate as far up the stack as it can go.
    #####: 2052:    OpOperand *outOperand = linalgOp.getDpsInitOperand(resultNumber);
call    0 never executed
    #####: 2053:    Value newOperand =
    #####: 2054:        rewriter.create<tensor::CastOp>(loc, resultType, outOperand->get());
call    0 never executed
call    1 never executed
    #####: 2055:    SmallVector<Value> newOperands{linalgOp.getDpsInputOperands()};
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2056:    SmallVector<Value> outputOperands{linalgOp.getDpsInitOperands()};
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2057:    outputOperands[resultNumber] = newOperand;
branch  0 never executed
branch  1 never executed
    #####: 2058:    newOperands.append(outputOperands.begin(), outputOperands.end());
call    0 never executed
        -: 2059:
    #####: 2060:    SmallVector<Type> resultTypes(linalgOp->result_type_begin(),
    #####: 2061:                                  linalgOp->result_type_end());
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2062:    resultTypes[resultNumber] = resultType;
branch  0 never executed
branch  1 never executed
    #####: 2063:    Operation *newOp = linalgOp.clone(rewriter, loc, resultTypes, newOperands);
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2064:
        -: 2065:    // Create a tensor.cast operation back to the original type.
    #####: 2066:    Value castBack = rewriter.create<tensor::CastOp>(
    #####: 2067:        loc, resultValue.getType(), newOp->getResult(resultNumber));
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -: 2068:
    #####: 2069:    SmallVector<Value> results(newOp->result_begin(), newOp->result_end());
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2070:    results[resultNumber] = castBack;
branch  0 never executed
branch  1 never executed
    #####: 2071:    rewriter.replaceOp(linalgOp, results);
call    0 never executed
call    1 never executed
    #####: 2072:    rewriter.replaceOp(castOp, newOp->getResult(resultNumber));
call    0 never executed
call    1 never executed
    #####: 2073:    return success();
branch  0 never executed
branch  1 never executed
        -: 2074:  }
        -: 2075:};
        -: 2076:
        -: 2077:/// For each of the operand in `operands` this function maps the static sizes of
        -: 2078:/// dimensions to their affine dim expressions.
function _ZN12_GLOBAL__N_1L11populateMapEN4mlir6linalg8LinalgOpEN4llvm15MutableArrayRefINS0_9OpOperandEEERNS3_8DenseMapINS0_10AffineExprElNS3_12DenseMapInfoIS8_vEENS3_6detail12DenseMapPairIS8_lEEEE called 3673 returned 100% blocks executed 78%
     3673: 2079:static void populateMap(LinalgOp linalgOp, MutableArrayRef<OpOperand> operands,
        -: 2080:                        llvm::DenseMap<AffineExpr, int64_t> &affineExprToSize) {
    11364: 2081:  for (OpOperand &opOperand : operands) {
branch  0 taken 68% (fallthrough)
branch  1 taken 32%
   15382*: 2082:    if (linalgOp.isScalar(&opOperand))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 2083:      continue;
     7691: 2084:    Value src = opOperand.get();
call    0 returned 100%
     7691: 2085:    auto sourceType = src.getType().cast<RankedTensorType>();
call    0 returned 100%
     7691: 2086:    auto sourceMap = linalgOp.getMatchingIndexingMap(&opOperand);
call    0 returned 100%
        -: 2087:
        -: 2088:    // Get the `sourceShape` of the `sourceType`. If the operand is a result of
        -: 2089:    // `tensor.cast` operation and source of the cast operation has a static
        -: 2090:    // shape, then assign it to the `sourceShape`.
     7691: 2091:    auto *parentOp = src.getDefiningOp();
call    0 returned 100%
     7691: 2092:    ArrayRef<int64_t> sourceShape = sourceType.getShape();
call    0 returned 100%
     7691: 2093:    if (parentOp) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     7691: 2094:      if (auto castOp = dyn_cast<tensor::CastOp>(parentOp)) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####: 2095:        Value castSource = castOp.getSource();
call    0 never executed
    #####: 2096:        auto castSourceType = castSource.getType().cast<RankedTensorType>();
call    0 never executed
    #####: 2097:        if (castSourceType.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2098:          sourceShape = castSourceType.getShape();
call    0 never executed
        -: 2099:      }
        -: 2100:    }
        -: 2101:
        -: 2102:    // If the source shape's dimension has a static shape, map the affine dim
        -: 2103:    // expression to the known static size.
    20816: 2104:    for (unsigned i = 0; i < sourceShape.size(); i++) {
branch  0 taken 63% (fallthrough)
branch  1 taken 37%
    13125: 2105:      if (sourceType.isDynamicDim(i))
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 100%
        1: 2106:        continue;
    13124: 2107:      if (auto affineDimExpr = sourceMap.getResult(i).dyn_cast<AffineDimExpr>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
    13124: 2108:        affineExprToSize.try_emplace(affineDimExpr, sourceShape[i]);
call    0 returned 100%
        -: 2109:    }
        -: 2110:  }
     3673: 2111:}
        -: 2112:
        -: 2113:/// Creates new operand w.r.t 'opOperand' of `linalgOp` with static sizes
        -: 2114:/// mapped in `affineExprToSize`. New operands are created in `newOperands` and
        -: 2115:/// their result types is stored in `resultTypes`. If `opOperand` requires no
        -: 2116:/// change then `changeNeeded` is false and same operand is added in the
        -: 2117:/// `newOperands` list.
function _ZN12_GLOBAL__N_1L31createNewOperandWithStaticSizesEN4mlir8LocationERNS0_15PatternRewriterEPNS0_9OpOperandERN4llvm8DenseMapINS0_10AffineExprElNS6_12DenseMapInfoIS8_vEENS6_6detail12DenseMapPairIS8_lEEEENS0_6linalg8LinalgOpERNS6_11SmallVectorINS0_5ValueELj6EEERNSI_INS0_4TypeELj6EEERb called 7691 returned 100% blocks executed 80%
     7691: 2118:static void createNewOperandWithStaticSizes(
        -: 2119:    Location loc, PatternRewriter &rewriter, OpOperand *opOperand,
        -: 2120:    llvm::DenseMap<AffineExpr, int64_t> &affineExprToSize, LinalgOp linalgOp,
        -: 2121:    SmallVector<Value> &newOperands, SmallVector<Type> &resultTypes,
        -: 2122:    bool &changeNeeded) {
     7691: 2123:  Value src = opOperand->get();
call    0 returned 100%
     7691: 2124:  newOperands.push_back(src);
call    0 returned 100%
    15382: 2125:  if (linalgOp.isScalar(opOperand))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
     3672: 2126:    return;
     7691: 2127:  auto sourceType = src.getType().cast<RankedTensorType>();
call    0 returned 100%
     7691: 2128:  Type resultType = sourceType;
    15381: 2129:  if (sourceType.hasStaticShape() && linalgOp.isDpsInit(opOperand)) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 1%
branch  3 taken 48% (fallthrough)
branch  4 taken 52%
     3672: 2130:    resultTypes.push_back(resultType);
     3672: 2131:    return;
call    0 returned 100%
        -: 2132:  }
     4019: 2133:  ArrayRef<int64_t> sourceShape = sourceType.getShape();
call    0 returned 100%
     4019: 2134:  AffineMap sourceMap = linalgOp.getMatchingIndexingMap(opOperand);
call    0 returned 100%
     8038: 2135:  SmallVector<int64_t> newShape;
        -: 2136:  // If operand is updated with new shape, `newOperandNeeded` will be
        -: 2137:  // true.
     4019: 2138:  bool newOperandNeeded = false;
    11448: 2139:  for (unsigned i = 0; i < sourceShape.size(); i++) {
branch  0 taken 65% (fallthrough)
branch  1 taken 35%
     7429: 2140:    int64_t dimShape = sourceShape[i];
call    0 returned 100%
     7429: 2141:    AffineExpr dimExpr = sourceMap.getResult(i);
call    0 returned 100%
   14857*: 2142:    if (affineExprToSize.find(dimExpr) == affineExprToSize.end() ||
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 1%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
     7428: 2143:        !sourceType.isDynamicDim(i)) {
call    0 returned 100%
     7429: 2144:      newShape.push_back(dimShape);
call    0 returned 100%
     7429: 2145:      continue;
        -: 2146:    }
        -: 2147:    // Dimension has a dynamic shape and corresponding affine dim
        -: 2148:    // expression is present in the map. So assign the size for the
        -: 2149:    // given affine dim expression to the dimension.
    #####: 2150:    newShape.push_back(affineExprToSize[dimExpr]);
call    0 never executed
call    1 never executed
    #####: 2151:    newOperandNeeded = true;
        -: 2152:  }
     4019: 2153:  resultType = RankedTensorType::get(newShape, sourceType.getElementType());
call    0 returned 100%
call    1 returned 100%
     4019: 2154:  if (newOperandNeeded) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 2155:    changeNeeded = true;
        -: 2156:    // Get the new operand value given its size and element type by
        -: 2157:    // casting it.
    #####: 2158:    Value newOperand = rewriter.create<tensor::CastOp>(loc, resultType, src);
call    0 never executed
call    1 never executed
    #####: 2159:    unsigned index = opOperand->getOperandNumber();
call    0 never executed
    #####: 2160:    newOperands[index] = newOperand;
branch  0 never executed
branch  1 never executed
        -: 2161:  }
     8038: 2162:  if (linalgOp.isDpsInit(opOperand))
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 100%
        1: 2163:    resultTypes.push_back(resultType);
call    0 returned 100%
        -: 2164:}
        -: 2165:
        -: 2166:/// Static shapes for the operands can be inferred if any one of the operands
        -: 2167:/// have a static shape. This can be done by referring to the affine dim
        -: 2168:/// expressions for the operand.
        -: 2169:struct InferStaticShapeOfOperands : public OpInterfaceRewritePattern<LinalgOp> {
        -: 2170:  using OpInterfaceRewritePattern<LinalgOp>::OpInterfaceRewritePattern;
        -: 2171:
function _ZNK12_GLOBAL__N_126InferStaticShapeOfOperands15matchAndRewriteEN4mlir6linalg8LinalgOpERNS1_15PatternRewriterE called 10694 returned 100% blocks executed 47%
    10694: 2172:  LogicalResult matchAndRewrite(LinalgOp linalgOp,
        -: 2173:                                PatternRewriter &rewriter) const override {
    21388: 2174:    if (!linalgOp.hasTensorSemantics())
call    0 returned 100%
branch  1 taken 66% (fallthrough)
branch  2 taken 34%
     7021: 2175:      return failure();
        -: 2176:
        -: 2177:    // Maps must be projected permutations.
     3673: 2178:    if (llvm::any_of(linalgOp.getIndexingMapsArray(), [](AffineMap map) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
        -: 2179:          return !map.isProjectedPermutation();
        -: 2180:        }))
    #####: 2181:      return failure();
        -: 2182:
        -: 2183:    // Maps affine dim expressions to the static size of that dimension.
     3673: 2184:    llvm::DenseMap<AffineExpr, int64_t> affineExprToSize;
call    0 returned 100%
     3673: 2185:    Location loc = linalgOp.getLoc();
call    0 returned 100%
        -: 2186:
        -: 2187:    // For each of the affine dim expression, check if the size is known. If
        -: 2188:    // known add that in the map.
     3673: 2189:    populateMap(linalgOp, linalgOp->getOpOperands(), affineExprToSize);
call    0 returned 100%
call    1 returned 100%
        -: 2190:
     7346: 2191:    SmallVector<Value> newOperands;
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
     3673: 2192:    SmallVector<Type> resultTypes;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 2193:
        -: 2194:    // `changeNeeded` is `false` if the operands of `linalgOp` require no
        -: 2195:    // change in their types.
     3673: 2196:    bool changeNeeded = false;
     7346: 2197:    newOperands.reserve(linalgOp->getNumOperands());
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
     7346: 2198:    resultTypes.reserve(linalgOp.getNumDpsInits());
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -: 2199:
        -: 2200:    // Iterate over all the operands and update the static sizes.
    11364: 2201:    for (OpOperand &opOperand : linalgOp->getOpOperands()) {
call    0 returned 100%
branch  1 taken 68% (fallthrough)
branch  2 taken 32%
     7691: 2202:      createNewOperandWithStaticSizes(loc, rewriter, &opOperand,
call    0 returned 100%
        -: 2203:                                      affineExprToSize, linalgOp, newOperands,
        -: 2204:                                      resultTypes, changeNeeded);
        -: 2205:    }
        -: 2206:
        -: 2207:    // If the generic op has all the required static information, no
        -: 2208:    // canonicalization needed.
     3673: 2209:    if (!changeNeeded)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     3673: 2210:      return failure();
        -: 2211:
        -: 2212:    // Clone op.
    #####: 2213:    Operation *newOp =
call    0 never executed
    #####: 2214:        linalgOp.clone(rewriter, linalgOp->getLoc(), resultTypes, newOperands);
call    0 never executed
call    1 never executed
    3673*: 2215:    SmallVector<Value> replacements;
branch  0 never executed
branch  1 never executed
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####: 2216:    replacements.reserve(newOp->getNumResults());
branch  0 never executed
branch  1 never executed
    #####: 2217:    for (auto it : llvm::zip(linalgOp->getResults(), newOp->getResults())) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2218:      Value newResult = std::get<1>(it);
branch  0 never executed
branch  1 never executed
    #####: 2219:      Value oldResult = std::get<0>(it);
branch  0 never executed
branch  1 never executed
    #####: 2220:      Type newType = newResult.getType();
branch  0 never executed
branch  1 never executed
    #####: 2221:      Type oldType = oldResult.getType();
    #####: 2222:      replacements.push_back(
branch  0 never executed
branch  1 never executed
    #####: 2223:          (newType != oldType)
branch  0 never executed
branch  1 never executed
    #####: 2224:              ? rewriter.create<tensor::CastOp>(loc, oldType, newResult)
call    0 never executed
    #####: 2225:              : newResult);
call    0 never executed
        -: 2226:    }
    #####: 2227:    rewriter.replaceOp(linalgOp, replacements);
call    0 never executed
call    1 never executed
    #####: 2228:    return success();
branch  0 never executed
branch  1 never executed
        -: 2229:  }
        -: 2230:};
        -: 2231:
        -: 2232:} // namespace
        -: 2233:
        -: 2234:// All named ops canonicalizers and folders are auto-generated in the
        -: 2235:// .cpp.inc.
        -: 2236:
        -: 2237://===----------------------------------------------------------------------===//
        -: 2238:// LinalgDialect
        -: 2239://===----------------------------------------------------------------------===//
        -: 2240:
function _ZNK4mlir6linalg13LinalgDialect27getCanonicalizationPatternsERNS_17RewritePatternSetE called 1673 returned 100% blocks executed 100%
     1673: 2241:void LinalgDialect::getCanonicalizationPatterns(
        -: 2242:    RewritePatternSet &results) const {
     1673: 2243:  results.add<EraseDeadLinalgOp, FoldTensorCastConsumerOp,
        -: 2244:              FoldTensorCastProducerOp, InferStaticShapeOfOperands>(
     1673: 2245:      getContext());
call    0 returned 100%
     1673: 2246:}
        -: 2247:
function _ZN4mlir6linalg13LinalgDialect19materializeConstantERNS_9OpBuilderENS_9AttributeENS_4TypeENS_8LocationE called 0 returned 0% blocks executed 0%
    #####: 2248:Operation *LinalgDialect::materializeConstant(OpBuilder &builder,
        -: 2249:                                              Attribute value, Type type,
        -: 2250:                                              Location loc) {
    #####: 2251:  return builder.create<arith::ConstantOp>(loc, type, value);
call    0 never executed
        -: 2252:}
