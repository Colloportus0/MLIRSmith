        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/SparseTensor/Transforms/CodegenUtils.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/SparseTensor/Transforms/CMakeFiles/obj.MLIRSparseTensorTransforms.dir/CodegenUtils.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/SparseTensor/Transforms/CMakeFiles/obj.MLIRSparseTensorTransforms.dir/CodegenUtils.cpp.gcda
        -:    0:Runs:116161
        -:    1://===- CodegenUtils.cpp - Utilities for generating MLIR -------------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8:
        -:    9:#include "CodegenUtils.h"
        -:   10:
        -:   11:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   12:#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
        -:   13:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   14:#include "mlir/Dialect/Linalg/Utils/Utils.h"
        -:   15:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   16:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   17:#include "mlir/IR/Matchers.h"
        -:   18:#include "mlir/IR/Types.h"
        -:   19:#include "mlir/IR/Value.h"
        -:   20:
        -:   21:using namespace mlir;
        -:   22:using namespace mlir::sparse_tensor;
        -:   23:
        -:   24:/// Generates a pointer/index load from the sparse storage scheme. Narrower
        -:   25:/// data types need to be zero extended before casting the value into the
        -:   26:/// index type used for looping and indexing.
function _ZL12genIndexLoadRN4mlir9OpBuilderENS_8LocationENS_5ValueES3_ called 0 returned 0% blocks executed 0%
    #####:   27:static Value genIndexLoad(OpBuilder &builder, Location loc, Value ptr,
        -:   28:                          Value s) {
        -:   29:  // For the scalar case, we simply zero extend narrower indices into 64-bit
        -:   30:  // values before casting to index without a performance penalty. Here too,
        -:   31:  // however, indices that already are 64-bit, in theory, cannot express the
        -:   32:  // full range as explained above.
    #####:   33:  Value load = builder.create<memref::LoadOp>(loc, ptr, s);
call    0 never executed
call    1 never executed
    #####:   34:  if (!load.getType().isa<IndexType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   35:    if (load.getType().getIntOrFloatBitWidth() < 64)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   36:      load = builder.create<arith::ExtUIOp>(loc, builder.getI64Type(), load);
call    0 never executed
call    1 never executed
    #####:   37:    load =
    #####:   38:        builder.create<arith::IndexCastOp>(loc, builder.getIndexType(), load);
call    0 never executed
call    1 never executed
        -:   39:  }
    #####:   40:  return load;
        -:   41:}
        -:   42:
        -:   43:/// If the tensor is a sparse constant, generates and returns the pair of
        -:   44:/// the constants for the indices and the values.
        -:   45:static Optional<std::pair<Value, Value>>
function _ZL22genSplitSparseConstantRN4mlir9OpBuilderENS_8LocationENS_5ValueE called 0 returned 0% blocks executed 0%
    #####:   46:genSplitSparseConstant(OpBuilder &builder, Location loc, Value tensor) {
    #####:   47:  if (auto constOp = tensor.getDefiningOp<arith::ConstantOp>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   48:    if (auto attr = constOp.getValue().dyn_cast<SparseElementsAttr>()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   49:      DenseElementsAttr indicesAttr = attr.getIndices();
call    0 never executed
    #####:   50:      Value indices = builder.create<arith::ConstantOp>(loc, indicesAttr);
call    0 never executed
call    1 never executed
    #####:   51:      DenseElementsAttr valuesAttr = attr.getValues();
call    0 never executed
    #####:   52:      Value values = builder.create<arith::ConstantOp>(loc, valuesAttr);
call    0 never executed
    #####:   53:      return std::make_pair(indices, values);
        -:   54:    }
        -:   55:  }
    #####:   56:  return {};
        -:   57:}
        -:   58:
        -:   59:/// Generates the code to copy the index at indices[ivs] to ind, and return
        -:   60:/// the value at value[ivs].
function _ZL25genIndexAndValueForSparseRN4mlir9OpBuilderENS_8LocationENS_5ValueES3_RN4llvm15SmallVectorImplIS3_EENS_10ValueRangeEj called 0 returned 0% blocks executed 0%
    #####:   61:static Value genIndexAndValueForSparse(OpBuilder &builder, Location loc,
        -:   62:                                       Value indices, Value values,
        -:   63:                                       SmallVectorImpl<Value> &indicesArray,
        -:   64:                                       ValueRange ivs, unsigned rank) {
    #####:   65:  for (unsigned i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
    #####:   66:    Value idx = constantIndex(builder, loc, i);
call    0 never executed
    #####:   67:    Value val = builder.create<tensor::ExtractOp>(loc, indices,
    #####:   68:                                                  ValueRange{ivs[0], idx});
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:   69:    val = builder.create<arith::IndexCastOp>(loc, builder.getIndexType(), val);
call    0 never executed
call    1 never executed
call    2 never executed
        -:   70:    // builder.create<memref::StoreOp>(loc, val, ind, idx);
    #####:   71:    indicesArray.push_back(val);
call    0 never executed
        -:   72:  }
    #####:   73:  return builder.create<tensor::ExtractOp>(loc, values, ivs[0]);
call    0 never executed
call    1 never executed
        -:   74:}
        -:   75:
        -:   76:/// Generates the code to read the value from tensor[ivs], and conditionally
        -:   77:/// stores the indices ivs to the memory in ind. The generated code looks like
        -:   78:/// the following and the insertion point after this routine is inside the
        -:   79:/// if-then branch behind the assignment to ind. This is to ensure that the
        -:   80:/// code that uses the ind, such as an addEltX call generated after, is inside
        -:   81:/// the if-then branch.
        -:   82:///    if (tensor[ivs] != 0)
        -:   83:///      ind = ivs
function _ZL24genIndexAndValueForDenseRN4mlir9OpBuilderENS_8LocationENS_5ValueERN4llvm15SmallVectorImplIS3_EENS_10ValueRangeE called 0 returned 0% blocks executed 0%
    #####:   84:static Value genIndexAndValueForDense(OpBuilder &builder, Location loc,
        -:   85:                                      Value tensor,
        -:   86:                                      SmallVectorImpl<Value> &indicesArray,
        -:   87:                                      ValueRange ivs) {
    #####:   88:  Value val = genValueForDense(builder, loc, tensor, ivs);
call    0 never executed
    #####:   89:  indicesArray.append(ivs.begin(), ivs.end());
call    0 never executed
    #####:   90:  return val;
        -:   91:}
        -:   92:
        -:   93://===----------------------------------------------------------------------===//
        -:   94:// Sparse tensor loop emitter class implementations
        -:   95://===----------------------------------------------------------------------===//
        -:   96:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitterC2ENS_10ValueRangeEbb called 0 returned 0% blocks executed 0%
    #####:   97:SparseTensorLoopEmitter::SparseTensorLoopEmitter(ValueRange tensors,
        -:   98:                                                 bool hasOutput,
    #####:   99:                                                 bool isSparseOut)
        -:  100:    : hasOutput(hasOutput), isSparseOut(isSparseOut),
        -:  101:      tensors(tensors.begin(), tensors.end()), dimTypes(tensors.size()),
        -:  102:      pidxs(tensors.size()), coord(tensors.size()), highs(tensors.size()),
        -:  103:      ptrBuffer(tensors.size()), idxBuffer(tensors.size()),
    #####:  104:      valBuffer(tensors.size()), loopStack() {
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
    #####:  105:  for (size_t tid = 0, e = tensors.size(); tid < e; tid++) {
branch  0 never executed
branch  1 never executed
    #####:  106:    auto t = tensors[tid];
call    0 never executed
        -:  107:    // a scalar or 0-dimension tensors
    #####:  108:    if (isZeroRankedTensorOrScalar(t.getType()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  109:      continue;
    #####:  110:    auto rtp = t.getType().cast<RankedTensorType>();
call    0 never executed
    #####:  111:    auto rank = static_cast<size_t>(rtp.getRank());
call    0 never executed
    #####:  112:    auto enc = getSparseTensorEncoding(rtp);
call    0 never executed
        -:  113:    // We always treat sparse output tensor as dense so that we always iterate
        -:  114:    // it based on dim size.
    #####:  115:    if (enc && !(isOutputTensor(tid) && isSparseOut))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  116:      for (auto dimTp : enc.getDimLevelType())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  117:        dimTypes[tid].push_back(dimTp);
call    0 never executed
        -:  118:    else
    #####:  119:      dimTypes[tid].assign(rank, DimLevelType::Dense);
call    0 never executed
        -:  120:
        -:  121:    // Initialize using empty value.
    #####:  122:    pidxs[tid].assign(rank, Value());
call    0 never executed
    #####:  123:    coord[tid].assign(rank, Value());
call    0 never executed
    #####:  124:    highs[tid].assign(rank, Value());
call    0 never executed
    #####:  125:    ptrBuffer[tid].assign(rank, Value());
call    0 never executed
    #####:  126:    idxBuffer[tid].assign(rank, Value());
call    0 never executed
        -:  127:  }
    #####:  128:}
        -:  129:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter18initializeLoopEmitERNS_9OpBuilderENS_8LocationEN4llvm12function_refIFNS_5ValueES3_S4_S7_S7_EEE called 0 returned 0% blocks executed 0%
    #####:  130:void SparseTensorLoopEmitter::initializeLoopEmit(
        -:  131:    OpBuilder &builder, Location loc,
        -:  132:    SparseTensorLoopEmitter::OutputUpdater updater) {
        -:  133:  // For every tensor, find lower and upper bound on dimensions, set the
        -:  134:  // same bounds on loop indices, and obtain dense or sparse buffer(s).
    #####:  135:  for (size_t t = 0, e = tensors.size(); t < e; t++) {
branch  0 never executed
branch  1 never executed
    #####:  136:    auto tensor = tensors[t];
call    0 never executed
    #####:  137:    auto rtp = tensor.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  138:    if (!rtp)
branch  0 never executed
branch  1 never executed
        -:  139:      // Skips only scalar, zero ranked tensor still need to be bufferized and
        -:  140:      // (probably) filled with zeros by users.
    #####:  141:      continue;
    #####:  142:    auto rank = rtp.getRank();
call    0 never executed
    #####:  143:    auto shape = rtp.getShape();
call    0 never executed
    #####:  144:    auto enc = getSparseTensorEncoding(rtp);
call    0 never executed
    #####:  145:    auto dynShape = {ShapedType::kDynamicSize};
        -:  146:    // Scan all dimensions of current tensor.
    #####:  147:    for (int64_t d = 0; d < rank; d++) {
branch  0 never executed
branch  1 never executed
        -:  148:      // This should be called only once at beginning.
    #####:  149:      assert(!ptrBuffer[t][d] && !idxBuffer[t][d] && !highs[t][d]);
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
        -:  150:      // Handle sparse storage schemes.
    #####:  151:      if (isCompressedDLT(dimTypes[t][d])) {
branch  0 never executed
branch  1 never executed
    #####:  152:        auto ptrTp =
    #####:  153:            MemRefType::get(dynShape, getPointerOverheadType(builder, enc));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  154:        auto indTp =
    #####:  155:            MemRefType::get(dynShape, getIndexOverheadType(builder, enc));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  156:        auto dim = builder.getIndexAttr(d);
call    0 never executed
        -:  157:        // Generate sparse primitives to obtains pointer and indices.
    #####:  158:        ptrBuffer[t][d] = builder.create<ToPointersOp>(loc, ptrTp, tensor, dim);
call    0 never executed
call    1 never executed
    #####:  159:        idxBuffer[t][d] = builder.create<ToIndicesOp>(loc, indTp, tensor, dim);
call    0 never executed
    #####:  160:      } else if (isSingletonDLT(dimTypes[t][d])) {
branch  0 never executed
branch  1 never executed
        -:  161:        // Singleton dimension, fetch indices.
    #####:  162:        auto indTp =
    #####:  163:            MemRefType::get(dynShape, getIndexOverheadType(builder, enc));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  164:        auto dim = builder.getIndexAttr(d);
call    0 never executed
    #####:  165:        idxBuffer[t][d] = builder.create<ToIndicesOp>(loc, indTp, tensor, dim);
call    0 never executed
        -:  166:      } else {
        -:  167:        // Dense dimension, nothing to fetch.
    #####:  168:        assert(isDenseDLT(dimTypes[t][d]));
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  169:      }
        -:  170:
        -:  171:      // Find upper bound in current dimension.
    #####:  172:      unsigned p = toOrigDim(enc, d);
call    0 never executed
    #####:  173:      Value up = mlir::linalg::createOrFoldDimOp(builder, loc, tensor, p);
call    0 never executed
    #####:  174:      highs[t][d] = up;
        -:  175:    }
        -:  176:
        -:  177:    // Perform the required bufferization. Dense inputs materialize
        -:  178:    // from the input tensors. Sparse inputs use sparse primitives to obtain the
        -:  179:    // values.
        -:  180:    // Delegates extra output initialization to clients.
    #####:  181:    bool isOutput = isOutputTensor(t);
branch  0 never executed
branch  1 never executed
    #####:  182:    Type elementType = rtp.getElementType();
call    0 never executed
    #####:  183:    if (!enc) {
branch  0 never executed
branch  1 never executed
        -:  184:      // Non-annotated dense tensors.
    #####:  185:      auto denseTp = MemRefType::get(shape, elementType);
call    0 never executed
call    1 never executed
    #####:  186:      Value denseVal =
    #####:  187:          builder.create<bufferization::ToMemrefOp>(loc, denseTp, tensor);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  188:      // Dense outputs need special handling.
    #####:  189:      if (isOutput && updater)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  190:        denseVal = updater(builder, loc, denseVal, tensor);
call    0 never executed
        -:  191:
    #####:  192:      valBuffer[t] = denseVal;
        -:  193:    } else {
        -:  194:      // Annotated sparse tensors.
        -:  195:      // We also need the value buffer for annotated all dense `sparse` tensor.
    #####:  196:      auto dynShape = {ShapedType::kDynamicSize};
    #####:  197:      auto sparseTp = MemRefType::get(dynShape, elementType);
call    0 never executed
call    1 never executed
    #####:  198:      valBuffer[t] = builder.create<ToValuesOp>(loc, sparseTp, tensor);
call    0 never executed
        -:  199:    }
        -:  200:    // NOTE: we can also prepares for 0 dim here in advance, this will hosit
        -:  201:    // some loop preparation from tensor iteration, but will also (undesirably)
        -:  202:    // hosit the code ouside if conditions.
        -:  203:  }
    #####:  204:}
        -:  205:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter15enterNewLoopSeqERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefImEES7_ called 0 returned 0% blocks executed 0%
    #####:  206:void SparseTensorLoopEmitter::enterNewLoopSeq(OpBuilder &builder, Location loc,
        -:  207:                                              ArrayRef<size_t> tids,
        -:  208:                                              ArrayRef<size_t> dims) {
        -:  209:  // Universal Index start from 0
    #####:  210:  assert(loopSeqStack.size() == loopStack.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  211:  // Universal index starts from 0
    #####:  212:  loopSeqStack.emplace_back(constantIndex(builder, loc, 0));
call    0 never executed
call    1 never executed
        -:  213:  // Prepares for all the tensors used in the current loop sequence.
    #####:  214:  for (auto [tid, dim] : llvm::zip(tids, dims))
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  215:    prepareLoopOverTensorAtDim(builder, loc, tid, dim);
call    0 never executed
    #####:  216:}
        -:  217:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter24enterLoopOverTensorAtDimERNS_9OpBuilderENS_8LocationEmmN4llvm15MutableArrayRefINS_5ValueEEEbNS5_8ArrayRefImEESA_ called 0 returned 0% blocks executed 0%
    #####:  218:Operation *SparseTensorLoopEmitter::enterLoopOverTensorAtDim(
        -:  219:    OpBuilder &builder, Location loc, size_t tid, size_t dim,
        -:  220:    MutableArrayRef<Value> reduc, bool isParallel, ArrayRef<size_t> extraTids,
        -:  221:    ArrayRef<size_t> extraDims) {
    #####:  222:  assert(dimTypes[tid].size() > dim);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  223:  // We can not re-enter the same level.
    #####:  224:  assert(!coord[tid][dim]);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  225:
    #####:  226:  Value step = constantIndex(builder, loc, 1);
call    0 never executed
    #####:  227:  auto dimType = dimTypes[tid][dim];
branch  0 never executed
branch  1 never executed
    #####:  228:  bool isSparseInput = isCompressedDLT(dimType) || isSingletonDLT(dimType);
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  229:  assert(isDenseDLT(dimType) || isCompressedDLT(dimType) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
        -:  230:         isSingletonDLT(dimType));
        -:  231:
    #####:  232:  Value lo = isSparseInput ? pidxs[tid][dim]      // current offset
branch  0 never executed
branch  1 never executed
    #####:  233:                           : loopSeqStack.back(); // univeral tid
    #####:  234:  Value hi = highs[tid][dim];
call    0 never executed
        -:  235:
    #####:  236:  scf::ForOp forOp = builder.create<scf::ForOp>(loc, lo, hi, step, reduc);
call    0 never executed
    #####:  237:  builder.setInsertionPointToStart(forOp.getBody());
call    0 never executed
call    1 never executed
    #####:  238:  Value iv = forOp.getInductionVar();
call    0 never executed
    #####:  239:  assert(iv);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  240:  if (isSparseInput) {
branch  0 never executed
branch  1 never executed
    #####:  241:    pidxs[tid][dim] = iv;
call    0 never executed
        -:  242:    // Generating a load on the indices array yields the coordinate.
    #####:  243:    Value ptr = idxBuffer[tid][dim];
    #####:  244:    coord[tid][dim] = genIndexLoad(builder, loc, ptr, iv);
call    0 never executed
        -:  245:  } else {
        -:  246:    // Dense tensor, the coordinates is the inducation variable.
    #####:  247:    coord[tid][dim] = iv;
call    0 never executed
        -:  248:    // generate pidx for dense dim (pidx = i * sz + j)
    #####:  249:    auto enc = getSparseTensorEncoding(tensors[tid].getType());
call    0 never executed
    #####:  250:    if (enc && !isSparseOutput(tid))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  251:      pidxs[tid][dim] = genAddress(builder, loc, tid, dim, iv);
call    0 never executed
        -:  252:  }
        -:  253:
        -:  254:  // NOTE: we can also prepares for next dim here in advance
        -:  255:  // Push the loop into stack
    #####:  256:  loopStack.emplace_back(ArrayRef<size_t>(tid), ArrayRef<size_t>(dim), forOp,
    #####:  257:                         coord[tid][dim]);
call    0 never executed
        -:  258:  // Emit extra locals.
    #####:  259:  emitExtraLocalsForTensorsAtDenseDims(builder, loc, extraTids, extraDims);
call    0 never executed
        -:  260:
        -:  261:  // In-place update on the reduction variable vector.
    #####:  262:  assert(forOp.getNumRegionIterArgs() == reduc.size());
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  263:  for (int i = 0, e = reduc.size(); i < e; i++)
branch  0 never executed
branch  1 never executed
    #####:  264:    reduc[i] = forOp.getRegionIterArg(i);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  265:  return forOp;
        -:  266:}
        -:  267:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter33enterCoIterationOverTensorsAtDimsERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefImEES7_bNS5_15MutableArrayRefINS_5ValueEEES7_S7_ called 0 returned 0% blocks executed 0%
    #####:  268:Operation *SparseTensorLoopEmitter::enterCoIterationOverTensorsAtDims(
        -:  269:    OpBuilder &builder, Location loc, ArrayRef<size_t> tids,
        -:  270:    ArrayRef<size_t> dims, bool needsUniv, MutableArrayRef<Value> reduc,
        -:  271:    ArrayRef<size_t> extraTids, ArrayRef<size_t> extraDims) {
    #####:  272:  assert(tids.size() == dims.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  273:  SmallVector<Type, 4> types;
call    0 never executed
    #####:  274:  SmallVector<Value, 4> operands;
branch  0 never executed
branch  1 never executed
        -:  275:  // Construct the while-loop with a parameter for each index.
    #####:  276:  Type indexType = builder.getIndexType();
call    0 never executed
    #####:  277:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  278:    if (isCompressedDLT(dimTypes[tid][dim]) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  279:        isSingletonDLT(dimTypes[tid][dim])) {
branch  0 never executed
branch  1 never executed
    #####:  280:      assert(pidxs[tid][dim]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  281:      types.push_back(indexType);
call    0 never executed
    #####:  282:      operands.push_back(pidxs[tid][dim]);
call    0 never executed
        -:  283:    }
        -:  284:  }
        -:  285:  // The position where user-supplied reduction variable starts.
    #####:  286:  for (Value rec : reduc) {
branch  0 never executed
branch  1 never executed
    #####:  287:    types.push_back(rec.getType());
call    0 never executed
    #####:  288:    operands.push_back(rec);
call    0 never executed
        -:  289:  }
    #####:  290:  if (needsUniv) {
branch  0 never executed
branch  1 never executed
    #####:  291:    types.push_back(indexType);
call    0 never executed
        -:  292:    // Update universal index.
    #####:  293:    operands.push_back(loopSeqStack.back());
call    0 never executed
        -:  294:  }
    #####:  295:  assert(types.size() == operands.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  296:  scf::WhileOp whileOp = builder.create<scf::WhileOp>(loc, types, operands);
call    0 never executed
        -:  297:
    #####:  298:  SmallVector<Location> locs(types.size(), loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  299:  Block *before = builder.createBlock(&whileOp.getBefore(), {}, types, locs);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  300:  Block *after = builder.createBlock(&whileOp.getAfter(), {}, types, locs);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  301:
        -:  302:  // Build the "before" region, which effectively consists
        -:  303:  // of a conjunction of "i < upper" tests on all induction.
    #####:  304:  builder.setInsertionPointToStart(&whileOp.getBefore().front());
call    0 never executed
call    1 never executed
    #####:  305:  Value cond;
    #####:  306:  unsigned o = 0;
    #####:  307:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  308:    if (isCompressedDLT(dimTypes[tid][dim]) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  309:        isSingletonDLT(dimTypes[tid][dim])) {
branch  0 never executed
branch  1 never executed
    #####:  310:      Value op1 = before->getArgument(o);
call    0 never executed
    #####:  311:      Value op2 = highs[tid][dim];
    #####:  312:      Value opc = builder.create<arith::CmpIOp>(loc, arith::CmpIPredicate::ult,
    #####:  313:                                                op1, op2);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  314:      cond = cond ? builder.create<arith::AndIOp>(loc, cond, opc) : opc;
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  315:      // Update
    #####:  316:      pidxs[tid][dim] = after->getArgument(o++);
        -:  317:    }
        -:  318:  }
    #####:  319:  builder.create<scf::ConditionOp>(loc, cond, before->getArguments());
call    0 never executed
        -:  320:
        -:  321:  // Generates while body.
    #####:  322:  builder.setInsertionPointToStart(&whileOp.getAfter().front());
call    0 never executed
call    1 never executed
    #####:  323:  Value min;
    #####:  324:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  325:    // Prepares for next level.
    #####:  326:    if (isCompressedDLT(dimTypes[tid][dim]) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  327:        isSingletonDLT(dimTypes[tid][dim])) {
branch  0 never executed
branch  1 never executed
    #####:  328:      Value ptr = idxBuffer[tid][dim];
call    0 never executed
    #####:  329:      Value s = pidxs[tid][dim];
    #####:  330:      Value load = genIndexLoad(builder, loc, ptr, s);
call    0 never executed
    #####:  331:      coord[tid][dim] = load;
branch  0 never executed
branch  1 never executed
    #####:  332:      if (!needsUniv) {
branch  0 never executed
branch  1 never executed
    #####:  333:        if (min) {
branch  0 never executed
branch  1 never executed
    #####:  334:          Value cmp = builder.create<arith::CmpIOp>(
    #####:  335:              loc, arith::CmpIPredicate::ult, load, min);
call    0 never executed
call    1 never executed
    #####:  336:          min = builder.create<arith::SelectOp>(loc, cmp, load, min);
call    0 never executed
        -:  337:        } else {
    #####:  338:          min = load;
        -:  339:        }
        -:  340:      }
        -:  341:    }
        -:  342:  }
        -:  343:
    #####:  344:  if (needsUniv) {
branch  0 never executed
branch  1 never executed
    #####:  345:    assert(!min);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  346:    // Otherwise, universal index is the minimal pidx.
    #####:  347:    min = after->getArguments().back();
call    0 never executed
        -:  348:  }
        -:  349:
    #####:  350:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  351:    // All dense dim (as well as sparse output tensor) shared the same pidx in
        -:  352:    // the while loop.
    #####:  353:    if (isDenseDLT(dimTypes[tid][dim])) {
branch  0 never executed
branch  1 never executed
    #####:  354:      pidxs[tid][dim] = min;
call    0 never executed
        -:  355:      // generate pidx for dense dim (pidx = i * sz + j)
    #####:  356:      auto enc = getSparseTensorEncoding(tensors[tid].getType());
call    0 never executed
    #####:  357:      if (enc && !isSparseOutput(tid))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  358:        pidxs[tid][dim] = genAddress(builder, loc, tid, dim, min);
call    0 never executed
        -:  359:    }
        -:  360:    // NOTE: we can also prepares for next dim here in advance
        -:  361:  }
        -:  362:  // Sets up the loop stack.
    #####:  363:  loopStack.emplace_back(tids, dims, whileOp, min);
call    0 never executed
    #####:  364:  assert(loopStack.size() == loopSeqStack.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  365:
        -:  366:  // Emits extra locals
    #####:  367:  emitExtraLocalsForTensorsAtDenseDims(builder, loc, extraTids, extraDims);
call    0 never executed
        -:  368:
        -:  369:  // Updates reduction variables
    #####:  370:  assert(after->getNumArguments() == o + reduc.size() + (needsUniv ? 1 : 0));
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
        -:  371:  // In-place update on reduction variable.
    #####:  372:  for (unsigned i = 0, e = reduc.size(); i < e; i++)
branch  0 never executed
branch  1 never executed
    #####:  373:    reduc[i] = after->getArgument(o + i);
branch  0 never executed
branch  1 never executed
        -:  374:
    #####:  375:  return whileOp;
branch  0 never executed
branch  1 never executed
        -:  376:}
        -:  377:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter26prepareLoopOverTensorAtDimERNS_9OpBuilderENS_8LocationEmm called 0 returned 0% blocks executed 0%
    #####:  378:void SparseTensorLoopEmitter::prepareLoopOverTensorAtDim(OpBuilder &builder,
        -:  379:                                                         Location loc,
        -:  380:                                                         size_t tid,
        -:  381:                                                         size_t dim) {
    #####:  382:  assert(dimTypes[tid].size() > dim);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  383:  auto dimType = dimTypes[tid][dim];
branch  0 never executed
branch  1 never executed
        -:  384:
    #####:  385:  if (isDenseDLT(dimType))
branch  0 never executed
branch  1 never executed
        -:  386:    return;
        -:  387:
        -:  388:  // Either the first dimension, or the previous dimension has been set.
    #####:  389:  assert(dim == 0 || pidxs[tid][dim - 1]);
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  390:  Value c0 = constantIndex(builder, loc, 0);
call    0 never executed
    #####:  391:  Value c1 = constantIndex(builder, loc, 1);
call    0 never executed
    #####:  392:  if (isCompressedDLT(dimType)) {
branch  0 never executed
branch  1 never executed
    #####:  393:    Value ptr = ptrBuffer[tid][dim];
branch  0 never executed
branch  1 never executed
        -:  394:
    #####:  395:    Value pLo = dim == 0 ? c0 : pidxs[tid][dim - 1];
branch  0 never executed
branch  1 never executed
    #####:  396:    pidxs[tid][dim] = genIndexLoad(builder, loc, ptr, pLo);
call    0 never executed
call    1 never executed
        -:  397:
    #####:  398:    Value pHi = builder.create<arith::AddIOp>(loc, pLo, c1);
call    0 never executed
call    1 never executed
    #####:  399:    highs[tid][dim] = genIndexLoad(builder, loc, ptr, pHi);
call    0 never executed
    #####:  400:    return;
        -:  401:  }
    #####:  402:  if (isSingletonDLT(dimType)) {
branch  0 never executed
branch  1 never executed
    #####:  403:    Value pLo = dim == 0 ? c0 : pidxs[tid][dim - 1];
branch  0 never executed
branch  1 never executed
    #####:  404:    Value pHi = builder.create<arith::AddIOp>(loc, pLo, c1);
call    0 never executed
        -:  405:
    #####:  406:    pidxs[tid][dim] = pLo;
    #####:  407:    highs[tid][dim] = pHi;
    #####:  408:    return;
        -:  409:  }
        -:  410:
    #####:  411:  llvm_unreachable("Unrecognizable dimesion type!");
call    0 never executed
        -:  412:}
        -:  413:
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter36emitExtraLocalsForTensorsAtDenseDimsERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefImEES7_ called 0 returned 0% blocks executed 0%
    #####:  414:void SparseTensorLoopEmitter::emitExtraLocalsForTensorsAtDenseDims(
        -:  415:    OpBuilder &builder, Location loc, ArrayRef<size_t> tids,
        -:  416:    ArrayRef<size_t> dims) {
        -:  417:  // Initialize dense positions. Note that we generate dense indices of the
        -:  418:  // output tensor unconditionally, since they may not appear in the lattice,
        -:  419:  // but may be needed for linearized codegen.
    #####:  420:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  421:    assert(isDenseDLT(dimTypes[tid][dim]));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  422:    auto enc = getSparseTensorEncoding(tensors[tid].getType());
call    0 never executed
    #####:  423:    if (enc && !isSparseOutput(tid)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  424:      bool validPidx = dim == 0 || pidxs[tid][dim - 1];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  425:      if (!validPidx) {
        -:  426:        // We might not find the pidx for the sparse output tensor as it is
        -:  427:        // unconditionally required by the sparsification.
    #####:  428:        assert(isOutputTensor(tid));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  429:        continue;
        -:  430:      }
    #####:  431:      pidxs[tid][dim] = genAddress(builder, loc, tid, dim, loopStack.back().iv);
call    0 never executed
        -:  432:      // NOTE: we can also prepares for next dim here in advance
        -:  433:    }
        -:  434:  }
    #####:  435:}
        -:  436:
        -:  437:SmallVector<Value, 2>
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter11exitForLoopERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefINS_5ValueEEE called 0 returned 0% blocks executed 0%
    #####:  438:SparseTensorLoopEmitter::exitForLoop(OpBuilder &builder, Location loc,
        -:  439:                                     ArrayRef<Value> reduc) {
    #####:  440:  LoopLevelInfo &loopInfo = loopStack.back();
call    0 never executed
    #####:  441:  auto &dims = loopStack.back().dims;
    #####:  442:  auto &tids = loopStack.back().tids;
    #####:  443:  auto forOp = llvm::cast<scf::ForOp>(loopInfo.loop);
call    0 never executed
    #####:  444:  if (!reduc.empty()) {
branch  0 never executed
branch  1 never executed
    #####:  445:    assert(reduc.size() == forOp.getNumResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  446:    builder.setInsertionPointToEnd(forOp.getBody());
call    0 never executed
call    1 never executed
    #####:  447:    builder.create<scf::YieldOp>(loc, reduc);
call    0 never executed
        -:  448:  }
        -:  449:
        -:  450:  // Finished iterating a tensor, clean up
        -:  451:  // We only do the clean up on for loop as while loops do not necessarily
        -:  452:  // finish the iteration on a sparse tensor
    #####:  453:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  454:    // Reset to null.
    #####:  455:    coord[tid][dim] = Value();
branch  0 never executed
branch  1 never executed
    #####:  456:    pidxs[tid][dim] = Value();
        -:  457:    // Dense dimension, high is fixed.
    #####:  458:    if (!isDenseDLT(dimTypes[tid][dim]))
branch  0 never executed
branch  1 never executed
    #####:  459:      highs[tid][dim] = Value();
        -:  460:  }
        -:  461:  // exit the loop
    #####:  462:  builder.setInsertionPointAfter(forOp);
call    0 never executed
    #####:  463:  return forOp.getResults();
call    0 never executed
call    1 never executed
        -:  464:}
        -:  465:
        -:  466:SmallVector<Value, 2>
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter19exitCoiterationLoopERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefINS_5ValueEEE called 0 returned 0% blocks executed 0%
    #####:  467:SparseTensorLoopEmitter::exitCoiterationLoop(OpBuilder &builder, Location loc,
        -:  468:                                             ArrayRef<Value> reduc) {
    #####:  469:  auto whileOp = llvm::cast<scf::WhileOp>(loopStack.back().loop);
call    0 never executed
    #####:  470:  auto &dims = loopStack.back().dims;
call    0 never executed
    #####:  471:  auto &tids = loopStack.back().tids;
    #####:  472:  Value iv = loopStack.back().iv;
        -:  473:  // Generation while loop induction at the end.
    #####:  474:  builder.setInsertionPointToEnd(&whileOp.getAfter().front());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  475:  // Finalize the induction. Note that the induction could be performed
        -:  476:  // in the individual if-branches to avoid re-evaluating the conditions.
        -:  477:  // However, that would result in a rather elaborate forest of yield
        -:  478:  // instructions during code generation. Moreover, performing the induction
        -:  479:  // after the if-statements more closely resembles code generated by TACO.
    #####:  480:  unsigned o = 0;
    #####:  481:  SmallVector<Value, 4> operands;
call    0 never executed
    #####:  482:  Value one = constantIndex(builder, loc, 1);
call    0 never executed
    #####:  483:  for (auto [tid, dim] : llvm::zip(tids, dims)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  484:    if (isCompressedDLT(dimTypes[tid][dim]) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  485:        isSingletonDLT(dimTypes[tid][dim])) {
branch  0 never executed
branch  1 never executed
    #####:  486:      Value op1 = coord[tid][dim];
call    0 never executed
    #####:  487:      Value op3 = pidxs[tid][dim];
    #####:  488:      Value cmp =
    #####:  489:          builder.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq, op1, iv);
call    0 never executed
call    1 never executed
    #####:  490:      Value add = builder.create<arith::AddIOp>(loc, op3, one);
call    0 never executed
call    1 never executed
    #####:  491:      operands.push_back(builder.create<arith::SelectOp>(loc, cmp, add, op3));
call    0 never executed
call    1 never executed
        -:  492:      // Following loops continue iteration from the break point of the
        -:  493:      // current while loop.
    #####:  494:      pidxs[tid][dim] = whileOp->getResult(o++);
branch  0 never executed
branch  1 never executed
        -:  495:      // The coordinates are invalid now.
    #####:  496:      coord[tid][dim] = nullptr;
        -:  497:      // highs remains unchanged.
        -:  498:    }
        -:  499:  }
        -:  500:
        -:  501:  // Reduction value from users.
    #####:  502:  SmallVector<Value, 2> ret;
    #####:  503:  for (auto red : reduc) {
branch  0 never executed
branch  1 never executed
    #####:  504:    operands.push_back(red);
call    0 never executed
    #####:  505:    ret.push_back(whileOp->getResult(o++));
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  506:  }
        -:  507:
        -:  508:  // An (optional) universal index.
    #####:  509:  if (operands.size() < whileOp.getNumResults()) {
branch  0 never executed
branch  1 never executed
    #####:  510:    assert(operands.size() + 1 == whileOp.getNumResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  511:    // The last one is the universial index.
    #####:  512:    operands.push_back(builder.create<arith::AddIOp>(loc, iv, one));
call    0 never executed
call    1 never executed
        -:  513:    // update the loop starting point of current loop sequence
    #####:  514:    loopSeqStack.back() = whileOp->getResult(o++);
branch  0 never executed
branch  1 never executed
        -:  515:  }
        -:  516:
    #####:  517:  assert(o == operands.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  518:  builder.create<scf::YieldOp>(loc, operands);
call    0 never executed
    #####:  519:  builder.setInsertionPointAfter(whileOp);
call    0 never executed
    #####:  520:  return ret;
branch  0 never executed
branch  1 never executed
        -:  521:}
        -:  522:
        -:  523:SmallVector<Value, 2>
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter15exitCurrentLoopERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefINS_5ValueEEE called 0 returned 0% blocks executed 0%
    #####:  524:SparseTensorLoopEmitter::exitCurrentLoop(OpBuilder &builder, Location loc,
        -:  525:                                         ArrayRef<Value> reduc) {
        -:  526:  // Clean up the values, it would help use to discover potential bug at a
        -:  527:  // earlier stage (instead of silently using a wrong value).
    #####:  528:  LoopLevelInfo &loopInfo = loopStack.back();
branch  0 never executed
branch  1 never executed
    #####:  529:  assert(loopInfo.tids.size() == loopInfo.dims.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  530:  SmallVector<Value, 2> red;
call    0 never executed
    #####:  531:  if (llvm::isa<scf::WhileOp>(loopInfo.loop)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  532:    red = exitCoiterationLoop(builder, loc, reduc);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  533:  } else {
    #####:  534:    red = exitForLoop(builder, loc, reduc);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  535:  }
        -:  536:
    #####:  537:  assert(loopStack.size() == loopSeqStack.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  538:  loopStack.pop_back();
call    0 never executed
    #####:  539:  return red;
        -:  540:}
        -:  541:
        -:  542://===----------------------------------------------------------------------===//
        -:  543:// ExecutionEngine/SparseTensorUtils helper functions.
        -:  544://===----------------------------------------------------------------------===//
        -:  545:
function _ZN4mlir13sparse_tensor20overheadTypeEncodingEj called 0 returned 0% blocks executed 0%
    #####:  546:OverheadType mlir::sparse_tensor::overheadTypeEncoding(unsigned width) {
    #####:  547:  switch (width) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
        -:  548:  case 64:
        -:  549:    return OverheadType::kU64;
    #####:  550:  case 32:
    #####:  551:    return OverheadType::kU32;
    #####:  552:  case 16:
    #####:  553:    return OverheadType::kU16;
    #####:  554:  case 8:
    #####:  555:    return OverheadType::kU8;
    #####:  556:  case 0:
    #####:  557:    return OverheadType::kIndex;
        -:  558:  }
    #####:  559:  llvm_unreachable("Unsupported overhead bitwidth");
call    0 never executed
        -:  560:}
        -:  561:
function _ZN4mlir13sparse_tensor20overheadTypeEncodingENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  562:OverheadType mlir::sparse_tensor::overheadTypeEncoding(Type tp) {
    #####:  563:  if (tp.isIndex())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  564:    return OverheadType::kIndex;
    #####:  565:  if (auto intTp = tp.dyn_cast<IntegerType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  566:    return overheadTypeEncoding(intTp.getWidth());
call    0 never executed
call    1 never executed
    #####:  567:  llvm_unreachable("Unknown overhead type");
call    0 never executed
        -:  568:}
        -:  569:
function _ZN4mlir13sparse_tensor15getOverheadTypeERNS_7BuilderENS0_12OverheadTypeE called 0 returned 0% blocks executed 0%
    #####:  570:Type mlir::sparse_tensor::getOverheadType(Builder &builder, OverheadType ot) {
    #####:  571:  switch (ot) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  572:  case OverheadType::kIndex:
    #####:  573:    return builder.getIndexType();
call    0 never executed
    #####:  574:  case OverheadType::kU64:
    #####:  575:    return builder.getIntegerType(64);
call    0 never executed
    #####:  576:  case OverheadType::kU32:
    #####:  577:    return builder.getIntegerType(32);
call    0 never executed
    #####:  578:  case OverheadType::kU16:
    #####:  579:    return builder.getIntegerType(16);
call    0 never executed
    #####:  580:  case OverheadType::kU8:
    #####:  581:    return builder.getIntegerType(8);
call    0 never executed
        -:  582:  }
    #####:  583:  llvm_unreachable("Unknown OverheadType");
call    0 never executed
        -:  584:}
        -:  585:
function _ZN4mlir13sparse_tensor27pointerOverheadTypeEncodingERKNS0_24SparseTensorEncodingAttrE called 0 returned 0% blocks executed 0%
    #####:  586:OverheadType mlir::sparse_tensor::pointerOverheadTypeEncoding(
        -:  587:    const SparseTensorEncodingAttr &enc) {
    #####:  588:  return overheadTypeEncoding(enc.getPointerBitWidth());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  589:}
        -:  590:
function _ZN4mlir13sparse_tensor25indexOverheadTypeEncodingERKNS0_24SparseTensorEncodingAttrE called 0 returned 0% blocks executed 0%
    #####:  591:OverheadType mlir::sparse_tensor::indexOverheadTypeEncoding(
        -:  592:    const SparseTensorEncodingAttr &enc) {
    #####:  593:  return overheadTypeEncoding(enc.getIndexBitWidth());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  594:}
        -:  595:
function _ZN4mlir13sparse_tensor22getPointerOverheadTypeERNS_7BuilderERKNS0_24SparseTensorEncodingAttrE called 0 returned 0% blocks executed 0%
    #####:  596:Type mlir::sparse_tensor::getPointerOverheadType(
        -:  597:    Builder &builder, const SparseTensorEncodingAttr &enc) {
    #####:  598:  return getOverheadType(builder, pointerOverheadTypeEncoding(enc));
call    0 never executed
call    1 never executed
        -:  599:}
        -:  600:
function _ZN4mlir13sparse_tensor20getIndexOverheadTypeERNS_7BuilderERKNS0_24SparseTensorEncodingAttrE called 0 returned 0% blocks executed 0%
    #####:  601:Type mlir::sparse_tensor::getIndexOverheadType(
        -:  602:    Builder &builder, const SparseTensorEncodingAttr &enc) {
    #####:  603:  return getOverheadType(builder, indexOverheadTypeEncoding(enc));
call    0 never executed
call    1 never executed
        -:  604:}
        -:  605:
        -:  606:// TODO: Adjust the naming convention for the constructors of
        -:  607:// `OverheadType` so we can use the `MLIR_SPARSETENSOR_FOREVERY_O` x-macro
        -:  608:// here instead of `MLIR_SPARSETENSOR_FOREVERY_FIXED_O`; to further reduce
        -:  609:// the possibility of typo bugs or things getting out of sync.
function _ZN4mlir13sparse_tensor26overheadTypeFunctionSuffixENS0_12OverheadTypeE called 0 returned 0% blocks executed 0%
    #####:  610:StringRef mlir::sparse_tensor::overheadTypeFunctionSuffix(OverheadType ot) {
    #####:  611:  switch (ot) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  612:  case OverheadType::kIndex:
    #####:  613:    return "0";
        -:  614:#define CASE(ONAME, O)                                                         \
        -:  615:  case OverheadType::kU##ONAME:                                                \
        -:  616:    return #ONAME;
    #####:  617:    MLIR_SPARSETENSOR_FOREVERY_FIXED_O(CASE)
        -:  618:#undef CASE
        -:  619:  }
    #####:  620:  llvm_unreachable("Unknown OverheadType");
call    0 never executed
        -:  621:}
        -:  622:
function _ZN4mlir13sparse_tensor26overheadTypeFunctionSuffixENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  623:StringRef mlir::sparse_tensor::overheadTypeFunctionSuffix(Type tp) {
    #####:  624:  return overheadTypeFunctionSuffix(overheadTypeEncoding(tp));
call    0 never executed
call    1 never executed
        -:  625:}
        -:  626:
function _ZN4mlir13sparse_tensor19primaryTypeEncodingENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  627:PrimaryType mlir::sparse_tensor::primaryTypeEncoding(Type elemTp) {
    #####:  628:  if (elemTp.isF64())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  629:    return PrimaryType::kF64;
    #####:  630:  if (elemTp.isF32())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  631:    return PrimaryType::kF32;
    #####:  632:  if (elemTp.isF16())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  633:    return PrimaryType::kF16;
    #####:  634:  if (elemTp.isBF16())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  635:    return PrimaryType::kBF16;
    #####:  636:  if (elemTp.isInteger(64))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  637:    return PrimaryType::kI64;
    #####:  638:  if (elemTp.isInteger(32))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  639:    return PrimaryType::kI32;
    #####:  640:  if (elemTp.isInteger(16))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  641:    return PrimaryType::kI16;
    #####:  642:  if (elemTp.isInteger(8))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  643:    return PrimaryType::kI8;
    #####:  644:  if (auto complexTp = elemTp.dyn_cast<ComplexType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  645:    auto complexEltTp = complexTp.getElementType();
call    0 never executed
    #####:  646:    if (complexEltTp.isF64())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  647:      return PrimaryType::kC64;
    #####:  648:    if (complexEltTp.isF32())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  649:      return PrimaryType::kC32;
        -:  650:  }
    #####:  651:  llvm_unreachable("Unknown primary type");
call    0 never executed
        -:  652:}
        -:  653:
function _ZN4mlir13sparse_tensor25primaryTypeFunctionSuffixENS0_11PrimaryTypeE called 0 returned 0% blocks executed 0%
    #####:  654:StringRef mlir::sparse_tensor::primaryTypeFunctionSuffix(PrimaryType pt) {
    #####:  655:  switch (pt) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
branch 10 never executed
        -:  656:#define CASE(VNAME, V)                                                         \
        -:  657:  case PrimaryType::k##VNAME:                                                  \
        -:  658:    return #VNAME;
    #####:  659:    MLIR_SPARSETENSOR_FOREVERY_V(CASE)
        -:  660:#undef CASE
        -:  661:  }
    #####:  662:  llvm_unreachable("Unknown PrimaryType");
call    0 never executed
        -:  663:}
        -:  664:
function _ZN4mlir13sparse_tensor25primaryTypeFunctionSuffixENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  665:StringRef mlir::sparse_tensor::primaryTypeFunctionSuffix(Type elemTp) {
    #####:  666:  return primaryTypeFunctionSuffix(primaryTypeEncoding(elemTp));
call    0 never executed
call    1 never executed
        -:  667:}
        -:  668:
        -:  669://===----------------------------------------------------------------------===//
        -:  670:// Misc code generators.
        -:  671://===----------------------------------------------------------------------===//
        -:  672:
function _ZN4mlir13sparse_tensor10getOneAttrERNS_7BuilderENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  673:mlir::Attribute mlir::sparse_tensor::getOneAttr(Builder &builder, Type tp) {
    #####:  674:  if (tp.isa<FloatType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  675:    return builder.getFloatAttr(tp, 1.0);
call    0 never executed
    #####:  676:  if (tp.isa<IndexType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  677:    return builder.getIndexAttr(1);
call    0 never executed
    #####:  678:  if (auto intTp = tp.dyn_cast<IntegerType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  679:    return builder.getIntegerAttr(tp, APInt(intTp.getWidth(), 1));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  680:  if (tp.isa<RankedTensorType, VectorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  681:    auto shapedTp = tp.cast<ShapedType>();
call    0 never executed
    #####:  682:    if (auto one = getOneAttr(builder, shapedTp.getElementType()))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  683:      return DenseElementsAttr::get(shapedTp, one);
call    0 never executed
        -:  684:  }
    #####:  685:  llvm_unreachable("Unsupported attribute type");
call    0 never executed
        -:  686:}
        -:  687:
function _ZN4mlir13sparse_tensor12genIsNonzeroERNS_9OpBuilderENS_8LocationENS_5ValueE called 0 returned 0% blocks executed 0%
    #####:  688:Value mlir::sparse_tensor::genIsNonzero(OpBuilder &builder, mlir::Location loc,
        -:  689:                                        Value v) {
    #####:  690:  Type tp = v.getType();
call    0 never executed
    #####:  691:  Value zero = constantZero(builder, loc, tp);
call    0 never executed
    #####:  692:  if (tp.isa<FloatType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  693:    return builder.create<arith::CmpFOp>(loc, arith::CmpFPredicate::UNE, v,
    #####:  694:                                         zero);
call    0 never executed
    #####:  695:  if (tp.isIntOrIndex())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  696:    return builder.create<arith::CmpIOp>(loc, arith::CmpIPredicate::ne, v,
    #####:  697:                                         zero);
call    0 never executed
    #####:  698:  if (tp.dyn_cast<ComplexType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  699:    return builder.create<complex::NotEqualOp>(loc, v, zero);
call    0 never executed
    #####:  700:  llvm_unreachable("Non-numeric type");
call    0 never executed
        -:  701:}
        -:  702:
function _ZN4mlir13sparse_tensor18genReshapeDstShapeENS_8LocationERNS_15PatternRewriterERN4llvm11SmallVectorINS_5ValueELj4EEENS4_8ArrayRefIS6_EENS9_IlEENS9_INS5_IlLj2EEEEE called 0 returned 0% blocks executed 0%
    #####:  703:void mlir::sparse_tensor::genReshapeDstShape(
        -:  704:    Location loc, PatternRewriter &rewriter, SmallVector<Value, 4> &dstShape,
        -:  705:    ArrayRef<Value> srcShape, ArrayRef<int64_t> staticDstShape,
        -:  706:    ArrayRef<ReassociationIndices> reassociation) {
        -:  707:  // Collapse shape.
    #####:  708:  if (reassociation.size() < srcShape.size()) {
branch  0 never executed
branch  1 never executed
    #####:  709:    unsigned start = 0;
    #####:  710:    for (const auto &map : llvm::enumerate(reassociation)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  711:      auto dstDim = constantIndex(rewriter, loc, 1);
call    0 never executed
    #####:  712:      for (unsigned i = start; i < start + map.value().size(); i++) {
branch  0 never executed
branch  1 never executed
    #####:  713:        dstDim = rewriter.create<arith::MulIOp>(loc, dstDim, srcShape[i]);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  714:      }
    #####:  715:      dstShape.push_back(dstDim);
call    0 never executed
    #####:  716:      start = start + map.value().size();
call    0 never executed
        -:  717:    }
    #####:  718:    assert(start == srcShape.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  719:    return;
        -:  720:  }
        -:  721:
        -:  722:  // Expand shape.
    #####:  723:  assert(reassociation.size() == srcShape.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  724:  unsigned start = 0;
        -:  725:  // Expand the i-th dimension in srcShape.
    #####:  726:  for (unsigned i = 0, size = srcShape.size(); i < size; i++) {
branch  0 never executed
branch  1 never executed
    #####:  727:    const auto &map = reassociation[i];
branch  0 never executed
branch  1 never executed
    #####:  728:    auto srcDim = srcShape[i];
branch  0 never executed
branch  1 never executed
        -:  729:    // Iterate through dimensions expanded from the i-th dimension.
    #####:  730:    for (unsigned j = start; j < start + map.size(); j++) {
branch  0 never executed
branch  1 never executed
        -:  731:      // There can be only one dynamic sized dimension among dimensions
        -:  732:      // expanded from the i-th dimension in srcShape.
        -:  733:      // For example, if srcDim = 8, then the expanded shape could be <2x?x2>,
        -:  734:      // but not <2x?x?>.
    #####:  735:      if (staticDstShape[j] == ShapedType::kDynamicSize) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  736:        // The expanded dimension has dynamic size. We compute the dimension
        -:  737:        // by dividing srcDim by the product of the static dimensions.
        -:  738:        int64_t product = 1;
    #####:  739:        for (unsigned k = start; k < start + map.size(); k++) {
branch  0 never executed
branch  1 never executed
    #####:  740:          if (staticDstShape[k] != ShapedType::kDynamicSize) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  741:            product *= staticDstShape[k];
        -:  742:          }
        -:  743:        }
        -:  744:        // Compute the dynamic dimension size.
    #####:  745:        Value productVal = constantIndex(rewriter, loc, product);
call    0 never executed
    #####:  746:        Value dynamicSize =
    #####:  747:            rewriter.create<arith::DivUIOp>(loc, srcDim, productVal);
call    0 never executed
call    1 never executed
    #####:  748:        dstShape.push_back(dynamicSize);
call    0 never executed
        -:  749:      } else {
        -:  750:        // The expanded dimension is statically known.
    #####:  751:        dstShape.push_back(constantIndex(rewriter, loc, staticDstShape[j]));
call    0 never executed
call    1 never executed
        -:  752:      }
        -:  753:    }
    #####:  754:    start = start + map.size();
        -:  755:  }
    #####:  756:  assert(start == staticDstShape.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  757:}
        -:  758:
function _ZN4mlir13sparse_tensor21translateIndicesArrayERNS_9OpBuilderENS_8LocationEN4llvm8ArrayRefINS4_11SmallVectorIlLj2EEEEENS_10ValueRangeENS5_INS_5ValueEEESB_RNS4_15SmallVectorImplISA_EE called 0 returned 0% blocks executed 0%
    #####:  759:void mlir::sparse_tensor::translateIndicesArray(
        -:  760:    OpBuilder &builder, Location loc,
        -:  761:    ArrayRef<ReassociationIndices> reassociation, ValueRange srcIndices,
        -:  762:    ArrayRef<Value> srcShape, ArrayRef<Value> dstShape,
        -:  763:    SmallVectorImpl<Value> &dstIndices) {
    #####:  764:  unsigned i = 0;
    #####:  765:  unsigned start = 0;
    #####:  766:  unsigned dstRank = dstShape.size();
branch  0 never executed
branch  1 never executed
    #####:  767:  unsigned srcRank = srcShape.size();
    #####:  768:  assert(srcRank == srcIndices.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  769:  bool isCollapse = srcRank > dstRank;
    #####:  770:  ArrayRef<Value> shape = isCollapse ? srcShape : dstShape;
branch  0 never executed
branch  1 never executed
        -:  771:  // Iterate over reassociation map.
    #####:  772:  for (const auto &map : llvm::enumerate(reassociation)) {
branch  0 never executed
branch  1 never executed
        -:  773:    // Prepare strides information in dimension slice.
    #####:  774:    Value linear = constantIndex(builder, loc, 1);
call    0 never executed
    #####:  775:    for (unsigned j = start, end = start + map.value().size(); j < end; j++) {
branch  0 never executed
branch  1 never executed
    #####:  776:      linear = builder.create<arith::MulIOp>(loc, linear, shape[j]);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  777:    }
        -:  778:    // Start expansion.
    #####:  779:    Value val;
    #####:  780:    if (!isCollapse)
branch  0 never executed
branch  1 never executed
    #####:  781:      val = srcIndices[i];
call    0 never executed
        -:  782:    // Iterate over dimension slice.
    #####:  783:    for (unsigned j = start, end = start + map.value().size(); j < end; j++) {
branch  0 never executed
branch  1 never executed
    #####:  784:      linear = builder.create<arith::DivUIOp>(loc, linear, shape[j]);
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  785:      if (isCollapse) {
branch  0 never executed
branch  1 never executed
    #####:  786:        Value old = srcIndices[j];
call    0 never executed
    #####:  787:        Value mul = builder.create<arith::MulIOp>(loc, old, linear);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  788:        val = val ? builder.create<arith::AddIOp>(loc, val, mul) : mul;
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  789:      } else {
    #####:  790:        Value old = val;
    #####:  791:        val = builder.create<arith::DivUIOp>(loc, val, linear);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  792:        assert(dstIndices.size() == j);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  793:        dstIndices.push_back(val);
call    0 never executed
    #####:  794:        val = builder.create<arith::RemUIOp>(loc, old, linear);
call    0 never executed
        -:  795:      }
        -:  796:    }
        -:  797:    // Finalize collapse.
    #####:  798:    if (isCollapse) {
branch  0 never executed
branch  1 never executed
    #####:  799:      assert(dstIndices.size() == i);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  800:      dstIndices.push_back(val);
call    0 never executed
        -:  801:    }
    #####:  802:    start += map.value().size();
call    0 never executed
    #####:  803:    i++;
call    0 never executed
        -:  804:  }
    #####:  805:  assert(dstIndices.size() == dstRank);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  806:}
        -:  807:
function _ZN4mlir13sparse_tensor7getFuncENS_8ModuleOpEN4llvm9StringRefENS_9TypeRangeENS_10ValueRangeENS0_14EmitCInterfaceE called 0 returned 0% blocks executed 0%
    #####:  808:FlatSymbolRefAttr mlir::sparse_tensor::getFunc(ModuleOp module, StringRef name,
        -:  809:                                               TypeRange resultType,
        -:  810:                                               ValueRange operands,
        -:  811:                                               EmitCInterface emitCInterface) {
    #####:  812:  MLIRContext *context = module.getContext();
call    0 never executed
    #####:  813:  auto result = SymbolRefAttr::get(context, name);
call    0 never executed
    #####:  814:  auto func = module.lookupSymbol<func::FuncOp>(result.getAttr());
call    0 never executed
call    1 never executed
    #####:  815:  if (!func) {
branch  0 never executed
branch  1 never executed
    #####:  816:    OpBuilder moduleBuilder(module.getBodyRegion());
call    0 never executed
call    1 never executed
    #####:  817:    func = moduleBuilder.create<func::FuncOp>(
        -:  818:        module.getLoc(), name,
    #####:  819:        FunctionType::get(context, operands.getTypes(), resultType));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  820:    func.setPrivate();
call    0 never executed
    #####:  821:    if (static_cast<bool>(emitCInterface))
branch  0 never executed
branch  1 never executed
    #####:  822:      func->setAttr(LLVM::LLVMDialect::getEmitCWrapperAttrName(),
call    0 never executed
call    1 never executed
    #####:  823:                    UnitAttr::get(context));
call    0 never executed
        -:  824:  }
    #####:  825:  return result;
        -:  826:}
        -:  827:
function _ZN4mlir13sparse_tensor14createFuncCallERNS_9OpBuilderENS_8LocationEN4llvm9StringRefENS_9TypeRangeENS_10ValueRangeENS0_14EmitCInterfaceE called 0 returned 0% blocks executed 0%
    #####:  828:func::CallOp mlir::sparse_tensor::createFuncCall(
        -:  829:    OpBuilder &builder, Location loc, StringRef name, TypeRange resultType,
        -:  830:    ValueRange operands, EmitCInterface emitCInterface) {
    #####:  831:  auto module = builder.getBlock()->getParentOp()->getParentOfType<ModuleOp>();
call    0 never executed
call    1 never executed
    #####:  832:  FlatSymbolRefAttr fn =
    #####:  833:      getFunc(module, name, resultType, operands, emitCInterface);
call    0 never executed
    #####:  834:  return builder.create<func::CallOp>(loc, resultType, fn, operands);
call    0 never executed
        -:  835:}
        -:  836:
function _ZN4mlir13sparse_tensor20getOpaquePointerTypeERNS_9OpBuilderE called 0 returned 0% blocks executed 0%
    #####:  837:Type mlir::sparse_tensor::getOpaquePointerType(OpBuilder &builder) {
    #####:  838:  return LLVM::LLVMPointerType::get(builder.getI8Type());
call    0 never executed
call    1 never executed
        -:  839:}
        -:  840:
function _ZN4mlir13sparse_tensor9genAllocaERNS_9OpBuilderENS_8LocationEjNS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  841:Value mlir::sparse_tensor::genAlloca(OpBuilder &builder, Location loc,
        -:  842:                                     unsigned sz, Type tp) {
    #####:  843:  return genAlloca(builder, loc, constantIndex(builder, loc, sz), tp);
call    0 never executed
call    1 never executed
        -:  844:}
        -:  845:
function _ZN4mlir13sparse_tensor9genAllocaERNS_9OpBuilderENS_8LocationENS_5ValueENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  846:Value mlir::sparse_tensor::genAlloca(OpBuilder &builder, Location loc, Value sz,
        -:  847:                                     Type tp) {
    #####:  848:  auto memTp = MemRefType::get({ShapedType::kDynamicSize}, tp);
call    0 never executed
call    1 never executed
    #####:  849:  return builder.create<memref::AllocaOp>(loc, memTp, ValueRange{sz});
call    0 never executed
call    1 never executed
        -:  850:}
        -:  851:
function _ZN4mlir13sparse_tensor15genAllocaScalarERNS_9OpBuilderENS_8LocationENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  852:Value mlir::sparse_tensor::genAllocaScalar(OpBuilder &builder, Location loc,
        -:  853:                                           Type tp) {
    #####:  854:  return builder.create<memref::AllocaOp>(loc, MemRefType::get({}, tp));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  855:}
        -:  856:
function _ZN4mlir13sparse_tensor16allocDenseTensorERNS_9OpBuilderENS_8LocationENS_16RankedTensorTypeENS_10ValueRangeE called 0 returned 0% blocks executed 0%
    #####:  857:Value mlir::sparse_tensor::allocDenseTensor(OpBuilder &builder, Location loc,
        -:  858:                                            RankedTensorType tensorTp,
        -:  859:                                            ValueRange sizes) {
    #####:  860:  Type elemTp = tensorTp.getElementType();
call    0 never executed
    #####:  861:  auto shape = tensorTp.getShape();
call    0 never executed
    #####:  862:  auto memTp = MemRefType::get(shape, elemTp);
call    0 never executed
call    1 never executed
    #####:  863:  SmallVector<Value> dynamicSizes;
call    0 never executed
    #####:  864:  for (unsigned i = 0, rank = tensorTp.getRank(); i < rank; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  865:    if (shape[i] == ShapedType::kDynamicSize)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  866:      dynamicSizes.push_back(sizes[i]);
call    0 never executed
call    1 never executed
        -:  867:  }
    #####:  868:  Value mem = builder.create<memref::AllocOp>(loc, memTp, dynamicSizes);
call    0 never executed
call    1 never executed
    #####:  869:  Value zero = constantZero(builder, loc, elemTp);
call    0 never executed
    #####:  870:  builder.create<linalg::FillOp>(loc, ValueRange{zero}, ValueRange{mem});
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  871:  return mem;
branch  0 never executed
branch  1 never executed
        -:  872:}
        -:  873:
function _ZN4mlir13sparse_tensor16genValueForDenseERNS_9OpBuilderENS_8LocationENS_5ValueENS_10ValueRangeE called 0 returned 0% blocks executed 0%
    #####:  874:Value mlir::sparse_tensor::genValueForDense(OpBuilder &builder, Location loc,
        -:  875:                                            Value tensor, ValueRange ivs) {
    #####:  876:  Value val = builder.create<tensor::ExtractOp>(loc, tensor, ivs);
call    0 never executed
call    1 never executed
    #####:  877:  Value cond = genIsNonzero(builder, loc, val);
call    0 never executed
    #####:  878:  scf::IfOp ifOp = builder.create<scf::IfOp>(loc, cond, /*else*/ false);
call    0 never executed
    #####:  879:  builder.setInsertionPointToStart(&ifOp.getThenRegion().front());
call    0 never executed
call    1 never executed
    #####:  880:  return val;
        -:  881:}
        -:  882:
function _ZN4mlir13sparse_tensor38genDenseTensorOrSparseConstantIterLoopERNS_9OpBuilderENS_8LocationENS_5ValueEjN4llvm12function_refIFvS2_S3_S4_NS_10ValueRangeEEEE called 0 returned 0% blocks executed 0%
    #####:  883:void mlir::sparse_tensor::genDenseTensorOrSparseConstantIterLoop(
        -:  884:    OpBuilder &builder, Location loc, Value src, unsigned rank,
        -:  885:    function_ref<void(OpBuilder &, Location, Value, ValueRange)> bodyBuilder) {
    #####:  886:  SmallVector<Value, 4> indicesArray;
call    0 never executed
    #####:  887:  SmallVector<Value> lo;
branch  0 never executed
branch  1 never executed
    #####:  888:  SmallVector<Value> hi;
branch  0 never executed
branch  1 never executed
    #####:  889:  SmallVector<Value> st;
branch  0 never executed
branch  1 never executed
    #####:  890:  Value zero = constantIndex(builder, loc, 0);
call    0 never executed
    #####:  891:  Value one = constantIndex(builder, loc, 1);
call    0 never executed
    #####:  892:  auto indicesValues = genSplitSparseConstant(builder, loc, src);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  893:  bool isCOOConstant = indicesValues.has_value();
branch  0 never executed
branch  1 never executed
    #####:  894:  Value indices;
    #####:  895:  Value values;
    #####:  896:  if (isCOOConstant) {
branch  0 never executed
branch  1 never executed
    #####:  897:    indices = indicesValues->first;
call    0 never executed
    #####:  898:    values = indicesValues->second;
    #####:  899:    lo.push_back(zero);
call    0 never executed
    #####:  900:    hi.push_back(linalg::createOrFoldDimOp(builder, loc, values, 0));
call    0 never executed
call    1 never executed
    #####:  901:    st.push_back(one);
call    0 never executed
        -:  902:  } else {
    #####:  903:    for (unsigned i = 0; i < rank; i++) {
branch  0 never executed
branch  1 never executed
    #####:  904:      lo.push_back(zero);
call    0 never executed
    #####:  905:      hi.push_back(linalg::createOrFoldDimOp(builder, loc, src, i));
call    0 never executed
call    1 never executed
    #####:  906:      st.push_back(one);
call    0 never executed
        -:  907:    }
        -:  908:  }
        -:  909:
    #####:  910:  scf::buildLoopNest(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
        -:  911:      builder, loc, lo, hi, st, {},
function _ZZN4mlir13sparse_tensor38genDenseTensorOrSparseConstantIterLoopERNS_9OpBuilderENS_8LocationENS_5ValueEjN4llvm12function_refIFvS2_S3_S4_NS_10ValueRangeEEEEENKUlS2_S3_S7_S7_E_clES2_S3_S7_S7_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  912:      [&](OpBuilder &builder, Location loc, ValueRange ivs,
        -:  913:          ValueRange args) -> scf::ValueVector {
    #####:  914:        Value val;
    #####:  915:        if (isCOOConstant)
branch  0 never executed
branch  1 never executed
    #####:  916:          val = genIndexAndValueForSparse(builder, loc, indices, values,
    #####:  917:                                          indicesArray, ivs, rank);
call    0 never executed
        -:  918:        else
    #####:  919:          val = genIndexAndValueForDense(builder, loc, src, indicesArray, ivs);
call    0 never executed
    #####:  920:        bodyBuilder(builder, loc, val, indicesArray);
call    0 never executed
call    1 never executed
    #####:  921:        return {};
        -:  922:      });
    #####:  923:}
        -:  924:
function _ZN4mlir13sparse_tensor12sizesFromSrcERNS_9OpBuilderERN4llvm11SmallVectorINS_5ValueELj4EEENS_8LocationES5_ called 0 returned 0% blocks executed 0%
    #####:  925:void mlir::sparse_tensor::sizesFromSrc(OpBuilder &builder,
        -:  926:                                       SmallVector<Value, 4> &sizes,
        -:  927:                                       Location loc, Value src) {
    #####:  928:  unsigned rank = src.getType().cast<ShapedType>().getRank();
call    0 never executed
call    1 never executed
    #####:  929:  for (unsigned i = 0; i < rank; i++)
branch  0 never executed
branch  1 never executed
    #####:  930:    sizes.push_back(linalg::createOrFoldDimOp(builder, loc, src, i));
call    0 never executed
call    1 never executed
    #####:  931:}
        -:  932:
function _ZN4mlir13sparse_tensor6getTopEPNS_9OperationE called 0 returned 0% blocks executed 0%
    #####:  933:Operation *mlir::sparse_tensor::getTop(Operation *op) {
    #####:  934:  for (; isa<scf::ForOp>(op->getParentOp()) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  935:         isa<scf::WhileOp>(op->getParentOp()) ||
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  936:         isa<scf::ParallelOp>(op->getParentOp()) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
branch  7 never executed
branch  8 never executed
    #####:  937:         isa<scf::IfOp>(op->getParentOp());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  938:       op = op->getParentOp())
branch  0 never executed
branch  1 never executed
        -:  939:    ;
    #####:  940:  return op;
        -:  941:}
