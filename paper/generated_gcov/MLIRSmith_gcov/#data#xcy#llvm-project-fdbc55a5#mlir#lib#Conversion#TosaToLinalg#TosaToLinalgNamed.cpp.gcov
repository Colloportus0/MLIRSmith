        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Conversion/TosaToLinalg/TosaToLinalgNamed.cpp
        -:    0:Graph:../tools/mlir/lib/Conversion/TosaToLinalg/CMakeFiles/obj.MLIRTosaToLinalg.dir/TosaToLinalgNamed.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Conversion/TosaToLinalg/CMakeFiles/obj.MLIRTosaToLinalg.dir/TosaToLinalgNamed.cpp.gcda
        -:    0:Runs:116161
        -:    1://===- TosaToLinalgNamed.cpp - Lowering Tosa to Linalg Named Ops ----------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// These rewriters lower from the Tosa to the Linalg named ops.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#include "mlir/Conversion/TosaToLinalg/TosaToLinalg.h"
        -:   14:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   15:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   16:#include "mlir/Dialect/Math/IR/Math.h"
        -:   17:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   18:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   19:#include "mlir/Dialect/Tensor/Utils/Utils.h"
        -:   20:#include "mlir/Dialect/Tosa/IR/TosaOps.h"
        -:   21:#include "mlir/Dialect/Tosa/Utils/CoversionUtils.h"
        -:   22:#include "mlir/Dialect/Utils/ReshapeOpsUtils.h"
        -:   23:#include "mlir/IR/Matchers.h"
        -:   24:#include "mlir/IR/PatternMatch.h"
        -:   25:#include "mlir/Transforms/DialectConversion.h"
        -:   26:#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
        -:   27:
        -:   28:#include <numeric>
        -:   29:
        -:   30:using namespace mlir;
        -:   31:using namespace mlir::tosa;
        -:   32:
function _ZL8applyPadN4mlir8LocationENS_5ValueEN4llvm8ArrayRefIlEENS_9AttributeERNS_9OpBuilderE called 0 returned 0% blocks executed 0%
    #####:   33:static mlir::Value applyPad(Location loc, Value input, ArrayRef<int64_t> pad,
        -:   34:                            Attribute padAttr, OpBuilder &rewriter) {
        -:   35:  // Input should be padded if necessary.
    #####:   36:  if (llvm::all_of(pad, [](int64_t p) { return p == 0; }))
branch  0 never executed
branch  1 never executed
    #####:   37:    return input;
        -:   38:
    #####:   39:  ShapedType inputTy = input.getType().cast<ShapedType>();
call    0 never executed
    #####:   40:  Type inputETy = inputTy.getElementType();
call    0 never executed
    #####:   41:  auto inputShape = inputTy.getShape();
call    0 never executed
        -:   42:
    #####:   43:  assert((inputShape.size() * 2) == pad.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:   44:
    #####:   45:  SmallVector<int64_t, 4> paddedShape;
    #####:   46:  SmallVector<OpFoldResult, 8> lowIndices;
branch  0 never executed
branch  1 never executed
    #####:   47:  SmallVector<OpFoldResult, 8> highIndices;
branch  0 never executed
branch  1 never executed
    #####:   48:  for (int i = 0, s = inputShape.size(); i < s; i++) {
branch  0 never executed
branch  1 never executed
    #####:   49:    auto lowPad = pad[i * 2];
branch  0 never executed
branch  1 never executed
    #####:   50:    auto highPad = pad[i * 2 + 1];
branch  0 never executed
branch  1 never executed
    #####:   51:    if (ShapedType::isDynamic(inputShape[i]))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:   52:      paddedShape.push_back(inputShape[i]);
call    0 never executed
        -:   53:    else
    #####:   54:      paddedShape.push_back(inputShape[i] + highPad + lowPad);
call    0 never executed
    #####:   55:    lowIndices.push_back(rewriter.getIndexAttr(lowPad));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:   56:    highIndices.push_back(rewriter.getIndexAttr(highPad));
call    0 never executed
call    1 never executed
call    2 never executed
        -:   57:  }
        -:   58:
    #####:   59:  Value padValue = rewriter.create<arith::ConstantOp>(loc, padAttr);
call    0 never executed
call    1 never executed
        -:   60:
    #####:   61:  return rewriter.create<tensor::PadOp>(
    #####:   62:      loc, RankedTensorType::get(paddedShape, inputETy), input, lowIndices,
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   63:      highIndices, padValue);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:   64:}
        -:   65:
function _ZL16reifyConstantDimN4mlir9AttributeERNS_20ImplicitLocOpBuilderE called 0 returned 0% blocks executed 0%
    #####:   66:static mlir::Value reifyConstantDim(Attribute attr,
        -:   67:                                    ImplicitLocOpBuilder &builder) {
    #####:   68:  return builder.createOrFold<arith::IndexCastOp>(
call    0 never executed
    #####:   69:      builder.getIndexType(), builder.create<arith::ConstantOp>(attr));
call    0 never executed
call    1 never executed
        -:   70:}
        -:   71:
        -:   72:// Calculating the output width/height using the formula:
        -:   73:// H = ((IH+pad_top+pad_bottom-(dilation_y*(KH-1)+1))/stride_y)+1
        -:   74:// W = ((IW+pad_left+pad_right-(dilation_x*(KW-1)+1))/stride_x)+1
        -:   75:static mlir::Value
        -:   76:getConvOutputDim(Location loc, Value inputDim, Attribute padBeforeAttr,
        -:   77:                 Attribute padAfterAttr, Value kernelDim, Attribute strideAttr,
        -:   78:                 Attribute dilationAttr, Type inputETy, OpBuilder &rewriter) {
        -:   79:  ImplicitLocOpBuilder builder(loc, rewriter);
        -:   80:  auto one = rewriter.create<arith::ConstantOp>(
        -:   81:      loc, IntegerAttr::get(inputDim.getType(), 1));
        -:   82:  Value padBefore = reifyConstantDim(padBeforeAttr, builder);
        -:   83:  Value paddedBefore = builder.create<arith::AddIOp>(inputDim, padBefore);
        -:   84:  Value padAfter = reifyConstantDim(padAfterAttr, builder);
        -:   85:  Value paddedAfter = builder.create<arith::AddIOp>(paddedBefore, padAfter);
        -:   86:
        -:   87:  Value subOne = builder.create<arith::SubIOp>(kernelDim, one);
        -:   88:  Value dilation = reifyConstantDim(dilationAttr, builder);
        -:   89:  Value dilated = builder.create<arith::MulIOp>(dilation, subOne);
        -:   90:  Value addOne = builder.create<arith::AddIOp>(dilated, one);
        -:   91:
        -:   92:  Value subtract = builder.create<arith::SubIOp>(paddedAfter, addOne);
        -:   93:  Value stride = reifyConstantDim(strideAttr, builder);
        -:   94:  Value divide = builder.create<arith::DivUIOp>(subtract, stride);
        -:   95:  return builder.create<arith::AddIOp>(divide, one);
        -:   96:}
        -:   97:
        -:   98:// Creates a vector of the dynamic output dims for Conv2D and Depthwise_Conv2D
function _ZL23inferDynamicDimsForConvN4mlir8LocationENS_5ValueES1_NS_10ShapedTypeENS_9ArrayAttrES3_S3_llRNS_9OpBuilderE called 0 returned 0% blocks executed 0%
    #####:   99:static SmallVector<Value> inferDynamicDimsForConv(
        -:  100:    Location loc, Value input, Value weight, ShapedType resultTy,
        -:  101:    ArrayAttr padAttr, ArrayAttr strideAttr, ArrayAttr dilationAttr,
        -:  102:    int64_t weightHDim, int64_t weightWDim, OpBuilder &rewriter) {
    #####:  103:  ShapedType inputTy = input.getType().cast<ShapedType>();
call    0 never executed
    #####:  104:  Type inputETy = inputTy.getElementType();
call    0 never executed
    #####:  105:  int64_t inputRank = inputTy.getRank();
call    0 never executed
    #####:  106:  int64_t heightDim = 1;
    #####:  107:  int64_t weightDim = 2;
        -:  108:
    #####:  109:  SmallVector<Value> dynDims;
call    0 never executed
    #####:  110:  dynDims.resize(resultTy.getRank());
call    0 never executed
call    1 never executed
    #####:  111:  for (int i = 0; i < inputRank; i++) {
branch  0 never executed
branch  1 never executed
    #####:  112:    if (inputTy.isDynamicDim(i) && i != heightDim && i != weightDim)
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  113:      dynDims[i] = rewriter.create<tensor::DimOp>(loc, input, i);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  114:  }
        -:  115:
        -:  116:  // Dynamic input height
    #####:  117:  if (inputTy.isDynamicDim(heightDim)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  118:    Value initHDim =
    #####:  119:        rewriter.create<tensor::DimOp>(loc, input, heightDim).getResult();
call    0 never executed
call    1 never executed
    #####:  120:    Value kernelHDim =
    #####:  121:        rewriter.create<tensor::DimOp>(loc, weight, weightHDim).getResult();
call    0 never executed
call    1 never executed
        -:  122:    // H = F(IH, pad_top, pad_bottom, dilation_y, KH, stride_y)
    #####:  123:    dynDims[heightDim] = getConvOutputDim(
branch  0 never executed
branch  1 never executed
    #####:  124:        loc, initHDim, padAttr.getValue()[0], padAttr.getValue()[1], kernelHDim,
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  125:        strideAttr.getValue()[0], dilationAttr.getValue()[0], inputETy,
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
    #####:  126:        rewriter);
call    0 never executed
        -:  127:  }
        -:  128:
        -:  129:  // Dynamic input weight
    #####:  130:  if (inputTy.isDynamicDim(weightDim)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  131:    Value initWDim =
    #####:  132:        rewriter.create<tensor::DimOp>(loc, input, weightDim).getResult();
call    0 never executed
call    1 never executed
    #####:  133:    Value kernelWDim =
    #####:  134:        rewriter.create<tensor::DimOp>(loc, weight, weightWDim).getResult();
call    0 never executed
call    1 never executed
        -:  135:    // W = F(IW, pad_left, pad_right, dilation_x, KW, stride_x)
    #####:  136:    dynDims[weightDim] = getConvOutputDim(
branch  0 never executed
branch  1 never executed
    #####:  137:        loc, initWDim, padAttr.getValue()[2], padAttr.getValue()[3], kernelWDim,
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  138:        strideAttr.getValue()[1], dilationAttr.getValue()[1], inputETy,
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
    #####:  139:        rewriter);
call    0 never executed
        -:  140:  }
        -:  141:
    #####:  142:  SmallVector<Value> filteredDims = condenseValues(dynDims);
call    0 never executed
    #####:  143:  return filteredDims;
branch  0 never executed
branch  1 never executed
        -:  144:}
        -:  145:
        -:  146:// Creates a map to collapse the last dimension of the Depthwise convolution op
        -:  147:// due to a shape mismatch
function _ZL30createDepthwiseConvCollapseMaplRN4llvm11SmallVectorINS0_IN4mlir10AffineExprELj2EEELj4EEERNS1_9OpBuilderE called 0 returned 0% blocks executed 0%
    #####:  148:static void createDepthwiseConvCollapseMap(
        -:  149:    int64_t outputRank, SmallVector<ReassociationExprs, 4> &reassociationMap,
        -:  150:    OpBuilder &rewriter) {
    #####:  151:  reassociationMap.resize(outputRank);
call    0 never executed
    #####:  152:  for (int i = 0; i < outputRank; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  153:    reassociationMap[i].push_back(rewriter.getAffineDimExpr(i));
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -:  154:  }
    #####:  155:  reassociationMap[outputRank - 1].push_back(
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -:  156:      rewriter.getAffineDimExpr(outputRank));
    #####:  157:}
        -:  158:
        -:  159:namespace {
        -:  160:
        -:  161:class ConvConverter : public OpConversionPattern<tosa::Conv2DOp> {
        -:  162:public:
        -:  163:  using OpConversionPattern<tosa::Conv2DOp>::OpConversionPattern;
        -:  164:  LogicalResult
function _ZNK12_GLOBAL__N_113ConvConverter15matchAndRewriteEN4mlir4tosa8Conv2DOpENS2_15Conv2DOpAdaptorERNS1_25ConversionPatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  165:  matchAndRewrite(tosa::Conv2DOp op, OpAdaptor adaptor,
        -:  166:                  ConversionPatternRewriter &rewriter) const final {
    #####:  167:    Location loc = op->getLoc();
call    0 never executed
    #####:  168:    Value input = op->getOperand(0);
call    0 never executed
    #####:  169:    Value weight = op->getOperand(1);
call    0 never executed
    #####:  170:    Value bias = op->getOperand(2);
call    0 never executed
        -:  171:
    #####:  172:    ShapedType inputTy = input.getType().cast<ShapedType>();
call    0 never executed
    #####:  173:    ShapedType weightTy = weight.getType().cast<ShapedType>();
call    0 never executed
    #####:  174:    ShapedType biasTy = bias.getType().cast<ShapedType>();
call    0 never executed
    #####:  175:    ShapedType resultTy = op->getResult(0).getType().cast<ShapedType>();
call    0 never executed
        -:  176:
    #####:  177:    Type inputETy = inputTy.getElementType();
call    0 never executed
    #####:  178:    Type resultETy = resultTy.getElementType();
call    0 never executed
        -:  179:
    #####:  180:    auto padAttr = op->getAttr("pad").cast<ArrayAttr>();
call    0 never executed
call    1 never executed
    #####:  181:    auto strideTosaAttr = op->getAttr("stride").cast<ArrayAttr>();
call    0 never executed
call    1 never executed
    #####:  182:    auto dilationTosaAttr = op->getAttr("dilation").cast<ArrayAttr>();
call    0 never executed
call    1 never executed
    #####:  183:    bool isQuantized = op->hasAttr("quantization_info");
call    0 never executed
        -:  184:
    #####:  185:    if (!weightTy.hasStaticShape() || !biasTy.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  186:      return rewriter.notifyMatchFailure(
    #####:  187:          op, "tosa.conv ops require static shapes for weight and bias");
call    0 never executed
        -:  188:
    #####:  189:    if (inputETy.isUnsignedInteger())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  190:      return rewriter.notifyMatchFailure(
    #####:  191:          op, "tosa.conv ops does not support unsigned integer input");
call    0 never executed
        -:  192:
    #####:  193:    SmallVector<Value> filteredDims = inferDynamicDimsForConv(
        -:  194:        loc, input, weight, resultTy, padAttr, strideTosaAttr, dilationTosaAttr,
    #####:  195:        /*weightHDim=*/1, /*weightWDim=*/2, rewriter);
call    0 never executed
        -:  196:
    #####:  197:    auto weightShape = weightTy.getShape();
call    0 never executed
        -:  198:
        -:  199:    // Apply padding as necessary.
    #####:  200:    Attribute zeroAttr = rewriter.getZeroAttr(inputETy);
call    0 never executed
    #####:  201:    if (isQuantized) {
branch  0 never executed
branch  1 never executed
    #####:  202:      auto quantizationInfo =
    #####:  203:          op->getAttr("quantization_info").cast<tosa::ConvOpQuantizationAttr>();
call    0 never executed
call    1 never executed
    #####:  204:      int64_t iZp = quantizationInfo.getInputZp();
call    0 never executed
        -:  205:
    #####:  206:      int64_t intMin =
    #####:  207:          APInt::getSignedMinValue(inputETy.getIntOrFloatBitWidth())
call    0 never executed
call    1 never executed
    #####:  208:              .getSExtValue();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  209:      int64_t intMax =
    #####:  210:          APInt::getSignedMaxValue(inputETy.getIntOrFloatBitWidth())
call    0 never executed
call    1 never executed
    #####:  211:              .getSExtValue();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  212:
    #####:  213:      if (iZp < intMin || iZp > intMax)
branch  0 never executed
branch  1 never executed
    #####:  214:        return rewriter.notifyMatchFailure(
    #####:  215:            op, "tosa.conv op quantization has zp outside of input range");
call    0 never executed
        -:  216:
    #####:  217:      zeroAttr = rewriter.getIntegerAttr(inputETy, iZp);
call    0 never executed
        -:  218:    }
        -:  219:
    #####:  220:    llvm::SmallVector<int64_t> pad;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  221:    pad.resize(2, 0);
call    0 never executed
    #####:  222:    getValuesFromIntArrayAttribute(padAttr, pad);
call    0 never executed
    #####:  223:    pad.resize(pad.size() + 2, 0);
call    0 never executed
    #####:  224:    input = applyPad(loc, input, pad, zeroAttr, rewriter);
call    0 never executed
        -:  225:
        -:  226:    // Transpose the kernel to match dimension ordering of the linalg
        -:  227:    // convolution operation.
        -:  228:    // TODO(suderman): See if this can be efficiently folded - check whether
        -:  229:    // the input is used anywhere else, if not fold the constant.
    #####:  230:    SmallVector<int64_t> weightPerm{1, 2, 3, 0};
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  231:    SmallVector<int64_t> newWeightShape{weightShape[1], weightShape[2],
branch  0 never executed
branch  1 never executed
    #####:  232:                                        weightShape[3], weightShape[0]};
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####:  233:    auto weightPermAttr = DenseIntElementsAttr::get(
    #####:  234:        RankedTensorType::get({4}, rewriter.getI64Type()), weightPerm);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  235:    Value weightPermValue =
    #####:  236:        rewriter.create<arith::ConstantOp>(loc, weightPermAttr);
call    0 never executed
call    1 never executed
    #####:  237:    Type newWeightTy =
    #####:  238:        RankedTensorType::get(newWeightShape, weightTy.getElementType());
call    0 never executed
call    1 never executed
    #####:  239:    weight = rewriter.create<tosa::TransposeOp>(loc, newWeightTy, weight,
    #####:  240:                                                weightPermValue);
call    0 never executed
call    1 never executed
        -:  241:
    #####:  242:    Attribute resultZeroAttr = rewriter.getZeroAttr(resultETy);
call    0 never executed
    #####:  243:    Value emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  244:        loc, resultTy.getShape(), resultETy, filteredDims);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  245:    Value zero = rewriter.create<arith::ConstantOp>(loc, resultZeroAttr);
call    0 never executed
call    1 never executed
    #####:  246:    Value zeroTensor = rewriter
    #####:  247:                           .create<linalg::FillOp>(loc, ValueRange{zero},
    #####:  248:                                                   ValueRange{emptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  249:                           .result();
call    0 never executed
        -:  250:
        -:  251:    // Extract the attributes for convolution.
    #####:  252:    llvm::SmallVector<int64_t> stride, dilation;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  253:    getValuesFromIntArrayAttribute(strideTosaAttr, stride);
call    0 never executed
    #####:  254:    getValuesFromIntArrayAttribute(dilationTosaAttr, dilation);
call    0 never executed
        -:  255:
        -:  256:    // Create the convolution op.
    #####:  257:    auto strideAttr = DenseIntElementsAttr::get(
    #####:  258:        RankedTensorType::get({2}, rewriter.getI64Type()), stride);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  259:    auto dilationAttr = DenseIntElementsAttr::get(
    #####:  260:        RankedTensorType::get({2}, rewriter.getI64Type()), dilation);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  261:
        -:  262:    // Create maps for the bias broadcasting
    #####:  263:    SmallVector<AffineMap, 4> indexingMaps;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  264:    indexingMaps.push_back(AffineMap::get(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  265:        /*dimCount=*/resultTy.getRank(), /*symbolCount=*/0,
call    0 never executed
    #####:  266:        {rewriter.getAffineDimExpr(3)}, rewriter.getContext()));
call    0 never executed
    #####:  267:    indexingMaps.push_back(rewriter.getMultiDimIdentityMap(resultTy.getRank()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  268:    indexingMaps.push_back(rewriter.getMultiDimIdentityMap(resultTy.getRank()));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  269:
    #####:  270:    Value biasEmptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  271:        loc, resultTy.getShape(), resultETy, filteredDims);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  272:
    #####:  273:    if (isQuantized) {
branch  0 never executed
branch  1 never executed
    #####:  274:      auto quantizationInfo =
    #####:  275:          op->getAttr("quantization_info").cast<tosa::ConvOpQuantizationAttr>();
call    0 never executed
call    1 never executed
    #####:  276:      auto iZp = rewriter.getI32IntegerAttr(quantizationInfo.getInputZp());
call    0 never executed
call    1 never executed
    #####:  277:      auto kZp = rewriter.getI32IntegerAttr(quantizationInfo.getWeightZp());
call    0 never executed
call    1 never executed
        -:  278:
    #####:  279:      auto iZpVal = rewriter.create<arith::ConstantOp>(loc, iZp);
call    0 never executed
    #####:  280:      auto kZpVal = rewriter.create<arith::ConstantOp>(loc, kZp);
call    0 never executed
    #####:  281:      Value conv =
        -:  282:          rewriter
    #####:  283:              .create<linalg::Conv2DNhwcHwcfQOp>(
    #####:  284:                  loc, resultTy, ValueRange{input, weight, iZpVal, kZpVal},
    #####:  285:                  ValueRange{zeroTensor}, strideAttr, dilationAttr)
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  286:              ->getResult(0);
call    0 never executed
        -:  287:
    #####:  288:      Value result =
        -:  289:          rewriter
    #####:  290:              .create<linalg::GenericOp>(
    #####:  291:                  loc, resultTy, ValueRange({bias, conv}), biasEmptyTensor,
call    0 never executed
    #####:  292:                  indexingMaps, getNParallelLoopsAttrs(resultTy.getRank()),
call    0 never executed
function _ZZNK12_GLOBAL__N_113ConvConverter15matchAndRewriteEN4mlir4tosa8Conv2DOpENS2_15Conv2DOpAdaptorERNS1_25ConversionPatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES8_S9_SA_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  293:                  [&](OpBuilder &nestedBuilder, Location nestedLoc,
        -:  294:                      ValueRange args) {
    #####:  295:                    Value added = nestedBuilder.create<arith::AddIOp>(
    #####:  296:                        loc, args[0], args[1]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  297:                    nestedBuilder.create<linalg::YieldOp>(nestedLoc, added);
call    0 never executed
    #####:  298:                  })
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  299:              .getResult(0);
branch  0 never executed
branch  1 never executed
    #####:  300:      rewriter.replaceOp(op, result);
call    0 never executed
call    1 never executed
    #####:  301:      return success();
        -:  302:    }
        -:  303:
    #####:  304:    Value conv = rewriter
    #####:  305:                     .create<linalg::Conv2DNhwcHwcfOp>(
    #####:  306:                         loc, resultTy, ValueRange{input, weight},
    #####:  307:                         ValueRange{zeroTensor}, strideAttr, dilationAttr)
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  308:                     ->getResult(0);
call    0 never executed
        -:  309:
    #####:  310:    Value result =
        -:  311:        rewriter
    #####:  312:            .create<linalg::GenericOp>(
    #####:  313:                loc, resultTy, ValueRange({bias, conv}), biasEmptyTensor,
call    0 never executed
    #####:  314:                indexingMaps, getNParallelLoopsAttrs(resultTy.getRank()),
call    0 never executed
function _ZZNK12_GLOBAL__N_113ConvConverter15matchAndRewriteEN4mlir4tosa8Conv2DOpENS2_15Conv2DOpAdaptorERNS1_25ConversionPatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE0_clES8_S9_SA_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  315:                [&](OpBuilder &nestedBuilder, Location nestedLoc,
        -:  316:                    ValueRange args) {
    #####:  317:                  Value added = nestedBuilder.create<arith::AddFOp>(
    #####:  318:                      loc, args[0], args[1]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  319:                  nestedBuilder.create<linalg::YieldOp>(nestedLoc, added);
call    0 never executed
    #####:  320:                })
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  321:            .getResult(0);
branch  0 never executed
branch  1 never executed
        -:  322:
    #####:  323:    rewriter.replaceOp(op, result);
call    0 never executed
call    1 never executed
    #####:  324:    return success();
branch  0 never executed
branch  1 never executed
        -:  325:  }
        -:  326:};
        -:  327:
        -:  328:class DepthwiseConvConverter
        -:  329:    : public OpConversionPattern<tosa::DepthwiseConv2DOp> {
        -:  330:public:
        -:  331:  using OpConversionPattern<tosa::DepthwiseConv2DOp>::OpConversionPattern;
        -:  332:  LogicalResult
function _ZNK12_GLOBAL__N_122DepthwiseConvConverter15matchAndRewriteEN4mlir4tosa17DepthwiseConv2DOpENS2_24DepthwiseConv2DOpAdaptorERNS1_25ConversionPatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  333:  matchAndRewrite(tosa::DepthwiseConv2DOp op, OpAdaptor adaptor,
        -:  334:                  ConversionPatternRewriter &rewriter) const final {
    #####:  335:    Location loc = op->getLoc();
call    0 never executed
    #####:  336:    Value input = op->getOperand(0);
call    0 never executed
    #####:  337:    Value weight = op->getOperand(1);
call    0 never executed
    #####:  338:    Value bias = op->getOperand(2);
call    0 never executed
        -:  339:
    #####:  340:    ShapedType inputTy = input.getType().cast<ShapedType>();
call    0 never executed
    #####:  341:    ShapedType weightTy = weight.getType().cast<ShapedType>();
call    0 never executed
    #####:  342:    ShapedType biasTy = bias.getType().cast<ShapedType>();
call    0 never executed
    #####:  343:    ShapedType resultTy = op->getResult(0).getType().cast<ShapedType>();
call    0 never executed
    #####:  344:    int64_t resultRank = resultTy.getRank();
call    0 never executed
        -:  345:
    #####:  346:    Type inputETy = inputTy.getElementType();
call    0 never executed
    #####:  347:    Type resultETy = resultTy.getElementType();
call    0 never executed
        -:  348:
    #####:  349:    auto padAttr = op->getAttr("pad").cast<ArrayAttr>();
call    0 never executed
call    1 never executed
    #####:  350:    auto strideTosaAttr = op->getAttr("stride").cast<ArrayAttr>();
call    0 never executed
call    1 never executed
    #####:  351:    auto dilationTosaAttr = op->getAttr("dilation").cast<ArrayAttr>();
call    0 never executed
call    1 never executed
        -:  352:
    #####:  353:    if (!weightTy.hasStaticShape() || !biasTy.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  354:      return rewriter.notifyMatchFailure(
    #####:  355:          op, "tosa.depthwise_conv ops require static shapes");
call    0 never executed
        -:  356:
        -:  357:    // Compute output dynamic dims
    #####:  358:    SmallVector<Value> filteredDims = inferDynamicDimsForConv(
        -:  359:        loc, input, weight, resultTy, padAttr, strideTosaAttr, dilationTosaAttr,
    #####:  360:        0, 1, rewriter);
call    0 never executed
        -:  361:
    #####:  362:    bool isQuantized = op->hasAttr("quantization_info");
call    0 never executed
    #####:  363:    IntegerAttr iZp;
    #####:  364:    IntegerAttr kZp;
    #####:  365:    if (isQuantized) {
branch  0 never executed
branch  1 never executed
    #####:  366:      auto quantizationInfo =
    #####:  367:          op->getAttr("quantization_info").cast<tosa::ConvOpQuantizationAttr>();
call    0 never executed
call    1 never executed
    #####:  368:      iZp = rewriter.getI32IntegerAttr(quantizationInfo.getInputZp());
call    0 never executed
call    1 never executed
    #####:  369:      kZp = rewriter.getI32IntegerAttr(quantizationInfo.getWeightZp());
call    0 never executed
call    1 never executed
        -:  370:    }
        -:  371:
    #####:  372:    auto weightShape = weightTy.getShape();
call    0 never executed
    #####:  373:    auto resultShape = resultTy.getShape();
call    0 never executed
        -:  374:
        -:  375:    // Apply padding as necessary.
    #####:  376:    Attribute zeroAttr = rewriter.getZeroAttr(inputETy);
call    0 never executed
    #####:  377:    if (isQuantized) {
branch  0 never executed
branch  1 never executed
    #####:  378:      auto quantizationInfo =
    #####:  379:          op->getAttr("quantization_info").cast<tosa::ConvOpQuantizationAttr>();
call    0 never executed
call    1 never executed
    #####:  380:      int64_t iZp = quantizationInfo.getInputZp();
call    0 never executed
        -:  381:
    #####:  382:      int64_t intMin =
    #####:  383:          APInt::getSignedMinValue(inputETy.getIntOrFloatBitWidth())
call    0 never executed
call    1 never executed
    #####:  384:              .getSExtValue();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  385:      int64_t intMax =
    #####:  386:          APInt::getSignedMaxValue(inputETy.getIntOrFloatBitWidth())
call    0 never executed
call    1 never executed
    #####:  387:              .getSExtValue();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  388:
    #####:  389:      if (iZp < intMin || iZp > intMax)
branch  0 never executed
branch  1 never executed
    #####:  390:        return rewriter.notifyMatchFailure(
        -:  391:            op, "tosa.depthwise_conv op quantization has zp outside of input "
    #####:  392:                "range");
call    0 never executed
        -:  393:
    #####:  394:      zeroAttr = rewriter.getIntegerAttr(inputETy, iZp);
call    0 never executed
        -:  395:    }
        -:  396:
    #####:  397:    llvm::SmallVector<int64_t> pad;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  398:    pad.resize(2, 0);
call    0 never executed
    #####:  399:    getValuesFromIntArrayAttribute(padAttr, pad);
call    0 never executed
    #####:  400:    pad.resize(pad.size() + 2, 0);
call    0 never executed
        -:  401:
    #####:  402:    input = applyPad(loc, input, pad, zeroAttr, rewriter);
call    0 never executed
        -:  403:
        -:  404:    // Extract the attributes for convolution.
    #####:  405:    llvm::SmallVector<int64_t> stride, dilation;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  406:    getValuesFromIntArrayAttribute(strideTosaAttr, stride);
call    0 never executed
    #####:  407:    getValuesFromIntArrayAttribute(dilationTosaAttr, dilation);
call    0 never executed
        -:  408:
        -:  409:    // Create the convolution op.
    #####:  410:    auto strideAttr = DenseIntElementsAttr::get(
    #####:  411:        RankedTensorType::get({2}, rewriter.getI64Type()), stride);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  412:    auto dilationAttr = DenseIntElementsAttr::get(
    #####:  413:        RankedTensorType::get({2}, rewriter.getI64Type()), dilation);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  414:    ShapedType linalgConvTy =
    #####:  415:        RankedTensorType::get({resultShape[0], resultShape[1], resultShape[2],
branch  0 never executed
branch  1 never executed
    #####:  416:                               weightShape[2], weightShape[3]},
    #####:  417:                              resultETy);
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
call   10 never executed
call   11 never executed
        -:  418:
        -:  419:    // Broadcast the initial value to the output tensor before convolving.
    #####:  420:    SmallVector<AffineMap, 4> indexingMaps;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  421:    indexingMaps.push_back(AffineMap::get(
call    0 never executed
call    1 never executed
call    2 never executed
        -:  422:        /*dimCount=*/resultRank, /*symbolCount=*/0,
    #####:  423:        {rewriter.getAffineDimExpr(3)}, rewriter.getContext()));
call    0 never executed
    #####:  424:    indexingMaps.push_back(rewriter.getMultiDimIdentityMap(resultRank));
call    0 never executed
call    1 never executed
    #####:  425:    indexingMaps.push_back(rewriter.getMultiDimIdentityMap(resultRank));
call    0 never executed
call    1 never executed
        -:  426:
    #####:  427:    Attribute resultZeroAttr = rewriter.getZeroAttr(resultETy);
call    0 never executed
    #####:  428:    Value emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  429:        loc, linalgConvTy.getShape(), resultETy, filteredDims);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  430:    Value zero = rewriter.create<arith::ConstantOp>(loc, resultZeroAttr);
call    0 never executed
call    1 never executed
    #####:  431:    Value zeroTensor = rewriter
    #####:  432:                           .create<linalg::FillOp>(loc, ValueRange{zero},
    #####:  433:                                                   ValueRange{emptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  434:                           .result();
call    0 never executed
        -:  435:
    #####:  436:    Value biasEmptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  437:        loc, resultTy.getShape(), resultETy, filteredDims);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  438:    if (!isQuantized) {
branch  0 never executed
branch  1 never executed
    #####:  439:      Value conv = rewriter
    #####:  440:                       .create<linalg::DepthwiseConv2DNhwcHwcmOp>(
    #####:  441:                           loc, linalgConvTy, ValueRange{input, weight},
    #####:  442:                           ValueRange{zeroTensor}, strideAttr, dilationAttr)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  443:                       .getResult(0);
        -:  444:
    #####:  445:      SmallVector<ReassociationExprs, 4> reassociationMap;
call    0 never executed
    #####:  446:      createDepthwiseConvCollapseMap(resultRank, reassociationMap, rewriter);
call    0 never executed
    #####:  447:      Value convReshape = rewriter.create<tensor::CollapseShapeOp>(
    #####:  448:          loc, resultTy, conv, reassociationMap);
call    0 never executed
call    1 never executed
        -:  449:
    #####:  450:      Value result =
        -:  451:          rewriter
    #####:  452:              .create<linalg::GenericOp>(
    #####:  453:                  loc, resultTy, ValueRange({bias, convReshape}),
call    0 never executed
        -:  454:                  biasEmptyTensor, indexingMaps,
    #####:  455:                  getNParallelLoopsAttrs(resultRank),
function _ZZNK12_GLOBAL__N_122DepthwiseConvConverter15matchAndRewriteEN4mlir4tosa17DepthwiseConv2DOpENS2_24DepthwiseConv2DOpAdaptorERNS1_25ConversionPatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES8_S9_SA_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  456:                  [&](OpBuilder &nestedBuilder, Location nestedLoc,
        -:  457:                      ValueRange args) {
    #####:  458:                    Value added = nestedBuilder.create<arith::AddFOp>(
    #####:  459:                        loc, args[0], args[1]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  460:                    nestedBuilder.create<linalg::YieldOp>(nestedLoc, added);
call    0 never executed
    #####:  461:                  })
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  462:              .getResult(0);
branch  0 never executed
branch  1 never executed
    #####:  463:      rewriter.replaceOp(op, result);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  464:    } else {
    #####:  465:      auto iZpVal = rewriter.create<arith::ConstantOp>(loc, iZp);
call    0 never executed
    #####:  466:      auto kZpVal = rewriter.create<arith::ConstantOp>(loc, kZp);
call    0 never executed
    #####:  467:      Value conv =
        -:  468:          rewriter
    #####:  469:              .create<linalg::DepthwiseConv2DNhwcHwcmQOp>(
    #####:  470:                  loc, linalgConvTy, ValueRange{input, weight, iZpVal, kZpVal},
    #####:  471:                  ValueRange{zeroTensor}, strideAttr, dilationAttr)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  472:              .getResult(0);
    #####:  473:      SmallVector<ReassociationExprs, 4> reassociationMap;
call    0 never executed
    #####:  474:      createDepthwiseConvCollapseMap(resultRank, reassociationMap, rewriter);
call    0 never executed
    #####:  475:      Value convReshape = rewriter.create<tensor::CollapseShapeOp>(
    #####:  476:          loc, resultTy, conv, reassociationMap);
call    0 never executed
call    1 never executed
    #####:  477:      Value result =
        -:  478:          rewriter
    #####:  479:              .create<linalg::GenericOp>(
    #####:  480:                  loc, resultTy, ValueRange({bias, convReshape}),
call    0 never executed
        -:  481:                  biasEmptyTensor, indexingMaps,
    #####:  482:                  getNParallelLoopsAttrs(resultRank),
function _ZZNK12_GLOBAL__N_122DepthwiseConvConverter15matchAndRewriteEN4mlir4tosa17DepthwiseConv2DOpENS2_24DepthwiseConv2DOpAdaptorERNS1_25ConversionPatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE0_clES8_S9_SA_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  483:                  [&](OpBuilder &nestedBuilder, Location nestedLoc,
        -:  484:                      ValueRange args) {
    #####:  485:                    Value added = nestedBuilder.create<arith::AddIOp>(
    #####:  486:                        loc, args[0], args[1]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  487:                    nestedBuilder.create<linalg::YieldOp>(nestedLoc, added);
call    0 never executed
    #####:  488:                  })
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  489:              .getResult(0);
branch  0 never executed
branch  1 never executed
    #####:  490:      rewriter.replaceOp(op, result);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  491:    }
    #####:  492:    return success();
branch  0 never executed
branch  1 never executed
        -:  493:  }
        -:  494:};
        -:  495:
        -:  496:class MatMulConverter : public OpConversionPattern<tosa::MatMulOp> {
        -:  497:public:
        -:  498:  using OpConversionPattern<tosa::MatMulOp>::OpConversionPattern;
        -:  499:  LogicalResult
function _ZNK12_GLOBAL__N_115MatMulConverter15matchAndRewriteEN4mlir4tosa8MatMulOpENS2_15MatMulOpAdaptorERNS1_25ConversionPatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  500:  matchAndRewrite(tosa::MatMulOp op, OpAdaptor adaptor,
        -:  501:                  ConversionPatternRewriter &rewriter) const final {
    #####:  502:    Location loc = op.getLoc();
call    0 never executed
        -:  503:
    #####:  504:    auto outputTy = op.getType().cast<ShapedType>();
call    0 never executed
    #####:  505:    auto outputElementTy = outputTy.getElementType();
call    0 never executed
        -:  506:
    #####:  507:    auto firstOperandTy = op->getOperand(0).getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
    #####:  508:    auto secondOperandTy = op->getOperand(1).getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
        -:  509:
    #####:  510:    SmallVector<Value> dynDims;
call    0 never executed
    #####:  511:    dynDims.resize(op->getResult(0).getType().cast<ShapedType>().getRank());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  512:
    #####:  513:    if (!firstOperandTy.hasRank() || firstOperandTy.isDynamicDim(0)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  514:      dynDims[0] = rewriter.create<tensor::DimOp>(loc, op->getOperand(0), 0);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  515:    }
        -:  516:
    #####:  517:    if (!firstOperandTy.hasRank() || firstOperandTy.isDynamicDim(1)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  518:      dynDims[1] = rewriter.create<tensor::DimOp>(loc, op->getOperand(0), 1);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  519:    }
        -:  520:
    #####:  521:    if (!secondOperandTy.hasRank() || secondOperandTy.isDynamicDim(2)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  522:      dynDims[2] = rewriter.create<tensor::DimOp>(loc, op->getOperand(1), 2);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  523:    }
        -:  524:
    #####:  525:    SmallVector<Value> filteredDims = condenseValues(dynDims);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  526:
    #####:  527:    auto zeroAttr = rewriter.getZeroAttr(outputElementTy);
call    0 never executed
    #####:  528:    Value zero = rewriter.create<arith::ConstantOp>(loc, zeroAttr);
call    0 never executed
call    1 never executed
    #####:  529:    auto emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  530:        loc, outputTy.getShape(), outputTy.getElementType(), filteredDims);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  531:    Value zeroTensor = rewriter
    #####:  532:                           .create<linalg::FillOp>(loc, ValueRange{zero},
    #####:  533:                                                   ValueRange{emptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  534:                           .result();
call    0 never executed
    #####:  535:    if (!op.getQuantizationInfo()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  536:      rewriter.replaceOpWithNewOp<linalg::BatchMatmulOp>(
    #####:  537:          op, TypeRange{op.getType()},
call    0 never executed
    #####:  538:          ValueRange{adaptor.getA(), adaptor.getB()}, ValueRange{zeroTensor});
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####:  539:      return success();
        -:  540:    }
        -:  541:
    #####:  542:    auto quantizationInfo = *op.getQuantizationInfo();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  543:    auto aZp = rewriter.create<arith::ConstantOp>(
    #####:  544:        loc, rewriter.getI32IntegerAttr(quantizationInfo.getAZp()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  545:    auto bZp = rewriter.create<arith::ConstantOp>(
    #####:  546:        loc, rewriter.getI32IntegerAttr(quantizationInfo.getBZp()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  547:    rewriter.replaceOpWithNewOp<linalg::QuantizedBatchMatmulOp>(
    #####:  548:        op, TypeRange{op.getType()},
call    0 never executed
    #####:  549:        ValueRange{adaptor.getA(), adaptor.getB(), aZp, bZp}, zeroTensor);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -:  550:
    #####:  551:    return success();
branch  0 never executed
branch  1 never executed
        -:  552:  }
        -:  553:};
        -:  554:
        -:  555:class FullyConnectedConverter
        -:  556:    : public OpConversionPattern<tosa::FullyConnectedOp> {
        -:  557:public:
        -:  558:  using OpConversionPattern<tosa::FullyConnectedOp>::OpConversionPattern;
        -:  559:  LogicalResult
function _ZNK12_GLOBAL__N_123FullyConnectedConverter15matchAndRewriteEN4mlir4tosa16FullyConnectedOpENS2_23FullyConnectedOpAdaptorERNS1_25ConversionPatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  560:  matchAndRewrite(tosa::FullyConnectedOp op, OpAdaptor adaptor,
        -:  561:                  ConversionPatternRewriter &rewriter) const final {
    #####:  562:    Location loc = op.getLoc();
call    0 never executed
    #####:  563:    auto outputTy = op.getType().cast<ShapedType>();
call    0 never executed
    #####:  564:    auto input = op.getInput();
call    0 never executed
    #####:  565:    auto inputTy = input.getType().cast<ShapedType>();
call    0 never executed
        -:  566:
    #####:  567:    auto bias = op.getBias();
call    0 never executed
        -:  568:
    #####:  569:    auto weight = op.getWeight();
call    0 never executed
    #####:  570:    auto weightTy = weight.getType().cast<ShapedType>();
call    0 never executed
    #####:  571:    auto weightShape = weightTy.getShape();
call    0 never executed
        -:  572:
    #####:  573:    auto outputETy = outputTy.getElementType();
call    0 never executed
        -:  574:
    #####:  575:    SmallVector<Value> dynDims;
call    0 never executed
    #####:  576:    dynDims.resize(op->getResult(0).getType().cast<ShapedType>().getRank());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  577:
    #####:  578:    if (!inputTy.hasRank() || inputTy.isDynamicDim(0)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  579:      dynDims[0] = rewriter.create<tensor::DimOp>(loc, input, 0);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  580:    }
        -:  581:
    #####:  582:    if (!weightTy.hasRank() || weightTy.isDynamicDim(0)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  583:      dynDims[1] = rewriter.create<tensor::DimOp>(loc, weight, 0);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  584:    }
        -:  585:
    #####:  586:    SmallVector<Value> filteredDims = condenseValues(dynDims);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  587:
        -:  588:    // Creating maps for the output of MatMul and the bias
    #####:  589:    SmallVector<AffineMap, 4> indexingMaps;
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  590:
        -:  591:    // Broadcast the bias.
    #####:  592:    indexingMaps.push_back(AffineMap::get(/*dimCount=*/2, /*symbolCount=*/0,
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  593:                                          {rewriter.getAffineDimExpr(1)},
call    0 never executed
        -:  594:                                          rewriter.getContext()));
        -:  595:
    #####:  596:    indexingMaps.push_back(rewriter.getMultiDimIdentityMap(outputTy.getRank()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  597:    indexingMaps.push_back(rewriter.getMultiDimIdentityMap(outputTy.getRank()));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  598:
    #####:  599:    auto emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  600:        loc, outputTy.getShape(), outputTy.getElementType(), filteredDims);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  601:
        -:  602:    // When quantized, the input elemeny type is not the same as the output
    #####:  603:    Attribute resultZeroAttr = rewriter.getZeroAttr(outputETy);
call    0 never executed
    #####:  604:    Value zero = rewriter.create<arith::ConstantOp>(loc, resultZeroAttr);
call    0 never executed
call    1 never executed
    #####:  605:    Value zeroTensor = rewriter
    #####:  606:                           .create<linalg::FillOp>(loc, ValueRange{zero},
    #####:  607:                                                   ValueRange{emptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  608:                           .result();
call    0 never executed
        -:  609:
    #####:  610:    SmallVector<int64_t> permutation{1, 0};
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  611:    auto permutationAttr = DenseIntElementsAttr::get(
    #####:  612:        RankedTensorType::get({2}, rewriter.getI64Type()), permutation);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  613:    Value permutationValue =
    #####:  614:        rewriter.create<arith::ConstantOp>(loc, permutationAttr);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  615:
    #####:  616:    SmallVector<int64_t> newWeightShape{weightShape[1], weightShape[0]};
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  617:    Type newWeightTy =
    #####:  618:        RankedTensorType::get(newWeightShape, weightTy.getElementType());
call    0 never executed
call    1 never executed
        -:  619:
    #####:  620:    Value transposedWeight = rewriter.create<tosa::TransposeOp>(
    #####:  621:        loc, newWeightTy, weight, permutationValue);
call    0 never executed
call    1 never executed
        -:  622:
    #####:  623:    auto biasEmptyTensor =
        -:  624:        rewriter
    #####:  625:            .create<tensor::EmptyOp>(loc, outputTy.getShape(), outputETy,
call    0 never executed
    #####:  626:                                     filteredDims)
call    0 never executed
    #####:  627:            ->getResults();
branch  0 never executed
branch  1 never executed
        -:  628:
    #####:  629:    if (!op.getQuantizationInfo()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  630:      Value matmul = rewriter
    #####:  631:                         .create<linalg::MatmulOp>(
    #####:  632:                             loc, TypeRange{op.getType()},
call    0 never executed
    #####:  633:                             ValueRange{input, transposedWeight}, zeroTensor)
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  634:                         ->getResult(0);
call    0 never executed
        -:  635:
    #####:  636:      Value result =
        -:  637:          rewriter
    #####:  638:              .create<linalg::GenericOp>(
    #####:  639:                  loc, outputTy, ValueRange({bias, matmul}), biasEmptyTensor,
call    0 never executed
    #####:  640:                  indexingMaps, getNParallelLoopsAttrs(outputTy.getRank()),
call    0 never executed
function _ZZNK12_GLOBAL__N_123FullyConnectedConverter15matchAndRewriteEN4mlir4tosa16FullyConnectedOpENS2_23FullyConnectedOpAdaptorERNS1_25ConversionPatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES8_S9_SA_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  641:                  [&](OpBuilder &nestedBuilder, Location nestedLoc,
        -:  642:                      ValueRange args) {
    #####:  643:                    Value added = nestedBuilder.create<arith::AddFOp>(
    #####:  644:                        loc, args[0], args[1]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  645:                    nestedBuilder.create<linalg::YieldOp>(nestedLoc, added);
call    0 never executed
    #####:  646:                  })
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  647:              .getResult(0);
branch  0 never executed
branch  1 never executed
    #####:  648:      rewriter.replaceOp(op, result);
call    0 never executed
call    1 never executed
    #####:  649:      return success();
        -:  650:    }
        -:  651:
    #####:  652:    auto quantizationInfo = *op.getQuantizationInfo();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  653:    auto inputZp = rewriter.create<arith::ConstantOp>(
    #####:  654:        loc, rewriter.getI32IntegerAttr(quantizationInfo.getInputZp()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  655:    auto outputZp = rewriter.create<arith::ConstantOp>(
    #####:  656:        loc, rewriter.getI32IntegerAttr(quantizationInfo.getWeightZp()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  657:    Value matmul =
        -:  658:        rewriter
    #####:  659:            .create<linalg::QuantizedMatmulOp>(
    #####:  660:                loc, TypeRange{op.getType()},
call    0 never executed
    #####:  661:                ValueRange{input, transposedWeight, inputZp, outputZp},
call    0 never executed
call    1 never executed
    #####:  662:                zeroTensor)
call    0 never executed
    #####:  663:            ->getResult(0);
call    0 never executed
    #####:  664:    Value result =
        -:  665:        rewriter
    #####:  666:            .create<linalg::GenericOp>(
    #####:  667:                loc, outputTy, ValueRange({bias, matmul}), biasEmptyTensor,
call    0 never executed
    #####:  668:                indexingMaps, getNParallelLoopsAttrs(outputTy.getRank()),
call    0 never executed
function _ZZNK12_GLOBAL__N_123FullyConnectedConverter15matchAndRewriteEN4mlir4tosa16FullyConnectedOpENS2_23FullyConnectedOpAdaptorERNS1_25ConversionPatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE0_clES8_S9_SA_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  669:                [&](OpBuilder &nestedBuilder, Location nestedLoc,
        -:  670:                    ValueRange args) {
    #####:  671:                  Value added = nestedBuilder.create<arith::AddIOp>(
    #####:  672:                      loc, args[0], args[1]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  673:                  nestedBuilder.create<linalg::YieldOp>(nestedLoc, added);
call    0 never executed
    #####:  674:                })
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  675:            .getResult(0);
branch  0 never executed
branch  1 never executed
    #####:  676:    rewriter.replaceOp(op, result);
call    0 never executed
call    1 never executed
    #####:  677:    return success();
branch  0 never executed
branch  1 never executed
        -:  678:  }
        -:  679:};
        -:  680:
        -:  681:class MaxPool2dConverter : public OpRewritePattern<tosa::MaxPool2dOp> {
        -:  682:public:
        -:  683:  using OpRewritePattern<tosa::MaxPool2dOp>::OpRewritePattern;
        -:  684:
function _ZNK12_GLOBAL__N_118MaxPool2dConverter15matchAndRewriteEN4mlir4tosa11MaxPool2dOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  685:  LogicalResult matchAndRewrite(tosa::MaxPool2dOp op,
        -:  686:                                PatternRewriter &rewriter) const final {
    #####:  687:    Location loc = op.getLoc();
call    0 never executed
    #####:  688:    Value input = op.getInput();
call    0 never executed
    #####:  689:    ShapedType inputTy = input.getType().cast<ShapedType>();
call    0 never executed
        -:  690:
    #####:  691:    ShapedType resultTy = op.getType().template cast<ShapedType>();
call    0 never executed
    #####:  692:    Type resultETy = inputTy.getElementType();
call    0 never executed
        -:  693:
    #####:  694:    auto dynamicDimsOr =
    #####:  695:        checkHasDynamicBatchDims(rewriter, op, {input, op.getOutput()});
call    0 never executed
call    1 never executed
    #####:  696:    if (!dynamicDimsOr.has_value())
branch  0 never executed
branch  1 never executed
    #####:  697:      return failure();
    #####:  698:    SmallVector<Value> dynamicDims = dynamicDimsOr.value();
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  699:
        -:  700:    // Determine what the initial value needs to be for the max pool op.
    #####:  701:    Attribute initialAttr;
    #####:  702:    if (resultETy.isF32())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  703:      initialAttr = rewriter.getFloatAttr(
call    0 never executed
        -:  704:          resultETy,
    #####:  705:          APFloat::getLargest(resultETy.cast<FloatType>().getFloatSemantics(),
call    0 never executed
call    1 never executed
    #####:  706:                              true));
call    0 never executed
call    1 never executed
        -:  707:
    #####:  708:    if (resultETy.isa<IntegerType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  709:      initialAttr = rewriter.getIntegerAttr(
branch  0 never executed
branch  1 never executed
        -:  710:          resultETy,
    #####:  711:          APInt::getSignedMinValue(resultETy.getIntOrFloatBitWidth()));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  712:
    #####:  713:    if (!initialAttr)
branch  0 never executed
branch  1 never executed
    #####:  714:      return rewriter.notifyMatchFailure(
    #####:  715:          op, "Unsupported initial value for tosa.maxpool_2d op");
call    0 never executed
        -:  716:
        -:  717:    // Apply padding as necessary.
    #####:  718:    llvm::SmallVector<int64_t> pad;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  719:    pad.resize(2, 0);
call    0 never executed
    #####:  720:    getValuesFromIntArrayAttribute(op.getPad(), pad);
call    0 never executed
call    1 never executed
    #####:  721:    pad.resize(pad.size() + 2, 0);
call    0 never executed
    #####:  722:    Value paddedInput = applyPad(loc, input, pad, initialAttr, rewriter);
call    0 never executed
        -:  723:
    #####:  724:    Value initialValue = rewriter.create<arith::ConstantOp>(loc, initialAttr);
call    0 never executed
call    1 never executed
        -:  725:
    #####:  726:    SmallVector<int64_t> kernel, stride;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  727:    getValuesFromIntArrayAttribute(op.getKernel(), kernel);
call    0 never executed
call    1 never executed
    #####:  728:    getValuesFromIntArrayAttribute(op.getStride(), stride);
call    0 never executed
call    1 never executed
        -:  729:
    #####:  730:    Attribute strideAttr = rewriter.getI64VectorAttr(stride);
call    0 never executed
    #####:  731:    Attribute dilationAttr = rewriter.getI64VectorAttr({1, 1});
call    0 never executed
        -:  732:
        -:  733:    // Create the linalg op that performs pooling.
    #####:  734:    Value emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  735:        loc, resultTy.getShape(), resultTy.getElementType(), dynamicDims);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  736:
    #####:  737:    Value filledEmptyTensor =
        -:  738:        rewriter
    #####:  739:            .create<linalg::FillOp>(loc, ValueRange{initialValue},
    #####:  740:                                    ValueRange{emptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  741:            .result();
call    0 never executed
        -:  742:
    #####:  743:    Value fakeWindowDims =
    #####:  744:        rewriter.create<tensor::EmptyOp>(loc, kernel, resultETy);
call    0 never executed
call    1 never executed
        -:  745:
    #####:  746:    rewriter.replaceOpWithNewOp<linalg::PoolingNhwcMaxOp>(
    #####:  747:        op, ArrayRef<Type>{resultTy}, ValueRange{paddedInput, fakeWindowDims},
call    0 never executed
    #####:  748:        filledEmptyTensor, strideAttr, dilationAttr);
call    0 never executed
call    1 never executed
    #####:  749:    return success();
branch  0 never executed
branch  1 never executed
        -:  750:  }
        -:  751:};
        -:  752:
        -:  753:class AvgPool2dConverter : public OpRewritePattern<tosa::AvgPool2dOp> {
        -:  754:public:
        -:  755:  using OpRewritePattern<tosa::AvgPool2dOp>::OpRewritePattern;
        -:  756:
function _ZNK12_GLOBAL__N_118AvgPool2dConverter15matchAndRewriteEN4mlir4tosa11AvgPool2dOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  757:  LogicalResult matchAndRewrite(tosa::AvgPool2dOp op,
        -:  758:                                PatternRewriter &rewriter) const final {
    #####:  759:    Location loc = op.getLoc();
call    0 never executed
    #####:  760:    Value input = op.getInput();
call    0 never executed
    #####:  761:    ShapedType inputTy = input.getType().cast<ShapedType>();
call    0 never executed
    #####:  762:    Type inElementTy = inputTy.getElementType();
call    0 never executed
        -:  763:
    #####:  764:    ShapedType resultTy = op.getType().template cast<ShapedType>();
call    0 never executed
    #####:  765:    Type resultETy = op.getType().cast<ShapedType>().getElementType();
call    0 never executed
call    1 never executed
        -:  766:
    #####:  767:    Type accETy =
call    0 never executed
    #####:  768:        inElementTy.isa<IntegerType>() ? rewriter.getI32Type() : inElementTy;
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  769:    ShapedType accTy = resultTy.clone(accETy);
call    0 never executed
        -:  770:
    #####:  771:    auto dynamicDimsOr =
    #####:  772:        checkHasDynamicBatchDims(rewriter, op, {input, op.getOutput()});
call    0 never executed
call    1 never executed
    #####:  773:    if (!dynamicDimsOr.has_value())
branch  0 never executed
branch  1 never executed
    #####:  774:      return failure();
    #####:  775:    SmallVector<Value> dynamicDims = dynamicDimsOr.value();
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  776:
        -:  777:    // Apply padding as necessary.
    #####:  778:    llvm::SmallVector<int64_t> pad;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  779:    pad.resize(2, 0);
call    0 never executed
    #####:  780:    getValuesFromIntArrayAttribute(op.getPad(), pad);
call    0 never executed
call    1 never executed
    #####:  781:    pad.resize(pad.size() + 2, 0);
call    0 never executed
    #####:  782:    Attribute padAttr = rewriter.getZeroAttr(inElementTy);
call    0 never executed
    #####:  783:    Value paddedInput = applyPad(loc, input, pad, padAttr, rewriter);
call    0 never executed
        -:  784:
    #####:  785:    Attribute initialAttr = rewriter.getZeroAttr(accETy);
call    0 never executed
    #####:  786:    Value initialValue = rewriter.create<arith::ConstantOp>(loc, initialAttr);
call    0 never executed
call    1 never executed
        -:  787:
    #####:  788:    SmallVector<int64_t> kernel, stride;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  789:    getValuesFromIntArrayAttribute(op.getKernel(), kernel);
call    0 never executed
call    1 never executed
    #####:  790:    getValuesFromIntArrayAttribute(op.getStride(), stride);
call    0 never executed
call    1 never executed
        -:  791:
    #####:  792:    Attribute strideAttr = rewriter.getI64VectorAttr(stride);
call    0 never executed
    #####:  793:    Attribute dilationAttr = rewriter.getI64VectorAttr({1, 1});
call    0 never executed
        -:  794:
        -:  795:    // Create the linalg op that performs pooling.
    #####:  796:    Value poolEmptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  797:        loc, accTy.getShape(), accETy, dynamicDims);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  798:
    #####:  799:    Value filledEmptyTensor =
        -:  800:        rewriter
    #####:  801:            .create<linalg::FillOp>(loc, ValueRange{initialValue},
    #####:  802:                                    ValueRange{poolEmptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  803:            .result();
call    0 never executed
        -:  804:
    #####:  805:    Value fakeWindowDims =
    #####:  806:        rewriter.create<tensor::EmptyOp>(loc, kernel, accETy);
call    0 never executed
call    1 never executed
        -:  807:
        -:  808:    // Sum across the pooled region.
    #####:  809:    Value poolingOp = rewriter
    #####:  810:                          .create<linalg::PoolingNhwcSumOp>(
    #####:  811:                              loc, ArrayRef<Type>{accTy},
    #####:  812:                              ValueRange{paddedInput, fakeWindowDims},
call    0 never executed
call    1 never executed
    #####:  813:                              filledEmptyTensor, strideAttr, dilationAttr)
call    0 never executed
call    1 never executed
    #####:  814:                          .getResult(0);
        -:  815:
        -:  816:    // Normalize the summed value by the number of elements grouped in each
        -:  817:    // pool.
    #####:  818:    auto poolingOpTy = poolingOp.getType().cast<ShapedType>();
call    0 never executed
    #####:  819:    auto affineMap = rewriter.getMultiDimIdentityMap(resultTy.getRank());
call    0 never executed
call    1 never executed
        -:  820:
    #####:  821:    Value genericEmptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  822:        loc, resultTy.getShape(), resultETy, dynamicDims);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  823:
    #####:  824:    auto genericOp = rewriter.create<linalg::GenericOp>(
    #####:  825:        loc, ArrayRef<Type>({resultTy}), ValueRange{poolingOp},
call    0 never executed
    #####:  826:        ValueRange{genericEmptyTensor},
call    0 never executed
call    1 never executed
    #####:  827:        ArrayRef<AffineMap>({affineMap, affineMap}),
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  828:        getNParallelLoopsAttrs(resultTy.getRank()),
call    0 never executed
function _ZZNK12_GLOBAL__N_118AvgPool2dConverter15matchAndRewriteEN4mlir4tosa11AvgPool2dOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES7_S8_S9_ called 0 returned 0% blocks executed 0%
    #####:  829:        [&](OpBuilder &b, Location loc, ValueRange args) {
    #####:  830:          auto zero = rewriter.create<arith::ConstantIndexOp>(loc, 0);
call    0 never executed
    #####:  831:          auto one = rewriter.create<arith::ConstantIndexOp>(loc, 1);
call    0 never executed
    #####:  832:          auto iH = rewriter.create<arith::ConstantIndexOp>(
    #####:  833:              loc, poolingOpTy.getDimSize(1) - 1);
call    0 never executed
call    1 never executed
    #####:  834:          auto iW = rewriter.create<arith::ConstantIndexOp>(
    #####:  835:              loc, poolingOpTy.getDimSize(2) - 1);
call    0 never executed
call    1 never executed
        -:  836:
        -:  837:          // Compute the indices from either end.
    #####:  838:          auto y0 = rewriter.create<linalg::IndexOp>(loc, 1);
call    0 never executed
    #####:  839:          auto x0 = rewriter.create<linalg::IndexOp>(loc, 2);
call    0 never executed
    #####:  840:          auto y1 = rewriter.create<arith::SubIOp>(loc, iH, y0);
call    0 never executed
    #####:  841:          auto x1 = rewriter.create<arith::SubIOp>(loc, iW, x0);
call    0 never executed
        -:  842:
        -:  843:          // Determines what the portion of valid input is covered by the
        -:  844:          // kernel.
function _ZZZNK12_GLOBAL__N_118AvgPool2dConverter15matchAndRewriteEN4mlir4tosa11AvgPool2dOpERNS1_15PatternRewriterEENKUlRNS1_9OpBuilderENS1_8LocationENS1_10ValueRangeEE_clES7_S8_S9_ENKUlNS1_5ValueESB_lE_clESB_SB_l called 0 returned 0% blocks executed 0%
    #####:  845:          auto padFn = [&](Value v, Value x, int64_t pad) -> Value {
    #####:  846:            if (pad == 0)
branch  0 never executed
branch  1 never executed
    #####:  847:              return v;
        -:  848:
    #####:  849:            auto padVal = rewriter.create<arith::ConstantIndexOp>(loc, pad);
call    0 never executed
    #####:  850:            Value dx = rewriter.create<arith::SubIOp>(loc, x, padVal);
call    0 never executed
call    1 never executed
        -:  851:
    #####:  852:            Value cmp = rewriter.create<arith::CmpIOp>(
    #####:  853:                loc, arith::CmpIPredicate::slt, dx, zero);
call    0 never executed
call    1 never executed
    #####:  854:            Value offset = rewriter.create<arith::SelectOp>(loc, cmp, dx, zero);
call    0 never executed
call    1 never executed
    #####:  855:            return rewriter.create<arith::AddIOp>(loc, v, offset)->getResult(0);
call    0 never executed
    #####:  856:          };
        -:  857:
        -:  858:          // Compute the vertical component of coverage.
    #####:  859:          auto kH0 = rewriter.create<arith::ConstantIndexOp>(loc, kernel[0]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  860:          auto kH1 = padFn(kH0, y0, pad[2]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  861:          auto kH2 = padFn(kH1, y1, pad[3]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  862:          auto kHCmp = rewriter.create<arith::CmpIOp>(
    #####:  863:              loc, arith::CmpIPredicate::slt, kH2, one);
call    0 never executed
    #####:  864:          auto kH3 = rewriter.create<arith::SelectOp>(loc, kHCmp, one, kH2);
call    0 never executed
        -:  865:
        -:  866:          // compute the horizontal component of coverage.
    #####:  867:          auto kW0 = rewriter.create<arith::ConstantIndexOp>(loc, kernel[1]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  868:          auto kW1 = padFn(kW0, x0, pad[4]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  869:          auto kW2 = padFn(kW1, x1, pad[5]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  870:          auto kWCmp = rewriter.create<arith::CmpIOp>(
    #####:  871:              loc, arith::CmpIPredicate::slt, kW2, one);
call    0 never executed
    #####:  872:          auto kW3 = rewriter.create<arith::SelectOp>(loc, kWCmp, one, kW2);
call    0 never executed
        -:  873:
        -:  874:          // Compute the total number of elements and normalize.
    #####:  875:          Value count = rewriter.create<arith::MulIOp>(loc, kH3, kW3);
call    0 never executed
call    1 never executed
    #####:  876:          auto countI = rewriter.create<arith::IndexCastOp>(
    #####:  877:              loc, rewriter.getI32Type(), count);
call    0 never executed
call    1 never executed
        -:  878:
        -:  879:          // Divide by the number of summed values. For floats this is just
        -:  880:          // a div however for quantized values input normalization had
        -:  881:          // to be applied.
    #####:  882:          Value poolVal = args[0];
call    0 never executed
    #####:  883:          if (accETy.isa<FloatType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  884:            auto countF = rewriter.create<arith::SIToFPOp>(loc, accETy, countI);
call    0 never executed
    #####:  885:            poolVal = rewriter.create<arith::DivFOp>(loc, poolVal, countF)
call    0 never executed
    #####:  886:                          ->getResult(0);
        -:  887:          } else {
        -:  888:
        -:  889:            // If we have quantization information we need to apply an offset
        -:  890:            // for the input zp value.
    #####:  891:            if (op.getQuantizationInfo()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  892:              auto quantizationInfo = *op.getQuantizationInfo();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  893:              auto inputZp = rewriter.create<arith::ConstantOp>(
    #####:  894:                  loc, b.getIntegerAttr(accETy, quantizationInfo.getInputZp()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  895:              Value offset =
    #####:  896:                  rewriter.create<arith::MulIOp>(loc, accETy, countI, inputZp);
call    0 never executed
call    1 never executed
    #####:  897:              poolVal =
    #####:  898:                  rewriter.create<arith::SubIOp>(loc, accETy, poolVal, offset);
call    0 never executed
        -:  899:            }
        -:  900:
        -:  901:            // Compute the multiplier and shift values for the quantization
        -:  902:            // normalization. Preferably we would want to compute more bits
        -:  903:            // however 32-bits should be enough for compute. Honestly we
        -:  904:            // should probably straight divide.
    #####:  905:            int64_t numerator = ((1 << 30) + 1);
    #####:  906:            int64_t shift = 30;
        -:  907:
    #####:  908:            Value numeratorVal = rewriter.create<arith::ConstantOp>(
    #####:  909:                loc, rewriter.getI32IntegerAttr(numerator));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  910:            Value multiplierVal =
        -:  911:                rewriter
    #####:  912:                    .create<arith::DivUIOp>(loc, rewriter.getI32Type(),
    #####:  913:                                            numeratorVal, countI)
call    0 never executed
call    1 never executed
    #####:  914:                    .getResult();
call    0 never executed
    #####:  915:            Value shiftVal = rewriter.create<arith::ConstantOp>(
    #####:  916:                loc, rewriter.getI8IntegerAttr(shift));
call    0 never executed
call    1 never executed
call    2 never executed
        -:  917:
    #####:  918:            auto scaled =
        -:  919:                rewriter
    #####:  920:                    .create<tosa::ApplyScaleOp>(
    #####:  921:                        loc, rewriter.getI32Type(), poolVal, multiplierVal,
call    0 never executed
    #####:  922:                        shiftVal, rewriter.getBoolAttr(false))
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  923:                    .getResult();
        -:  924:
        -:  925:            // If we have quantization information we need to apply output
        -:  926:            // zeropoint.
    #####:  927:            if (op.getQuantizationInfo()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  928:              auto quantizationInfo = *op.getQuantizationInfo();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  929:              auto outputZp = rewriter.create<arith::ConstantOp>(
    #####:  930:                  loc, b.getIntegerAttr(scaled.getType(),
    #####:  931:                                        quantizationInfo.getOutputZp()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  932:              scaled = rewriter.create<arith::AddIOp>(loc, scaled, outputZp)
call    0 never executed
    #####:  933:                           .getResult();
call    0 never executed
        -:  934:            }
        -:  935:
        -:  936:            // Apply Clip.
    #####:  937:            int64_t outBitwidth = resultETy.getIntOrFloatBitWidth();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  938:
    #####:  939:            auto min = rewriter.create<arith::ConstantIntOp>(
    #####:  940:                loc, APInt::getSignedMinValue(outBitwidth).getSExtValue(),
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  941:                accETy);
call    0 never executed
call    1 never executed
    #####:  942:            auto max = rewriter.create<arith::ConstantIntOp>(
    #####:  943:                loc, APInt::getSignedMaxValue(outBitwidth).getSExtValue(),
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  944:                accETy);
call    0 never executed
call    1 never executed
    #####:  945:            auto clamp = clampIntHelper(loc, scaled, min, max, rewriter);
call    0 never executed
        -:  946:
    #####:  947:            poolVal = clamp;
        -:  948:            // Convert type.
    #####:  949:            if (resultETy != clamp.getType()) {
branch  0 never executed
branch  1 never executed
    #####:  950:              poolVal =
    #####:  951:                  rewriter.create<arith::TruncIOp>(loc, resultETy, poolVal);
call    0 never executed
        -:  952:            }
        -:  953:          }
        -:  954:
    #####:  955:          rewriter.create<linalg::YieldOp>(loc, poolVal);
call    0 never executed
    #####:  956:        });
call    0 never executed
call    1 never executed
        -:  957:
    #####:  958:    rewriter.replaceOp(op, genericOp.getResult(0));
call    0 never executed
call    1 never executed
    #####:  959:    return success();
branch  0 never executed
branch  1 never executed
        -:  960:  }
        -:  961:};
        -:  962:
        -:  963:} // namespace
        -:  964:
function _ZN4mlir4tosa43populateTosaToLinalgNamedConversionPatternsEPNS_17RewritePatternSetE called 0 returned 0% blocks executed 0%
    #####:  965:void mlir::tosa::populateTosaToLinalgNamedConversionPatterns(
        -:  966:    RewritePatternSet *patterns) {
    #####:  967:  patterns->add<
        -:  968:      // clang-format off
        -:  969:      ConvConverter,
        -:  970:      DepthwiseConvConverter,
        -:  971:      MatMulConverter,
        -:  972:      MaxPool2dConverter,
        -:  973:      AvgPool2dConverter,
    #####:  974:      FullyConnectedConverter>(patterns->getContext());
call    0 never executed
        -:  975:  // clang-format on
    #####:  976:}
