        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/GPU/Transforms/ParallelLoopMapper.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/GPU/CMakeFiles/obj.MLIRGPUTransforms.dir/Transforms/ParallelLoopMapper.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/GPU/CMakeFiles/obj.MLIRGPUTransforms.dir/Transforms/ParallelLoopMapper.cpp.gcda
        -:    0:Runs:116158
        -:    1://===- ParallelLoopMapper.cpp - Utilities for mapping parallel loops to GPU =//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements utilities to generate mappings for parallel loops to
        -:   10:// GPU devices.
        -:   11://
        -:   12://===----------------------------------------------------------------------===//
        -:   13:
        -:   14:#include "mlir/Dialect/GPU/Transforms/Passes.h"
        -:   15:
        -:   16:#include "mlir/Dialect/Func/IR/FuncOps.h"
        -:   17:#include "mlir/Dialect/GPU/IR/GPUDialect.h"
        -:   18:#include "mlir/Dialect/GPU/Transforms/ParallelLoopMapper.h"
        -:   19:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   20:#include "mlir/IR/AffineMap.h"
        -:   21:
        -:   22:namespace mlir {
        -:   23:#define GEN_PASS_DEF_GPUMAPPARALLELLOOPSPASS
        -:   24:#include "mlir/Dialect/GPU/Transforms/Passes.h.inc"
        -:   25:} // namespace mlir
        -:   26:
        -:   27:namespace mlir {
        -:   28:
        -:   29:using scf::ParallelOp;
        -:   30:
function _ZN4mlir3gpu18getMappingAttrNameEv called 1704 returned 100% blocks executed 100%
     1731:   31:StringRef gpu::getMappingAttrName() { return "mapping"; }
call    0 returned 100%
call    1 returned 100%
        -:   32:
        -:   33:LogicalResult
function _ZN4mlir3gpu14setMappingAttrENS_3scf10ParallelOpEN4llvm8ArrayRefINS0_26ParallelLoopDimMappingAttrEEE called 862 returned 100% blocks executed 75%
      862:   34:gpu::setMappingAttr(ParallelOp ploopOp,
        -:   35:                    ArrayRef<ParallelLoopDimMappingAttr> mapping) {
        -:   36:  // Verify that each processor is mapped to only once.
      862:   37:  llvm::DenseSet<gpu::Processor> specifiedMappings;
call    0 returned 100%
     2264:   38:  for (auto dimAttr : mapping) {
branch  0 taken 62% (fallthrough)
branch  1 taken 38%
     1402:   39:    gpu::Processor processor = dimAttr.getProcessor();
call    0 returned 100%
     1402:   40:    if (processor != gpu::Processor::Sequential &&
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    1402*:   41:        specifiedMappings.count(processor))
    #####:   42:      return ploopOp.emitError(
call    0 never executed
call    1 never executed
call    2 never executed
    #####:   43:          "invalid mapping multiple loops to same processor");
call    0 never executed
        -:   44:  }
      862:   45:  ArrayRef<Attribute> mappingAsAttrs(mapping.data(), mapping.size());
call    0 returned 100%
     1724:   46:  ploopOp->setAttr(getMappingAttrName(),
call    0 returned 100%
call    1 returned 100%
      862:   47:                   ArrayAttr::get(ploopOp.getContext(), mappingAsAttrs));
call    0 returned 100%
      862:   48:  return success();
call    0 returned 100%
        -:   49:}
        -:   50:
        -:   51:namespace gpu {
        -:   52:namespace {
        -:   53:enum MappingLevel { MapGrid = 0, MapBlock = 1, Sequential = 2 };
        -:   54:} // namespace
        -:   55:
        -:   56:static constexpr int kNumHardwareIds = 3;
        -:   57:
        -:   58:/// Bounded increment on MappingLevel. Increments to the next
        -:   59:/// level unless Sequential was already reached.
      862:   60:static MappingLevel &operator++(MappingLevel &mappingLevel) {
      862:   61:  if (mappingLevel < Sequential) {
      862:   62:    mappingLevel = static_cast<MappingLevel>(mappingLevel + 1);
        -:   63:  }
      862:   64:  return mappingLevel;
        -:   65:}
        -:   66:
        -:   67:/// Computed the hardware id to use for a given mapping level. Will
        -:   68:/// assign x,y and z hardware ids for the first 3 dimensions and use
        -:   69:/// sequential after.
        -:   70:/// TODO: Make this use x for the inner-most loop that is
        -:   71:/// distributed to map to x, the next innermost to y and the next innermost to
        -:   72:/// z.
     1402:   73:static Processor getHardwareIdForMapping(MappingLevel level, int dimension) {
        -:   74:
     1402:   75:  if (dimension >= kNumHardwareIds || level == Sequential)
        -:   76:    return Processor::Sequential;
     1402:   77:  switch (level) {
branch  0 taken 100%
branch  1 taken 1%
branch  2 taken 0%
     1401:   78:  case MapGrid:
     1401:   79:    switch (dimension) {
        -:   80:    case 0:
        -:   81:      return Processor::BlockX;
        -:   82:    case 1:
        -:   83:      return Processor::BlockY;
        -:   84:    case 2:
        -:   85:      return Processor::BlockZ;
        -:   86:    default:
        -:   87:      return Processor::Sequential;
        -:   88:    }
        1:   89:    break;
        1:   90:  case MapBlock:
        1:   91:    switch (dimension) {
        -:   92:    case 0:
        -:   93:      return Processor::ThreadX;
        -:   94:    case 1:
        -:   95:      return Processor::ThreadY;
        -:   96:    case 2:
        -:   97:      return Processor::ThreadZ;
        -:   98:    default:
        -:   99:      return Processor::Sequential;
        -:  100:    }
        -:  101:  default:;
        -:  102:  }
        -:  103:  return Processor::Sequential;
        -:  104:}
        -:  105:
        -:  106:/// Add mapping information to the given parallel loop. Do not add
        -:  107:/// mapping information if the loop already has it. Also, don't
        -:  108:/// start a mapping at a nested loop.
function _ZN4mlir3gpuL13mapParallelOpENS_3scf10ParallelOpENS0_12_GLOBAL__N_112MappingLevelE called 869 returned 100% blocks executed 92%
      869:  109:static void mapParallelOp(ParallelOp parallelOp,
        -:  110:                          MappingLevel mappingLevel = MapGrid) {
        -:  111:  // Do not try to add a mapping to already mapped loops or nested loops.
     1732:  112:  if (parallelOp->getAttr(getMappingAttrName()) ||
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 1%
     1731:  113:      ((mappingLevel == MapGrid) && parallelOp->getParentOfType<ParallelOp>()))
branch  0 taken 99% (fallthrough)
branch  1 taken 1%
call    2 returned 100%
branch  3 taken 1% (fallthrough)
branch  4 taken 100%
        7:  114:    return;
        -:  115:
      862:  116:  MLIRContext *ctx = parallelOp.getContext();
call    0 returned 100%
      862:  117:  Builder b(ctx);
call    0 returned 100%
     1724:  118:  SmallVector<ParallelLoopDimMappingAttr, 4> attrs;
call    0 returned 100%
      862:  119:  attrs.reserve(parallelOp.getNumLoops());
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
     2264:  120:  for (int i = 0, e = parallelOp.getNumLoops(); i < e; ++i) {
call    0 returned 100%
branch  1 taken 62% (fallthrough)
branch  2 taken 38%
     1402:  121:    attrs.push_back(b.getAttr<ParallelLoopDimMappingAttr>(
call    0 returned 100%
     2804:  122:        getHardwareIdForMapping(mappingLevel, i), b.getDimIdentityMap(),
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
     2804:  123:        b.getDimIdentityMap()));
call    0 returned 100%
        -:  124:  }
      862:  125:  (void)setMappingAttr(parallelOp, attrs);
call    0 returned 100%
      862:  126:  ++mappingLevel;
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        -:  127:  // Parallel loop operations are immediately nested, so do not use
        -:  128:  // walk but just iterate over the operations.
    15905:  129:  for (Operation &op : *parallelOp.getBody()) {
call    0 returned 100%
branch  1 taken 95% (fallthrough)
branch  2 taken 5%
call    3 returned 100%
    30086:  130:    if (ParallelOp nested = dyn_cast<ParallelOp>(op))
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 100%
        1:  131:      mapParallelOp(nested, mappingLevel);
call    0 returned 100%
        -:  132:  }
        -:  133:}
        -:  134:
        -:  135:namespace {
   148259:  136:struct GpuMapParallelLoopsPass
call    0 returned 100%
call    1 returned 100%
        -:  137:    : public impl::GpuMapParallelLoopsPassBase<GpuMapParallelLoopsPass> {
function _ZN4mlir3gpu12_GLOBAL__N_123GpuMapParallelLoopsPass14runOnOperationEv called 574 returned 101% blocks executed 100%
      574:  138:  void runOnOperation() override {
     1725:  139:    for (Region &region : getOperation()->getRegions()) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
branch  3 taken 50% (fallthrough)
branch  4 taken 50%
     1442:  140:      region.walk([](ParallelOp parallelOp) { mapParallelOp(parallelOp); });
call    0 returned 100%
call    1 returned 101%
        -:  141:    }
      577:  142:  }
        -:  143:};
        -:  144:
        -:  145:} // namespace
        -:  146:} // namespace gpu
        -:  147:} // namespace mlir
        -:  148:
        -:  149:std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
function _ZN4mlir29createGpuMapParallelLoopsPassEv called 116723 returned 100% blocks executed 100%
   116723:  150:mlir::createGpuMapParallelLoopsPass() {
   116723:  151:  return std::make_unique<gpu::GpuMapParallelLoopsPass>();
call    0 returned 100%
        -:  152:}
