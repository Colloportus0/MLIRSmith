        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Affine/Analysis/LoopAnalysis.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Affine/Analysis/CMakeFiles/obj.MLIRAffineAnalysis.dir/LoopAnalysis.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Affine/Analysis/CMakeFiles/obj.MLIRAffineAnalysis.dir/LoopAnalysis.cpp.gcda
        -:    0:Runs:116163
        -:    1://===- LoopAnalysis.cpp - Misc loop analysis routines //-------------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements miscellaneous loop analysis routines.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#include "mlir/Dialect/Affine/Analysis/LoopAnalysis.h"
        -:   14:
        -:   15:#include "mlir/Analysis/SliceAnalysis.h"
        -:   16:#include "mlir/Dialect/Affine/Analysis/AffineAnalysis.h"
        -:   17:#include "mlir/Dialect/Affine/Analysis/AffineStructures.h"
        -:   18:#include "mlir/Dialect/Affine/Analysis/NestedMatcher.h"
        -:   19:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   20:#include "mlir/Dialect/Affine/IR/AffineValueMap.h"
        -:   21:#include "mlir/Support/MathExtras.h"
        -:   22:
        -:   23:#include "llvm/ADT/DenseSet.h"
        -:   24:#include "llvm/ADT/SmallPtrSet.h"
        -:   25:#include "llvm/ADT/SmallString.h"
        -:   26:#include <numeric>
        -:   27:#include <type_traits>
        -:   28:
        -:   29:using namespace mlir;
        -:   30:
        -:   31:/// Returns the trip count of the loop as an affine expression if the latter is
        -:   32:/// expressible as an affine expression, and nullptr otherwise. The trip count
        -:   33:/// expression is simplified before returning. This method only utilizes map
        -:   34:/// composition to construct lower and upper bounds before computing the trip
        -:   35:/// count expressions.
function _ZN4mlir26getTripCountMapAndOperandsENS_11AffineForOpEPNS_9AffineMapEPN4llvm15SmallVectorImplINS_5ValueEEE called 6983 returned 100% blocks executed 96%
     6983:   36:void mlir::getTripCountMapAndOperands(
        -:   37:    AffineForOp forOp, AffineMap *tripCountMap,
        -:   38:    SmallVectorImpl<Value> *tripCountOperands) {
     6983:   39:  MLIRContext *context = forOp.getContext();
call    0 returned 100%
     6983:   40:  int64_t step = forOp.getStep();
call    0 returned 100%
     6983:   41:  int64_t loopSpan;
     6983:   42:  if (forOp.hasConstantBounds()) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 1%
     6976:   43:    int64_t lb = forOp.getConstantLowerBound();
call    0 returned 100%
     6976:   44:    int64_t ub = forOp.getConstantUpperBound();
call    0 returned 100%
     6976:   45:    loopSpan = ub - lb;
     6976:   46:    if (loopSpan < 0)
        -:   47:      loopSpan = 0;
     6976:   48:    *tripCountMap = AffineMap::getConstantMap(ceilDiv(loopSpan, step), context);
call    0 returned 100%
call    1 returned 100%
     6976:   49:    tripCountOperands->clear();
     6976:   50:    return;
        -:   51:  }
        7:   52:  auto lbMap = forOp.getLowerBoundMap();
call    0 returned 100%
        7:   53:  auto ubMap = forOp.getUpperBoundMap();
call    0 returned 100%
        7:   54:  if (lbMap.getNumResults() != 1) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:   55:    *tripCountMap = AffineMap();
    #####:   56:    return;
        -:   57:  }
        -:   58:
        -:   59:  // Difference of each upper bound expression from the single lower bound
        -:   60:  // expression (divided by the step) provides the expressions for the trip
        -:   61:  // count map.
        7:   62:  AffineValueMap ubValueMap(ubMap, forOp.getUpperBoundOperands());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -:   63:
        7:   64:  SmallVector<AffineExpr, 4> lbSplatExpr(ubValueMap.getNumResults(),
call    0 returned 100%
       14:   65:                                         lbMap.getResult(0));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        7:   66:  auto lbMapSplat = AffineMap::get(lbMap.getNumDims(), lbMap.getNumSymbols(),
call    0 returned 100%
        7:   67:                                   lbSplatExpr, context);
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
       14:   68:  AffineValueMap lbSplatValueMap(lbMapSplat, forOp.getLowerBoundOperands());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
        -:   69:
        7:   70:  AffineValueMap tripCountValueMap;
call    0 returned 100%
        7:   71:  AffineValueMap::difference(ubValueMap, lbSplatValueMap, &tripCountValueMap);
call    0 returned 100%
       16:   72:  for (unsigned i = 0, e = tripCountValueMap.getNumResults(); i < e; ++i)
branch  0 taken 56% (fallthrough)
branch  1 taken 44%
       18:   73:    tripCountValueMap.setResult(i,
call    0 returned 100%
call    1 returned 100%
       18:   74:                                tripCountValueMap.getResult(i).ceilDiv(step));
call    0 returned 100%
        -:   75:
        7:   76:  *tripCountMap = tripCountValueMap.getAffineMap();
call    0 returned 100%
        7:   77:  tripCountOperands->assign(tripCountValueMap.getOperands().begin(),
call    0 returned 100%
call    1 returned 100%
       14:   78:                            tripCountValueMap.getOperands().end());
call    0 returned 100%
call    1 returned 100%
        -:   79:}
        -:   80:
        -:   81:/// Returns the trip count of the loop if it's a constant, None otherwise. This
        -:   82:/// method uses affine expression analysis (in turn using getTripCount) and is
        -:   83:/// able to determine constant trip count in non-trivial cases.
function _ZN4mlir20getConstantTripCountENS_11AffineForOpE called 6652 returned 100% blocks executed 71%
     6652:   84:Optional<uint64_t> mlir::getConstantTripCount(AffineForOp forOp) {
     6652:   85:  SmallVector<Value, 4> operands;
call    0 returned 100%
     6652:   86:  AffineMap map;
     6652:   87:  getTripCountMapAndOperands(forOp, &map, &operands);
call    0 returned 100%
        -:   88:
     6652:   89:  if (!map)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:   90:    return None;
        -:   91:
        -:   92:  // Take the min if all trip counts are constant.
     6652:   93:  Optional<uint64_t> tripCount;
    13300:   94:  for (auto resultExpr : map.getResults()) {
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
     6652:   95:    if (auto constExpr = resultExpr.dyn_cast<AffineConstantExpr>()) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 1%
     6648:   96:      if (tripCount.has_value())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:   97:        tripCount = std::min(tripCount.value(),
branch  0 never executed
branch  1 never executed
    #####:   98:                             static_cast<uint64_t>(constExpr.getValue()));
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:   99:      else
     6648:  100:        tripCount = constExpr.getValue();
call    0 returned 100%
        -:  101:    } else
        4:  102:      return None;
        -:  103:  }
     6648:  104:  return tripCount;
        -:  105:}
        -:  106:
        -:  107:/// Returns the greatest known integral divisor of the trip count. Affine
        -:  108:/// expression analysis is used (indirectly through getTripCount), and
        -:  109:/// this method is thus able to determine non-trivial divisors.
function _ZN4mlir28getLargestDivisorOfTripCountENS_11AffineForOpE called 331 returned 100% blocks executed 68%
      331:  110:uint64_t mlir::getLargestDivisorOfTripCount(AffineForOp forOp) {
      331:  111:  SmallVector<Value, 4> operands;
call    0 returned 100%
      331:  112:  AffineMap map;
      331:  113:  getTripCountMapAndOperands(forOp, &map, &operands);
call    0 returned 100%
        -:  114:
      331:  115:  if (!map)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        -:  116:    return 1;
        -:  117:
        -:  118:  // The largest divisor of the trip count is the GCD of the individual largest
        -:  119:  // divisors.
     331*:  120:  assert(map.getNumResults() >= 1 && "expected one or more results");
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
      331:  121:  Optional<uint64_t> gcd;
      662:  122:  for (auto resultExpr : map.getResults()) {
call    0 returned 100%
branch  1 taken 50% (fallthrough)
branch  2 taken 50%
      331:  123:    uint64_t thisGcd;
      331:  124:    if (auto constExpr = resultExpr.dyn_cast<AffineConstantExpr>()) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
      331:  125:      uint64_t tripCount = constExpr.getValue();
call    0 returned 100%
        -:  126:      // 0 iteration loops (greatest divisor is 2^64 - 1).
      331:  127:      if (tripCount == 0)
branch  0 taken 99% (fallthrough)
branch  1 taken 1%
        -:  128:        thisGcd = std::numeric_limits<uint64_t>::max();
        -:  129:      else
        -:  130:        // The greatest divisor is the trip count.
      329:  131:        thisGcd = tripCount;
        -:  132:    } else {
        -:  133:      // Trip count is not a known constant; return its largest known divisor.
    #####:  134:      thisGcd = resultExpr.getLargestKnownDivisor();
call    0 never executed
        -:  135:    }
      331:  136:    if (gcd.has_value())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
     331*:  137:      gcd = std::gcd(gcd.value(), thisGcd);
branch  0 never executed
branch  1 never executed
        -:  138:    else
      331:  139:      gcd = thisGcd;
        -:  140:  }
     331*:  141:  assert(gcd.has_value() && "value expected per above logic");
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
      331:  142:  return gcd.value();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  143:}
        -:  144:
        -:  145:/// Given an induction variable `iv` of type AffineForOp and an access `index`
        -:  146:/// of type index, returns `true` if `index` is independent of `iv` and
        -:  147:/// false otherwise. The determination supports composition with at most one
        -:  148:/// AffineApplyOp. The 'at most one AffineApplyOp' comes from the fact that
        -:  149:/// the composition of AffineApplyOp needs to be canonicalized by construction
        -:  150:/// to avoid writing code that composes arbitrary numbers of AffineApplyOps
        -:  151:/// everywhere. To achieve this, at the very least, the compose-affine-apply
        -:  152:/// pass must have been run.
        -:  153:///
        -:  154:/// Prerequisites:
        -:  155:///   1. `iv` and `index` of the proper type;
        -:  156:///   2. at most one reachable AffineApplyOp from index;
        -:  157:///
        -:  158:/// Returns false in cases with more than one AffineApplyOp, this is
        -:  159:/// conservative.
function _ZL22isAccessIndexInvariantN4mlir5ValueES0_ called 0 returned 0% blocks executed 0%
    #####:  160:static bool isAccessIndexInvariant(Value iv, Value index) {
    #####:  161:  assert(isForInductionVar(iv) && "iv must be a AffineForOp");
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  162:  assert(index.getType().isa<IndexType>() && "index must be of IndexType");
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  163:  SmallVector<Operation *, 4> affineApplyOps;
call    0 never executed
    #####:  164:  getReachableAffineApplyOps({index}, affineApplyOps);
call    0 never executed
        -:  165:
    #####:  166:  if (affineApplyOps.empty()) {
branch  0 never executed
branch  1 never executed
        -:  167:    // Pointer equality test because of Value pointer semantics.
    #####:  168:    return index != iv;
        -:  169:  }
        -:  170:
    #####:  171:  if (affineApplyOps.size() > 1) {
branch  0 never executed
branch  1 never executed
    #####:  172:    affineApplyOps[0]->emitRemark(
call    0 never executed
call    1 never executed
call    2 never executed
        -:  173:        "CompositionAffineMapsPass must have been run: there should be at most "
        -:  174:        "one AffineApplyOp, returning false conservatively.");
    #####:  175:    return false;
        -:  176:  }
        -:  177:
    #####:  178:  auto composeOp = cast<AffineApplyOp>(affineApplyOps[0]);
call    0 never executed
        -:  179:  // We need yet another level of indirection because the `dim` index of the
        -:  180:  // access may not correspond to the `dim` index of composeOp.
    #####:  181:  return !composeOp.getAffineValueMap().isFunctionOf(0, iv);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  182:}
        -:  183:
function _ZN4mlir20getInvariantAccessesENS_5ValueEN4llvm8ArrayRefIS0_EE called 0 returned 0% blocks executed 0%
    #####:  184:DenseSet<Value> mlir::getInvariantAccesses(Value iv, ArrayRef<Value> indices) {
    #####:  185:  DenseSet<Value> res;
call    0 never executed
    #####:  186:  for (auto val : indices) {
branch  0 never executed
branch  1 never executed
    #####:  187:    if (isAccessIndexInvariant(iv, val)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  188:      res.insert(val);
call    0 never executed
        -:  189:    }
        -:  190:  }
    #####:  191:  return res;
        -:  192:}
        -:  193:
        -:  194:/// Given:
        -:  195:///   1. an induction variable `iv` of type AffineForOp;
        -:  196:///   2. a `memoryOp` of type const LoadOp& or const StoreOp&;
        -:  197:/// determines whether `memoryOp` has a contiguous access along `iv`. Contiguous
        -:  198:/// is defined as either invariant or varying only along a unique MemRef dim.
        -:  199:/// Upon success, the unique MemRef dim is written in `memRefDim` (or -1 to
        -:  200:/// convey the memRef access is invariant along `iv`).
        -:  201:///
        -:  202:/// Prerequisites:
        -:  203:///   1. `memRefDim` ~= nullptr;
        -:  204:///   2. `iv` of the proper type;
        -:  205:///   3. the MemRef accessed by `memoryOp` has no layout map or at most an
        -:  206:///      identity layout map.
        -:  207:///
        -:  208:/// Currently only supports no layoutMap or identity layoutMap in the MemRef.
        -:  209:/// Returns false if the MemRef has a non-identity layoutMap or more than 1
        -:  210:/// layoutMap. This is conservative.
        -:  211:///
        -:  212:// TODO: check strides.
        -:  213:template <typename LoadOrStoreOp>
    #####:  214:static bool isContiguousAccess(Value iv, LoadOrStoreOp memoryOp,
        -:  215:                               int *memRefDim) {
        -:  216:  static_assert(
        -:  217:      llvm::is_one_of<LoadOrStoreOp, AffineLoadOp, AffineStoreOp>::value,
        -:  218:      "Must be called on either LoadOp or StoreOp");
    #####:  219:  assert(memRefDim && "memRefDim == nullptr");
    #####:  220:  auto memRefType = memoryOp.getMemRefType();
        -:  221:
    #####:  222:  if (!memRefType.getLayout().isIdentity())
    #####:  223:    return memoryOp.emitError("NYI: non-trivial layoutMap"), false;
        -:  224:
    #####:  225:  int uniqueVaryingIndexAlongIv = -1;
    #####:  226:  auto accessMap = memoryOp.getAffineMap();
    #####:  227:  SmallVector<Value, 4> mapOperands(memoryOp.getMapOperands());
    #####:  228:  unsigned numDims = accessMap.getNumDims();
    #####:  229:  for (unsigned i = 0, e = memRefType.getRank(); i < e; ++i) {
        -:  230:    // Gather map operands used result expr 'i' in 'exprOperands'.
    #####:  231:    SmallVector<Value, 4> exprOperands;
    #####:  232:    auto resultExpr = accessMap.getResult(i);
    #####:  233:    resultExpr.walk([&](AffineExpr expr) {
    #####:  234:      if (auto dimExpr = expr.dyn_cast<AffineDimExpr>())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  235:        exprOperands.push_back(mapOperands[dimExpr.getPosition()]);
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
call    7 never executed
    #####:  236:      else if (auto symExpr = expr.dyn_cast<AffineSymbolExpr>())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  237:        exprOperands.push_back(mapOperands[numDims + symExpr.getPosition()]);
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
call    7 never executed
        -:  238:    });
        -:  239:    // Check access invariance of each operand in 'exprOperands'.
    #####:  240:    for (auto exprOperand : exprOperands) {
    #####:  241:      if (!isAccessIndexInvariant(iv, exprOperand)) {
    #####:  242:        if (uniqueVaryingIndexAlongIv != -1) {
        -:  243:          // 2+ varying indices -> do not vectorize along iv.
    #####:  244:          return false;
        -:  245:        }
    #####:  246:        uniqueVaryingIndexAlongIv = i;
        -:  247:      }
        -:  248:    }
        -:  249:  }
        -:  250:
    #####:  251:  if (uniqueVaryingIndexAlongIv == -1)
    #####:  252:    *memRefDim = -1;
        -:  253:  else
    #####:  254:    *memRefDim = memRefType.getRank() - (uniqueVaryingIndexAlongIv + 1);
        -:  255:  return true;
        -:  256:}
------------------
_Z18isContiguousAccessIN4mlir13AffineStoreOpEEbNS0_5ValueET_Pi:
function _Z18isContiguousAccessIN4mlir13AffineStoreOpEEbNS0_5ValueET_Pi called 0 returned 0% blocks executed 0%
    #####:  214:static bool isContiguousAccess(Value iv, LoadOrStoreOp memoryOp,
        -:  215:                               int *memRefDim) {
        -:  216:  static_assert(
        -:  217:      llvm::is_one_of<LoadOrStoreOp, AffineLoadOp, AffineStoreOp>::value,
        -:  218:      "Must be called on either LoadOp or StoreOp");
    #####:  219:  assert(memRefDim && "memRefDim == nullptr");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  220:  auto memRefType = memoryOp.getMemRefType();
call    0 never executed
        -:  221:
    #####:  222:  if (!memRefType.getLayout().isIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  223:    return memoryOp.emitError("NYI: non-trivial layoutMap"), false;
call    0 never executed
call    1 never executed
call    2 never executed
        -:  224:
    #####:  225:  int uniqueVaryingIndexAlongIv = -1;
    #####:  226:  auto accessMap = memoryOp.getAffineMap();
call    0 never executed
    #####:  227:  SmallVector<Value, 4> mapOperands(memoryOp.getMapOperands());
call    0 never executed
call    1 never executed
    #####:  228:  unsigned numDims = accessMap.getNumDims();
call    0 never executed
    #####:  229:  for (unsigned i = 0, e = memRefType.getRank(); i < e; ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  230:    // Gather map operands used result expr 'i' in 'exprOperands'.
    #####:  231:    SmallVector<Value, 4> exprOperands;
    #####:  232:    auto resultExpr = accessMap.getResult(i);
call    0 never executed
    #####:  233:    resultExpr.walk([&](AffineExpr expr) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  234:      if (auto dimExpr = expr.dyn_cast<AffineDimExpr>())
        -:  235:        exprOperands.push_back(mapOperands[dimExpr.getPosition()]);
        -:  236:      else if (auto symExpr = expr.dyn_cast<AffineSymbolExpr>())
        -:  237:        exprOperands.push_back(mapOperands[numDims + symExpr.getPosition()]);
        -:  238:    });
        -:  239:    // Check access invariance of each operand in 'exprOperands'.
    #####:  240:    for (auto exprOperand : exprOperands) {
branch  0 never executed
branch  1 never executed
    #####:  241:      if (!isAccessIndexInvariant(iv, exprOperand)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  242:        if (uniqueVaryingIndexAlongIv != -1) {
branch  0 never executed
branch  1 never executed
        -:  243:          // 2+ varying indices -> do not vectorize along iv.
    #####:  244:          return false;
branch  0 never executed
branch  1 never executed
        -:  245:        }
    #####:  246:        uniqueVaryingIndexAlongIv = i;
        -:  247:      }
        -:  248:    }
        -:  249:  }
        -:  250:
    #####:  251:  if (uniqueVaryingIndexAlongIv == -1)
branch  0 never executed
branch  1 never executed
    #####:  252:    *memRefDim = -1;
        -:  253:  else
    #####:  254:    *memRefDim = memRefType.getRank() - (uniqueVaryingIndexAlongIv + 1);
call    0 never executed
        -:  255:  return true;
        -:  256:}
------------------
_Z18isContiguousAccessIN4mlir12AffineLoadOpEEbNS0_5ValueET_Pi:
function _Z18isContiguousAccessIN4mlir12AffineLoadOpEEbNS0_5ValueET_Pi called 0 returned 0% blocks executed 0%
    #####:  214:static bool isContiguousAccess(Value iv, LoadOrStoreOp memoryOp,
        -:  215:                               int *memRefDim) {
        -:  216:  static_assert(
        -:  217:      llvm::is_one_of<LoadOrStoreOp, AffineLoadOp, AffineStoreOp>::value,
        -:  218:      "Must be called on either LoadOp or StoreOp");
    #####:  219:  assert(memRefDim && "memRefDim == nullptr");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  220:  auto memRefType = memoryOp.getMemRefType();
call    0 never executed
        -:  221:
    #####:  222:  if (!memRefType.getLayout().isIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  223:    return memoryOp.emitError("NYI: non-trivial layoutMap"), false;
call    0 never executed
call    1 never executed
call    2 never executed
        -:  224:
    #####:  225:  int uniqueVaryingIndexAlongIv = -1;
    #####:  226:  auto accessMap = memoryOp.getAffineMap();
call    0 never executed
    #####:  227:  SmallVector<Value, 4> mapOperands(memoryOp.getMapOperands());
call    0 never executed
call    1 never executed
    #####:  228:  unsigned numDims = accessMap.getNumDims();
call    0 never executed
    #####:  229:  for (unsigned i = 0, e = memRefType.getRank(); i < e; ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  230:    // Gather map operands used result expr 'i' in 'exprOperands'.
    #####:  231:    SmallVector<Value, 4> exprOperands;
    #####:  232:    auto resultExpr = accessMap.getResult(i);
call    0 never executed
    #####:  233:    resultExpr.walk([&](AffineExpr expr) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  234:      if (auto dimExpr = expr.dyn_cast<AffineDimExpr>())
        -:  235:        exprOperands.push_back(mapOperands[dimExpr.getPosition()]);
        -:  236:      else if (auto symExpr = expr.dyn_cast<AffineSymbolExpr>())
        -:  237:        exprOperands.push_back(mapOperands[numDims + symExpr.getPosition()]);
        -:  238:    });
        -:  239:    // Check access invariance of each operand in 'exprOperands'.
    #####:  240:    for (auto exprOperand : exprOperands) {
branch  0 never executed
branch  1 never executed
    #####:  241:      if (!isAccessIndexInvariant(iv, exprOperand)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  242:        if (uniqueVaryingIndexAlongIv != -1) {
branch  0 never executed
branch  1 never executed
        -:  243:          // 2+ varying indices -> do not vectorize along iv.
    #####:  244:          return false;
branch  0 never executed
branch  1 never executed
        -:  245:        }
    #####:  246:        uniqueVaryingIndexAlongIv = i;
        -:  247:      }
        -:  248:    }
        -:  249:  }
        -:  250:
    #####:  251:  if (uniqueVaryingIndexAlongIv == -1)
branch  0 never executed
branch  1 never executed
    #####:  252:    *memRefDim = -1;
        -:  253:  else
    #####:  254:    *memRefDim = memRefType.getRank() - (uniqueVaryingIndexAlongIv + 1);
call    0 never executed
        -:  255:  return true;
        -:  256:}
------------------
        -:  257:
        -:  258:template <typename LoadOrStoreOp>
    #####:  259:static bool isVectorElement(LoadOrStoreOp memoryOp) {
    #####:  260:  auto memRefType = memoryOp.getMemRefType();
    #####:  261:  return memRefType.getElementType().template isa<VectorType>();
        -:  262:}
------------------
_Z15isVectorElementIN4mlir13AffineStoreOpEEbT_:
function _Z15isVectorElementIN4mlir13AffineStoreOpEEbT_ called 0 returned 0% blocks executed 0%
    #####:  259:static bool isVectorElement(LoadOrStoreOp memoryOp) {
    #####:  260:  auto memRefType = memoryOp.getMemRefType();
call    0 never executed
    #####:  261:  return memRefType.getElementType().template isa<VectorType>();
call    0 never executed
call    1 never executed
        -:  262:}
------------------
_Z15isVectorElementIN4mlir12AffineLoadOpEEbT_:
function _Z15isVectorElementIN4mlir12AffineLoadOpEEbT_ called 0 returned 0% blocks executed 0%
    #####:  259:static bool isVectorElement(LoadOrStoreOp memoryOp) {
    #####:  260:  auto memRefType = memoryOp.getMemRefType();
call    0 never executed
    #####:  261:  return memRefType.getElementType().template isa<VectorType>();
call    0 never executed
call    1 never executed
        -:  262:}
------------------
        -:  263:
        -:  264:using VectorizableOpFun = std::function<bool(AffineForOp, Operation &)>;
        -:  265:
        -:  266:static bool
function _ZL32isVectorizableLoopBodyWithOpCondN4mlir11AffineForOpERKSt8functionIFbS0_RNS_9OperationEEERNS_13NestedPatternE called 0 returned 0% blocks executed 0%
    #####:  267:isVectorizableLoopBodyWithOpCond(AffineForOp loop,
        -:  268:                                 const VectorizableOpFun &isVectorizableOp,
        -:  269:                                 NestedPattern &vectorTransferMatcher) {
    #####:  270:  auto *forOp = loop.getOperation();
call    0 never executed
        -:  271:
        -:  272:  // No vectorization across conditionals for now.
    #####:  273:  auto conditionals = matcher::If();
call    0 never executed
    #####:  274:  SmallVector<NestedMatch, 8> conditionalsMatched;
call    0 never executed
call    1 never executed
    #####:  275:  conditionals.match(forOp, &conditionalsMatched);
call    0 never executed
    #####:  276:  if (!conditionalsMatched.empty()) {
branch  0 never executed
branch  1 never executed
        -:  277:    return false;
        -:  278:  }
        -:  279:
        -:  280:  // No vectorization across unknown regions.
    #####:  281:  auto regions = matcher::Op([](Operation &op) -> bool {
    #####:  282:    return op.getNumRegions() != 0 && !isa<AffineIfOp, AffineForOp>(op);
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  283:  });
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  284:  SmallVector<NestedMatch, 8> regionsMatched;
call    0 never executed
call    1 never executed
    #####:  285:  regions.match(forOp, &regionsMatched);
call    0 never executed
    #####:  286:  if (!regionsMatched.empty()) {
branch  0 never executed
branch  1 never executed
        -:  287:    return false;
        -:  288:  }
        -:  289:
    #####:  290:  SmallVector<NestedMatch, 8> vectorTransfersMatched;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  291:  vectorTransferMatcher.match(forOp, &vectorTransfersMatched);
call    0 never executed
    #####:  292:  if (!vectorTransfersMatched.empty()) {
branch  0 never executed
branch  1 never executed
        -:  293:    return false;
        -:  294:  }
        -:  295:
    #####:  296:  auto loadAndStores = matcher::Op(matcher::isLoadOrStore);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  297:  SmallVector<NestedMatch, 8> loadAndStoresMatched;
call    0 never executed
call    1 never executed
    #####:  298:  loadAndStores.match(forOp, &loadAndStoresMatched);
call    0 never executed
    #####:  299:  for (auto ls : loadAndStoresMatched) {
branch  0 never executed
branch  1 never executed
    #####:  300:    auto *op = ls.getMatchedOperation();
call    0 never executed
    #####:  301:    auto load = dyn_cast<AffineLoadOp>(op);
call    0 never executed
    #####:  302:    auto store = dyn_cast<AffineStoreOp>(op);
call    0 never executed
        -:  303:    // Only scalar types are considered vectorizable, all load/store must be
        -:  304:    // vectorizable for a loop to qualify as vectorizable.
        -:  305:    // TODO: ponder whether we want to be more general here.
    #####:  306:    bool vector = load ? isVectorElement(load) : isVectorElement(store);
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  307:    if (vector) {
branch  0 never executed
branch  1 never executed
    #####:  308:      return false;
        -:  309:    }
    #####:  310:    if (isVectorizableOp && !isVectorizableOp(loop, *op)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  311:      return false;
        -:  312:    }
        -:  313:  }
    #####:  314:  return true;
        -:  315:}
        -:  316:
function _ZN4mlir22isVectorizableLoopBodyENS_11AffineForOpEPiRNS_13NestedPatternE called 0 returned 0% blocks executed 0%
    #####:  317:bool mlir::isVectorizableLoopBody(AffineForOp loop, int *memRefDim,
        -:  318:                                  NestedPattern &vectorTransferMatcher) {
    #####:  319:  *memRefDim = -1;
function _ZZN4mlir22isVectorizableLoopBodyENS_11AffineForOpEPiRNS_13NestedPatternEENKUlS0_RNS_9OperationEE_clES0_S5_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  320:  VectorizableOpFun fun([memRefDim](AffineForOp loop, Operation &op) {
    #####:  321:    auto load = dyn_cast<AffineLoadOp>(op);
call    0 never executed
    #####:  322:    auto store = dyn_cast<AffineStoreOp>(op);
call    0 never executed
    #####:  323:    int thisOpMemRefDim = -1;
    #####:  324:    bool isContiguous = load ? isContiguousAccess(loop.getInductionVar(), load,
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  325:                                                  &thisOpMemRefDim)
    #####:  326:                             : isContiguousAccess(loop.getInductionVar(), store,
call    0 never executed
    #####:  327:                                                  &thisOpMemRefDim);
branch  0 never executed
branch  1 never executed
    #####:  328:    if (thisOpMemRefDim != -1) {
branch  0 never executed
branch  1 never executed
        -:  329:      // If memory accesses vary across different dimensions then the loop is
        -:  330:      // not vectorizable.
    #####:  331:      if (*memRefDim != -1 && *memRefDim != thisOpMemRefDim)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  332:        return false;
    #####:  333:      *memRefDim = thisOpMemRefDim;
        -:  334:    }
        -:  335:    return isContiguous;
    #####:  336:  });
call    0 never executed
    #####:  337:  return isVectorizableLoopBodyWithOpCond(loop, fun, vectorTransferMatcher);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  338:}
        -:  339:
function _ZN4mlir22isVectorizableLoopBodyENS_11AffineForOpERNS_13NestedPatternE called 0 returned 0% blocks executed 0%
    #####:  340:bool mlir::isVectorizableLoopBody(AffineForOp loop,
        -:  341:                                  NestedPattern &vectorTransferMatcher) {
    #####:  342:  return isVectorizableLoopBodyWithOpCond(loop, nullptr, vectorTransferMatcher);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  343:}
        -:  344:
        -:  345:/// Checks whether SSA dominance would be violated if a for op's body
        -:  346:/// operations are shifted by the specified shifts. This method checks if a
        -:  347:/// 'def' and all its uses have the same shift factor.
        -:  348:// TODO: extend this to check for memory-based dependence violation when we have
        -:  349:// the support.
function _ZN4mlir18isOpwiseShiftValidENS_11AffineForOpEN4llvm8ArrayRefImEE called 0 returned 0% blocks executed 0%
    #####:  350:bool mlir::isOpwiseShiftValid(AffineForOp forOp, ArrayRef<uint64_t> shifts) {
    #####:  351:  auto *forBody = forOp.getBody();
call    0 never executed
    #####:  352:  assert(shifts.size() == forBody->getOperations().size());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  353:
        -:  354:  // Work backwards over the body of the block so that the shift of a use's
        -:  355:  // ancestor operation in the block gets recorded before it's looked up.
    #####:  356:  DenseMap<Operation *, uint64_t> forBodyShift;
call    0 never executed
    #####:  357:  for (const auto &it :
    #####:  358:       llvm::enumerate(llvm::reverse(forBody->getOperations()))) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  359:    auto &op = it.value();
call    0 never executed
        -:  360:
        -:  361:    // Get the index of the current operation, note that we are iterating in
        -:  362:    // reverse so we need to fix it up.
    #####:  363:    size_t index = shifts.size() - it.index() - 1;
branch  0 never executed
branch  1 never executed
        -:  364:
        -:  365:    // Remember the shift of this operation.
    #####:  366:    uint64_t shift = shifts[index];
branch  0 never executed
branch  1 never executed
    #####:  367:    forBodyShift.try_emplace(&op, shift);
call    0 never executed
        -:  368:
        -:  369:    // Validate the results of this operation if it were to be shifted.
    #####:  370:    for (unsigned i = 0, e = op.getNumResults(); i < e; ++i) {
branch  0 never executed
branch  1 never executed
    #####:  371:      Value result = op.getResult(i);
branch  0 never executed
branch  1 never executed
    #####:  372:      for (auto *user : result.getUsers()) {
branch  0 never executed
branch  1 never executed
        -:  373:        // If an ancestor operation doesn't lie in the block of forOp,
        -:  374:        // there is no shift to check.
    #####:  375:        if (auto *ancOp = forBody->findAncestorOpInBlock(*user)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  376:          assert(forBodyShift.count(ancOp) > 0 && "ancestor expected in map");
call    0 never executed
call    1 never executed
    #####:  377:          if (shift != forBodyShift[ancOp])
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  378:            return false;
        -:  379:        }
        -:  380:      }
        -:  381:    }
        -:  382:  }
    #####:  383:  return true;
        -:  384:}
