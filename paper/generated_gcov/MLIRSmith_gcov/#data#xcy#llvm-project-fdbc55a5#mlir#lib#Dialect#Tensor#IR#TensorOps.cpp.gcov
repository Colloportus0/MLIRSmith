        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Tensor/IR/TensorOps.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Tensor/IR/CMakeFiles/obj.MLIRTensorDialect.dir/TensorOps.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Tensor/IR/CMakeFiles/obj.MLIRTensorDialect.dir/TensorOps.cpp.gcda
        -:    0:Runs:116161
        -:    1://===----------------------------------------------------------------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8:
        -:    9:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   10:#include "mlir/Dialect/Arith/Utils/Utils.h"
        -:   11:#include "mlir/Dialect/Complex/IR/Complex.h"
        -:   12:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   13:#include "mlir/Dialect/Utils/ReshapeOpsUtils.h"
        -:   14:#include "mlir/Dialect/Utils/StaticValueUtils.h"
        -:   15:#include "mlir/IR/BlockAndValueMapping.h"
        -:   16:#include "mlir/IR/Builders.h"
        -:   17:#include "mlir/IR/BuiltinAttributeInterfaces.h"
        -:   18:#include "mlir/IR/Matchers.h"
        -:   19:#include "mlir/IR/TypeUtilities.h"
        -:   20:#include "mlir/Interfaces/DestinationStyleOpInterface.h"
        -:   21:#include "llvm/ADT/DenseSet.h"
        -:   22:#include "llvm/ADT/STLExtras.h"
        -:   23:#include "llvm/ADT/SmallBitVector.h"
        -:   24:#include "llvm/ADT/StringRef.h"
        -:   25:#include <algorithm>
        -:   26:
        -:   27:using namespace mlir;
        -:   28:using namespace mlir::tensor;
        -:   29:
        -:   30:/// Materialize a single constant operation from a given attribute value with
        -:   31:/// the desired resultant type.
function _ZN4mlir6tensor13TensorDialect19materializeConstantERNS_9OpBuilderENS_9AttributeENS_4TypeENS_8LocationE called 515130 returned 100% blocks executed 50%
   515130:   32:Operation *TensorDialect::materializeConstant(OpBuilder &builder,
        -:   33:                                              Attribute value, Type type,
        -:   34:                                              Location loc) {
   515130:   35:  if (arith::ConstantOp::isBuildableWith(value, type))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
   515130:   36:    return builder.create<arith::ConstantOp>(loc, value, type);
call    0 returned 100%
    #####:   37:  if (complex::ConstantOp::isBuildableWith(value, type))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   38:    return builder.create<complex::ConstantOp>(loc, type,
call    0 never executed
    #####:   39:                                               value.cast<ArrayAttr>());
call    0 never executed
        -:   40:  return nullptr;
        -:   41:}
        -:   42:
function _ZN4mlir6tensor13getMixedSizesERNS_9OpBuilderENS_8LocationENS_5ValueE called 0 returned 0% blocks executed 0%
    #####:   43:SmallVector<OpFoldResult> tensor::getMixedSizes(OpBuilder &builder,
        -:   44:                                                Location loc, Value value) {
    #####:   45:  auto tensorType = value.getType().cast<RankedTensorType>();
call    0 never executed
    #####:   46:  SmallVector<OpFoldResult> result;
    #####:   47:  for (int64_t i = 0; i < tensorType.getRank(); ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   48:    if (tensorType.isDynamicDim(i)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   49:      Value size = builder.create<tensor::DimOp>(loc, value, i);
call    0 never executed
call    1 never executed
    #####:   50:      result.push_back(size);
call    0 never executed
call    1 never executed
        -:   51:    } else {
    #####:   52:      result.push_back(builder.getIndexAttr(tensorType.getDimSize(i)));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:   53:    }
        -:   54:  }
    #####:   55:  return result;
        -:   56:}
        -:   57:
function _ZN4mlir6tensor22getOrCreateDestinationERNS_9OpBuilderENS_8LocationENS_8OpResultE called 0 returned 0% blocks executed 0%
    #####:   58:FailureOr<Value> tensor::getOrCreateDestination(OpBuilder &b, Location loc,
        -:   59:                                                OpResult opResult) {
    #####:   60:  auto tensorType = opResult.getType().dyn_cast<TensorType>();
call    0 never executed
    #####:   61:  assert(tensorType && "expected tensor type");
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:   62:
        -:   63:  // If the op has a destination, it implements DestinationStyleOpInterface and
        -:   64:  // we can query the destination operand from that interface.
    #####:   65:  auto destOp = opResult.getDefiningOp<DestinationStyleOpInterface>();
call    0 never executed
    #####:   66:  if (destOp)
branch  0 never executed
branch  1 never executed
    #####:   67:    return destOp.getTiedOpOperand(opResult)->get();
call    0 never executed
        -:   68:
        -:   69:  // Otherwise, create a new destination tensor with the same shape.
    #####:   70:  OpBuilder::InsertionGuard g(b);
call    0 never executed
    #####:   71:  b.setInsertionPoint(opResult.getDefiningOp());
call    0 never executed
call    1 never executed
        -:   72:
        -:   73:  // Compute sizes.
    #####:   74:  SmallVector<OpFoldResult> mixedSizes;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   75:  if (!tensorType.hasStaticShape()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:   76:    // Dynamic shape: Query ReifyRankedShapedTypeOpInterface.
    #####:   77:    ReifiedRankedShapedTypeDims reifiedShapes;
call    0 never executed
    #####:   78:    ReifyRankedShapedTypeOpInterface reifyShapedTypeInterface =
    #####:   79:        dyn_cast<ReifyRankedShapedTypeOpInterface>(opResult.getDefiningOp());
call    0 never executed
call    1 never executed
    #####:   80:    if (!reifyShapedTypeInterface)
branch  0 never executed
branch  1 never executed
    #####:   81:      return failure();
    #####:   82:    if (failed(reifyShapedTypeInterface.reifyResultShapes(b, reifiedShapes)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   83:      return failure();
    #####:   84:    mixedSizes = getAsOpFoldResult(reifiedShapes[opResult.getResultNumber()]);
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
        -:   85:  } else {
        -:   86:    // Static shape: Take static sizes directly.
    #####:   87:    for (int64_t sz : tensorType.getShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   88:      mixedSizes.push_back(b.getIndexAttr(sz));
call    0 never executed
call    1 never executed
call    2 never executed
        -:   89:  }
        -:   90:
        -:   91:  // Create empty tensor.
    #####:   92:  Value emptyTensor =
    #####:   93:      b.create<tensor::EmptyOp>(loc, mixedSizes, tensorType.getElementType());
call    0 never executed
call    1 never executed
    #####:   94:  return emptyTensor;
        -:   95:}
        -:   96:
function _ZN4mlir6tensor23getOrCreateDestinationsERNS_9OpBuilderENS_8LocationEPNS_9OperationERN4llvm11SmallVectorINS_5ValueELj6EEE called 0 returned 0% blocks executed 0%
    #####:   97:LogicalResult tensor::getOrCreateDestinations(OpBuilder &b, Location loc,
        -:   98:                                              Operation *op,
        -:   99:                                              SmallVector<Value> &result) {
    #####:  100:  for (OpResult opResult : op->getResults()) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  101:    if (opResult.getType().isa<TensorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  102:      FailureOr<Value> destination = getOrCreateDestination(b, loc, opResult);
call    0 never executed
    #####:  103:      if (failed(destination))
branch  0 never executed
branch  1 never executed
    #####:  104:        return failure();
    #####:  105:      result.push_back(*destination);
call    0 never executed
        -:  106:    }
        -:  107:  }
    #####:  108:  return success();
        -:  109:}
        -:  110:
        -:  111://===----------------------------------------------------------------------===//
        -:  112:// CastOp
        -:  113://===----------------------------------------------------------------------===//
        -:  114:
function _ZN4mlir6tensor6CastOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 35445253 returned 100% blocks executed 100%
 35445253:  115:void CastOp::getAsmResultNames(function_ref<void(Value, StringRef)> setNameFn) {
 35445253:  116:  setNameFn(getResult(), "cast");
call    0 returned 100%
 35445253:  117:}
        -:  118:
        -:  119:/// Returns true if `target` is a ranked tensor type that preserves static
        -:  120:/// information available in the `source` ranked tensor type.
function _ZN4mlir6tensor26preservesStaticInformationENS_4TypeES1_ called 4477 returned 100% blocks executed 100%
     4477:  121:bool mlir::tensor::preservesStaticInformation(Type source, Type target) {
     4477:  122:  auto sourceType = source.dyn_cast<RankedTensorType>();
call    0 returned 100%
     4477:  123:  auto targetType = target.dyn_cast<RankedTensorType>();
call    0 returned 100%
        -:  124:
        -:  125:  // Requires RankedTensorType.
     4477:  126:  if (!sourceType || !targetType)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  127:    return false;
        -:  128:
        -:  129:  // Requires same elemental type.
     4477:  130:  if (sourceType.getElementType() != targetType.getElementType())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  131:    return false;
        -:  132:
        -:  133:  // Requires same rank.
     4477:  134:  if (sourceType.getRank() != targetType.getRank())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  135:    return false;
        -:  136:
        -:  137:  // If cast is towards more static sizes along any dimension, don't fold.
     4591:  138:  for (auto t : llvm::zip(sourceType.getShape(), targetType.getShape())) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 99% (fallthrough)
branch  3 taken 1%
     4546:  139:    if (!ShapedType::isDynamic(std::get<0>(t)) &&
branch  0 taken 98% (fallthrough)
branch  1 taken 2%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
     4436:  140:        ShapedType::isDynamic(std::get<1>(t)))
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
     4432:  141:      return false;
        -:  142:  }
        -:  143:
       45:  144:  return true;
        -:  145:}
        -:  146:
        -:  147:/// Determines whether tensor::CastOp casts to a more dynamic version of the
        -:  148:/// source tensor. This is useful to fold a tensor.cast into a consuming op and
        -:  149:/// implement canonicalization patterns for ops in different dialects that may
        -:  150:/// consume the results of tensor.cast operations. Such foldable tensor.cast
        -:  151:/// operations are typically inserted as `slice` ops and are canonicalized,
        -:  152:/// to preserve the type compatibility of their uses.
        -:  153:///
        -:  154:/// Returns true when all conditions are met:
        -:  155:/// 1. source and result are ranked tensors with same element type and rank.
        -:  156:/// 2. the tensor type has more static information than the result
        -:  157:///
        -:  158:/// Example:
        -:  159:/// ```mlir
        -:  160:///   %1 = tensor.cast %0 : tensor<8x16xf32> to tensor<?x?xf32>
        -:  161:///   %2 = consumer %1 ... : tensor<?x?xf32> ...
        -:  162:/// ```
        -:  163:///
        -:  164:/// folds into:
        -:  165:///
        -:  166:/// ```mlir
        -:  167:///   %2 = consumer %0 ... : tensor<8x16xf32> ...
        -:  168:/// ```
function _ZN4mlir6tensor21canFoldIntoConsumerOpENS0_6CastOpE called 879 returned 100% blocks executed 100%
      879:  169:bool mlir::tensor::canFoldIntoConsumerOp(CastOp castOp) {
      879:  170:  if (!castOp)
branch  0 taken 5% (fallthrough)
branch  1 taken 95%
        -:  171:    return false;
        -:  172:
        -:  173:  // Can fold if the source of cast has at least as much static information as
        -:  174:  // its results.
       45:  175:  return preservesStaticInformation(castOp.getType(),
call    0 returned 100%
       90:  176:                                    castOp.getSource().getType());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -:  177:}
        -:  178:
        -:  179:/// Determines whether the tensor::CastOp casts to a more static version of the
        -:  180:/// source tensor. This is useful to fold into a producing op and implement
        -:  181:/// canonicaliation patterns with the `tensor.cast` op as the root, but producer
        -:  182:/// being from different dialects. Returns true when all conditions are met:
        -:  183:/// 1. source and result and ranked tensors with same element type and rank.
        -:  184:/// 2. the result type has more static information than the source.
        -:  185:///
        -:  186:/// Example:
        -:  187:/// ```mlir
        -:  188:///   %1 = producer ... : tensor<?x?xf32>
        -:  189:///   %2 = tensor.cast %1 : tensor<?x?xf32> to tensor<8x16xf32>
        -:  190:/// ```
        -:  191:///
        -:  192:/// can be canonicalized to :
        -:  193:///
        -:  194:/// ```mlir
        -:  195:///   %2 = producer ... : tensor<8x16xf32>
        -:  196:/// ```
        -:  197:/// Not all ops might be canonicalizable this way, but for those that can be,
        -:  198:/// this method provides a check that it is worth doing the canonicalization.
function _ZN4mlir6tensor21canFoldIntoProducerOpENS0_6CastOpE called 4432 returned 100% blocks executed 100%
     4432:  199:bool mlir::tensor::canFoldIntoProducerOp(CastOp castOp) {
     4432:  200:  if (!castOp)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        -:  201:    return false;
     8864:  202:  return preservesStaticInformation(castOp.getSource().getType(),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
     8864:  203:                                    castOp.getType());
call    0 returned 100%
        -:  204:}
        -:  205:
        -:  206:/// Performs folding of any operand of `op` if it comes from a tensor::CastOp
        -:  207:/// that can be folded.
function _ZN4mlir6tensor14foldTensorCastEPNS_9OperationE called 243896 returned 100% blocks executed 44%
   243896:  208:LogicalResult mlir::tensor::foldTensorCast(Operation *op) {
   243896:  209:  bool folded = false;
  1209429:  210:  for (OpOperand &operand : op->getOpOperands()) {
call    0 returned 100%
branch  1 taken 80% (fallthrough)
branch  2 taken 20%
   965533:  211:    auto castOp = operand.get().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
  965533*:  212:    if (castOp && tensor::canFoldIntoConsumerOp(castOp)) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  213:      operand.set(castOp.getOperand());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  214:      folded = true;
        -:  215:    }
        -:  216:  }
   243896:  217:  return success(folded);
        -:  218:}
        -:  219:
function _ZN4mlir6tensor6CastOp17areCastCompatibleENS_9TypeRangeES2_ called 37372178 returned 100% blocks executed 100%
 37372178:  220:bool CastOp::areCastCompatible(TypeRange inputs, TypeRange outputs) {
 37372178:  221:  if (inputs.size() != 1 || outputs.size() != 1)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  222:    return false;
 37372178:  223:  Type a = inputs.front(), b = outputs.front();
call    0 returned 100%
call    1 returned 100%
 37372178:  224:  auto aT = a.dyn_cast<TensorType>();
call    0 returned 100%
 37372178:  225:  auto bT = b.dyn_cast<TensorType>();
call    0 returned 100%
 37372178:  226:  if (!aT || !bT)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  227:    return false;
        -:  228:
 37372178:  229:  if (aT.getElementType() != bT.getElementType())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  230:    return false;
        -:  231:
 37372178:  232:  return succeeded(verifyCompatibleShape(aT, bT));
call    0 returned 100%
        -:  233:}
        -:  234:
        -:  235:/// Compute a TensorType that has the joined shape knowledge of the two
        -:  236:/// given TensorTypes. The element types need to match.
function _ZL10joinShapesN4mlir10TensorTypeES0_ called 0 returned 0% blocks executed 0%
    #####:  237:static TensorType joinShapes(TensorType one, TensorType two) {
    #####:  238:  assert(one.getElementType() == two.getElementType());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
        -:  239:
    #####:  240:  if (!one.hasRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  241:    return two;
    #####:  242:  if (!two.hasRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  243:    return one;
        -:  244:
    #####:  245:  int64_t rank = one.getRank();
call    0 never executed
    #####:  246:  if (rank != two.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  247:    return {};
        -:  248:
    #####:  249:  SmallVector<int64_t, 4> join;
branch  0 never executed
branch  1 never executed
    #####:  250:  join.reserve(rank);
branch  0 never executed
branch  1 never executed
    #####:  251:  for (int64_t i = 0; i < rank; ++i) {
branch  0 never executed
branch  1 never executed
    #####:  252:    if (one.isDynamicDim(i)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  253:      join.push_back(two.getDimSize(i));
call    0 never executed
call    1 never executed
    #####:  254:      continue;
        -:  255:    }
    #####:  256:    if (two.isDynamicDim(i)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  257:      join.push_back(one.getDimSize(i));
call    0 never executed
call    1 never executed
    #####:  258:      continue;
        -:  259:    }
    #####:  260:    if (one.getDimSize(i) != two.getDimSize(i))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  261:      return {};
    #####:  262:    join.push_back(one.getDimSize(i));
call    0 never executed
call    1 never executed
        -:  263:  }
    #####:  264:  return RankedTensorType::get(join, one.getElementType());
call    0 never executed
call    1 never executed
        -:  265:}
        -:  266:
        -:  267:namespace {
        -:  268:
        -:  269:/// Replaces chains of two tensor.cast operations by a single tensor.cast
        -:  270:/// operation if doing so does not remove runtime constraints.
        -:  271:struct ChainedTensorCast : public OpRewritePattern<CastOp> {
        -:  272:  using OpRewritePattern<CastOp>::OpRewritePattern;
        -:  273:
function _ZNK12_GLOBAL__N_117ChainedTensorCast15matchAndRewriteEN4mlir6tensor6CastOpERNS1_15PatternRewriterE called 1965 returned 100% blocks executed 24%
     1965:  274:  LogicalResult matchAndRewrite(CastOp tensorCast,
        -:  275:                                PatternRewriter &rewriter) const final {
     1965:  276:    auto tensorCastOperand = tensorCast.getOperand().getDefiningOp<CastOp>();
call    0 returned 100%
call    1 returned 100%
        -:  277:
     1965:  278:    if (!tensorCastOperand)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     1965:  279:      return failure();
        -:  280:
    #####:  281:    auto sourceType =
call    0 never executed
    #####:  282:        tensorCastOperand.getOperand().getType().cast<TensorType>();
call    0 never executed
    #####:  283:    auto intermediateType = tensorCastOperand.getType().cast<TensorType>();
call    0 never executed
call    1 never executed
    #####:  284:    auto resultType = tensorCast.getType().cast<TensorType>();
call    0 never executed
call    1 never executed
        -:  285:
        -:  286:    // We can remove the intermediate cast if joining all three produces the
        -:  287:    // same result as just joining the source and result shapes.
    #####:  288:    auto firstJoin =
    #####:  289:        joinShapes(joinShapes(sourceType, intermediateType), resultType);
call    0 never executed
call    1 never executed
        -:  290:
        -:  291:    // The join might not exist if the cast sequence would fail at runtime.
    #####:  292:    if (!firstJoin)
branch  0 never executed
branch  1 never executed
    #####:  293:      return failure();
        -:  294:
        -:  295:    // The newJoin always exists if the above join exists, it might just contain
        -:  296:    // less information. If so, we cannot drop the intermediate cast, as doing
        -:  297:    // so would remove runtime checks.
    #####:  298:    auto newJoin = joinShapes(sourceType, resultType);
call    0 never executed
    #####:  299:    if (firstJoin != newJoin)
branch  0 never executed
branch  1 never executed
    #####:  300:      return failure();
        -:  301:
    #####:  302:    rewriter.replaceOpWithNewOp<CastOp>(tensorCast, resultType,
    #####:  303:                                        tensorCastOperand.getOperand());
call    0 never executed
call    1 never executed
    #####:  304:    return success();
        -:  305:  }
        -:  306:};
        -:  307:
        -:  308:/// Fold tensor.cast into tesor.extract_slice producer.
        -:  309:/// Example:
        -:  310:/// ```
        -:  311:///  %0 = tensor.extract_slice %arg0[%o, 0] [%s, 512] [1, 1] :
        -:  312:///    tensor<128x512xf32> to tensor<?x512xf32>
        -:  313:///  %1 = tensor.cast %0 : tensor<?x512xf32> to tensor<16x512xf32>
        -:  314:/// ```
        -:  315:/// ->
        -:  316:/// ```
        -:  317:/// %1 = tensor.extract_slice %arg0[%o, 0] [16, 512] [1, 1] :
        -:  318:///   tensor<128x512xf32> to tensor<16x512xf32>
        -:  319:/// ```
        -:  320:struct TensorCastExtractSlice : public OpRewritePattern<CastOp> {
        -:  321:  using OpRewritePattern<CastOp>::OpRewritePattern;
        -:  322:
function _ZNK12_GLOBAL__N_122TensorCastExtractSlice15matchAndRewriteEN4mlir6tensor6CastOpERNS1_15PatternRewriterE called 1965 returned 100% blocks executed 9%
     1965:  323:  LogicalResult matchAndRewrite(CastOp tensorCast,
        -:  324:                                PatternRewriter &rewriter) const final {
     1965:  325:    auto extractOperand =
call    0 returned 100%
     1965:  326:        tensorCast.getOperand().getDefiningOp<ExtractSliceOp>();
call    0 returned 100%
        -:  327:
    1965*:  328:    if (!extractOperand || !canFoldIntoProducerOp(tensorCast) ||
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  329:        tensorCast.getType().getShape() == tensorCast.getSource()
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  330:                                               .getType()
call    0 never executed
    #####:  331:                                               .cast<RankedTensorType>()
call    0 never executed
        -:  332:                                               .getShape())
     1965:  333:      return failure();
        -:  334:
    #####:  335:    SmallVector<OpFoldResult, 4> sizes = extractOperand.getMixedSizes();
call    0 never executed
    #####:  336:    auto dimMask = computeRankReductionMask(
    #####:  337:        extractFromI64ArrayAttr(extractOperand.getStaticSizes()),
call    0 never executed
call    1 never executed
    #####:  338:        extractOperand.getType().getShape());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:  339:    size_t dimIndex = 0;
    #####:  340:    for (size_t i = 0, e = sizes.size(); i < e; i++) {
branch  0 never executed
branch  1 never executed
    #####:  341:      if (dimMask && dimMask->count(i))
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  342:        continue;
    #####:  343:      int64_t dim = tensorCast.getType().getShape()[dimIndex++];
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  344:      if (ShapedType::isDynamic(dim))
branch  0 never executed
branch  1 never executed
    #####:  345:        continue;
    #####:  346:      sizes[i] = rewriter.getIndexAttr(dim);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  347:    }
        -:  348:
    #####:  349:    rewriter.replaceOpWithNewOp<ExtractSliceOp>(
    #####:  350:        tensorCast, tensorCast.getType().cast<RankedTensorType>(),
call    0 never executed
    #####:  351:        extractOperand.getSource(), extractOperand.getMixedOffsets(), sizes,
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  352:        extractOperand.getMixedStrides());
call    0 never executed
call    1 never executed
    #####:  353:    return success();
branch  0 never executed
branch  1 never executed
        -:  354:  }
        -:  355:};
        -:  356:
        -:  357:} // namespace
        -:  358:
function _ZN4mlir6tensor6CastOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1261 returned 100% blocks executed 100%
     1261:  359:void CastOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  360:                                         MLIRContext *context) {
     1261:  361:  results.add<ChainedTensorCast, TensorCastExtractSlice>(context);
call    0 returned 100%
     1261:  362:}
        -:  363:
        -:  364://===----------------------------------------------------------------------===//
        -:  365:// DimOp
        -:  366://===----------------------------------------------------------------------===//
        -:  367:
function _ZN4mlir6tensor5DimOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####:  368:void DimOp::getAsmResultNames(function_ref<void(Value, StringRef)> setNameFn) {
    #####:  369:  setNameFn(getResult(), "dim");
call    0 never executed
call    1 never executed
    #####:  370:}
        -:  371:
function _ZN4mlir6tensor5DimOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueEl called 2803 returned 100% blocks executed 100%
     2803:  372:void DimOp::build(OpBuilder &builder, OperationState &result, Value source,
        -:  373:                  int64_t index) {
     2803:  374:  auto loc = result.location;
     2803:  375:  Value indexValue = builder.create<arith::ConstantIndexOp>(loc, index);
call    0 returned 100%
call    1 returned 100%
     2803:  376:  build(builder, result, source, indexValue);
call    0 returned 100%
     2803:  377:}
        -:  378:
function _ZN4mlir6tensor5DimOp16getConstantIndexEv called 0 returned 0% blocks executed 0%
    #####:  379:Optional<int64_t> DimOp::getConstantIndex() {
    #####:  380:  if (auto constantOp = getIndex().getDefiningOp<arith::ConstantOp>())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  381:    return constantOp.getValue().cast<IntegerAttr>().getInt();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  382:  return {};
        -:  383:}
        -:  384:
function _ZN4mlir6tensor5DimOp18getSpeculatabilityEv called 0 returned 0% blocks executed 0%
    #####:  385:Speculation::Speculatability DimOp::getSpeculatability() {
    #####:  386:  auto constantIndex = getConstantIndex();
call    0 never executed
    #####:  387:  if (!constantIndex)
branch  0 never executed
branch  1 never executed
        -:  388:    return Speculation::NotSpeculatable;
        -:  389:
    #####:  390:  auto rankedSourceType = dyn_cast<RankedTensorType>(getSource().getType());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  391:  if (!rankedSourceType)
branch  0 never executed
branch  1 never executed
        -:  392:    return Speculation::NotSpeculatable;
        -:  393:
        -:  394:  // The verifier rejects operations that violate this assertion.
    #####:  395:  assert(constantIndex < rankedSourceType.getRank());
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  396:  return Speculation::Speculatable;
        -:  397:}
        -:  398:
function _ZN4mlir6tensor5DimOp6verifyEv called 0 returned 0% blocks executed 0%
    #####:  399:LogicalResult DimOp::verify() {
        -:  400:  // Assume unknown index to be in range.
    #####:  401:  Optional<int64_t> index = getConstantIndex();
call    0 never executed
    #####:  402:  if (!index)
branch  0 never executed
branch  1 never executed
    #####:  403:    return success();
        -:  404:
        -:  405:  // Check that constant index is not knowingly out of range.
    #####:  406:  auto type = getSource().getType();
call    0 never executed
call    1 never executed
    #####:  407:  if (auto tensorType = type.dyn_cast<RankedTensorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  408:    if (*index >= tensorType.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  409:      return emitOpError("index is out of range");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  410:  } else if (type.isa<UnrankedTensorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  411:    // Assume index to be in range.
        -:  412:  } else {
    #####:  413:    llvm_unreachable("expected operand with tensor type");
call    0 never executed
        -:  414:  }
    #####:  415:  return success();
        -:  416:}
        -:  417:
function _ZN4mlir6tensor5DimOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 2795 returned 100% blocks executed 25%
     2795:  418:OpFoldResult DimOp::fold(ArrayRef<Attribute> operands) {
        -:  419:  // All forms of folding require a known index.
     2795:  420:  auto index = operands[1].dyn_cast_or_null<IntegerAttr>();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
     2795:  421:  if (!index)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  422:    return {};
        -:  423:
        -:  424:  // Folding for unranked types (UnrankedTensorType) is not supported.
     2795:  425:  auto tensorType = getSource().getType().dyn_cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
     2795:  426:  if (!tensorType)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  427:    return {};
        -:  428:
        -:  429:  // Fold if the shape extent along the given index is known.
     2795:  430:  if (!tensorType.isDynamicDim(index.getInt())) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
     2795:  431:    Builder builder(getContext());
call    0 returned 100%
call    1 returned 100%
     2795:  432:    return builder.getIndexAttr(tensorType.getShape()[index.getInt()]);
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
call    4 returned 100%
call    5 returned 100%
        -:  433:  }
        -:  434:
    #####:  435:  Operation *definingOp = getSource().getDefiningOp();
call    0 never executed
call    1 never executed
        -:  436:
        -:  437:  // Fold dim to the operand of tensor.generate.
    #####:  438:  if (auto fromElements = dyn_cast_or_null<tensor::GenerateOp>(definingOp)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  439:    auto resultType =
    #####:  440:        fromElements.getResult().getType().cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  441:    // The case where the type encodes the size of the dimension is handled
        -:  442:    // above.
    #####:  443:    assert(ShapedType::isDynamic(resultType.getShape()[index.getInt()]));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
        -:  444:
        -:  445:    // Find the operand of the fromElements that corresponds to this index.
    #####:  446:    auto dynExtents = fromElements.getDynamicExtents().begin();
call    0 never executed
call    1 never executed
    #####:  447:    for (auto dim : resultType.getShape().take_front(index.getInt()))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  448:      if (ShapedType::isDynamic(dim))
branch  0 never executed
branch  1 never executed
    #####:  449:        dynExtents++;
        -:  450:
    #####:  451:    return Value{*dynExtents};
call    0 never executed
        -:  452:  }
        -:  453:
        -:  454:  // The size at the given index is now known to be a dynamic size.
    #####:  455:  unsigned unsignedIndex = index.getValue().getZExtValue();
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  456:
    #####:  457:  if (auto sliceOp = dyn_cast_or_null<tensor::ExtractSliceOp>(definingOp)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  458:    // Fold only for non-rank reduced ops. For the rank-reduced version, rely on
        -:  459:    // `resolve-shaped-type-result-dims` pass.
    #####:  460:    if (sliceOp.getType().getRank() == sliceOp.getSourceType().getRank() &&
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:  461:        sliceOp.isDynamicSize(unsignedIndex)) {
call    0 never executed
    #####:  462:      return {sliceOp.getDynamicSize(unsignedIndex)};
call    0 never executed
call    1 never executed
        -:  463:    }
        -:  464:  }
        -:  465:
        -:  466:  // dim(cast) -> dim
    #####:  467:  if (succeeded(foldTensorCast(*this)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  468:    return getResult();
call    0 never executed
call    1 never executed
        -:  469:
    #####:  470:  return {};
        -:  471:}
        -:  472:
        -:  473:namespace {
        -:  474:/// Fold dim of a cast into the dim of the source of the tensor cast.
        -:  475:struct DimOfCastOp : public OpRewritePattern<DimOp> {
        -:  476:  using OpRewritePattern<DimOp>::OpRewritePattern;
        -:  477:
function _ZNK12_GLOBAL__N_111DimOfCastOp15matchAndRewriteEN4mlir6tensor5DimOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  478:  LogicalResult matchAndRewrite(DimOp dimOp,
        -:  479:                                PatternRewriter &rewriter) const override {
    #####:  480:    auto castOp = dimOp.getSource().getDefiningOp<CastOp>();
call    0 never executed
call    1 never executed
    #####:  481:    if (!castOp)
branch  0 never executed
branch  1 never executed
    #####:  482:      return failure();
    #####:  483:    Value newSource = castOp.getOperand();
call    0 never executed
    #####:  484:    rewriter.replaceOpWithNewOp<DimOp>(dimOp, newSource, dimOp.getIndex());
call    0 never executed
call    1 never executed
    #####:  485:    return success();
        -:  486:  }
        -:  487:};
        -:  488:} // namespace
        -:  489:
function _ZN4mlir6tensor5DimOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1280 returned 100% blocks executed 100%
     1280:  490:void DimOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  491:                                        MLIRContext *context) {
     1280:  492:  results.add<DimOfCastOp>(context);
call    0 returned 100%
     1283:  493:}
        -:  494:
        -:  495://===----------------------------------------------------------------------===//
        -:  496:// EmptyOp
        -:  497://===----------------------------------------------------------------------===//
        -:  498:
function _ZN4mlir6tensor7EmptyOp5buildERNS_9OpBuilderERNS_14OperationStateEN4llvm8ArrayRefIlEENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  499:void EmptyOp::build(OpBuilder &builder, OperationState &result,
        -:  500:                    ArrayRef<int64_t> staticShape, Type elementType) {
    #####:  501:  assert(all_of(staticShape,
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  502:                [](int64_t sz) { return !ShapedType::isDynamic(sz); }) &&
        -:  503:         "expected only static sizes");
    #####:  504:  build(builder, result, staticShape, elementType, {});
call    0 never executed
call    1 never executed
    #####:  505:}
        -:  506:
function _ZN4mlir6tensor7EmptyOp5buildERNS_9OpBuilderERNS_14OperationStateEN4llvm8ArrayRefIlEENS_4TypeENS_10ValueRangeE called 2483 returned 100% blocks executed 100%
     2483:  507:void EmptyOp::build(OpBuilder &builder, OperationState &result,
        -:  508:                    ArrayRef<int64_t> staticShape, Type elementType,
        -:  509:                    ValueRange dynamicSizes) {
     2483:  510:  auto tensorType = RankedTensorType::get(staticShape, elementType);
call    0 returned 100%
     2483:  511:  build(builder, result, tensorType, dynamicSizes);
call    0 returned 100%
     2483:  512:}
        -:  513:
function _ZN4mlir6tensor7EmptyOp5buildERNS_9OpBuilderERNS_14OperationStateEN4llvm8ArrayRefINS_12OpFoldResultEEENS_4TypeE called 2483 returned 100% blocks executed 75%
     2483:  514:void EmptyOp::build(OpBuilder &builder, OperationState &result,
        -:  515:                    ArrayRef<OpFoldResult> sizes, Type elementType) {
     2483:  516:  SmallVector<int64_t> staticShape;
call    0 returned 100%
     2483:  517:  SmallVector<Value> dynamicSizes;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
     2483:  518:  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticShape,
call    0 returned 100%
        -:  519:                             ShapedType::kDynamicSize);
     2483:  520:  build(builder, result, staticShape, elementType, dynamicSizes);
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
     2483:  521:}
        -:  522:
function _ZN4mlir6tensor7EmptyOp6verifyEv called 249061534 returned 100% blocks executed 35%
249061534:  523:LogicalResult EmptyOp::verify() {
249061534:  524:  if (getType().getNumDynamicDims() !=
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
249059948:  525:      static_cast<int64_t>(getDynamicSizes().size()))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  526:    return emitOpError("incorrect number of dynamic sizes, has ")
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  527:           << getDynamicSizes().size() << ", expected "
call    0 never executed
call    1 never executed
    #####:  528:           << getType().getNumDynamicDims();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
249059948:  529:  return success();
        -:  530:}
        -:  531:
        -:  532:LogicalResult
function _ZN4mlir6tensor7EmptyOp17reifyResultShapesERNS_9OpBuilderERN4llvm11SmallVectorINS5_INS_5ValueELj6EEELj1EEE called 4263 returned 100% blocks executed 82%
     4263:  533:EmptyOp::reifyResultShapes(OpBuilder &builder,
        -:  534:                           ReifiedRankedShapedTypeDims &reifiedReturnShapes) {
     4263:  535:  reifiedReturnShapes.resize(1, SmallVector<Value>(getType().getRank()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
     4263:  536:  unsigned ctr = 0;
    12671:  537:  for (int64_t i = 0; i < getType().getRank(); ++i) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 66% (fallthrough)
branch  3 taken 34%
     8408:  538:    if (getType().isDynamicDim(i)) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
       35:  539:      reifiedReturnShapes[0][i] = getDynamicSizes()[ctr++];
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
        -:  540:    } else {
     8373:  541:      reifiedReturnShapes[0][i] =
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    16746:  542:          builder.create<arith::ConstantIndexOp>(getLoc(), i);
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -:  543:    }
        -:  544:  }
     4263:  545:  return success();
        -:  546:}
        -:  547:
function _ZN4mlir6tensor7EmptyOp14getDynamicSizeEj called 0 returned 0% blocks executed 0%
    #####:  548:Value EmptyOp::getDynamicSize(unsigned idx) {
    #####:  549:  assert(getType().isDynamicDim(idx) && "expected dynamic dim");
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  550:  unsigned ctr = 0;
    #####:  551:  for (int64_t i = 0; i < static_cast<int64_t>(idx); ++i)
branch  0 never executed
branch  1 never executed
    #####:  552:    if (getType().isDynamicDim(i))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  553:      ++ctr;
    #####:  554:  return getDynamicSizes()[ctr];
call    0 never executed
call    1 never executed
        -:  555:}
        -:  556:
function _ZN4mlir6tensor7EmptyOp13getMixedSizesEv called 0 returned 0% blocks executed 0%
    #####:  557:SmallVector<OpFoldResult> EmptyOp::getMixedSizes() {
    #####:  558:  SmallVector<OpFoldResult> result;
call    0 never executed
    #####:  559:  unsigned ctr = 0;
    #####:  560:  OpBuilder b(getContext());
call    0 never executed
    #####:  561:  for (int64_t i = 0; i < getType().getRank(); ++i) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  562:    if (getType().isDynamicDim(i)) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  563:      result.push_back(getDynamicSizes()[ctr++]);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  564:    } else {
    #####:  565:      result.push_back(b.getIndexAttr(getType().getShape()[i]));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
        -:  566:    }
        -:  567:  }
    #####:  568:  return result;
        -:  569:}
        -:  570:
        -:  571:namespace {
        -:  572:/// Change the type of the result of a `tensor.empty` by making the result
        -:  573:/// type statically sized along dimensions that in the original operation were
        -:  574:/// defined as dynamic, but the size was defined using a `constant` op. For
        -:  575:/// example
        -:  576:///
        -:  577:///  %c5 = arith.constant 5: index
        -:  578:///  %0 = tensor.empty(%arg0, %c5) : tensor<?x?xf32>
        -:  579:///
        -:  580:///  to
        -:  581:///
        -:  582:///  %0 = tensor.empty(%arg0) : tensor<?x5xf32>
        -:  583:struct ReplaceEmptyTensorStaticShapeDims : OpRewritePattern<EmptyOp> {
        -:  584:  using OpRewritePattern<EmptyOp>::OpRewritePattern;
        -:  585:
function _ZNK12_GLOBAL__N_133ReplaceEmptyTensorStaticShapeDims15matchAndRewriteEN4mlir6tensor7EmptyOpERNS1_15PatternRewriterE called 68845 returned 100% blocks executed 92%
    68845:  586:  LogicalResult matchAndRewrite(EmptyOp op,
        -:  587:                                PatternRewriter &rewriter) const override {
    68845:  588:    SmallVector<int64_t> staticShape(op.getType().getShape().begin(),
call    0 returned 100%
call    1 returned 100%
   137690:  589:                                     op.getType().getShape().end());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
   137690:  590:    SmallVector<Value> dynamicSizes;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  591:
        -:  592:    // Compute new static and dynamic sizes.
    68845:  593:    unsigned ctr = 0;
    68845:  594:    bool changedType = false;
   198952:  595:    for (int64_t i = 0; i < op.getType().getRank(); ++i) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 65% (fallthrough)
branch  3 taken 35%
   130107:  596:      if (op.getType().isDynamicDim(i)) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
       87:  597:        Value dynamicSize = op.getDynamicSizes()[ctr++];
call    0 returned 100%
call    1 returned 100%
       87:  598:        Optional<int64_t> cst = getConstantIntValue(dynamicSize);
call    0 returned 100%
call    1 returned 100%
       87:  599:        if (cst.has_value()) {
branch  0 taken 47% (fallthrough)
branch  1 taken 53%
       41:  600:          staticShape[i] = *cst;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
       41:  601:          changedType = true;
        -:  602:        } else {
       46:  603:          dynamicSizes.push_back(dynamicSize);
call    0 returned 100%
        -:  604:        }
        -:  605:      }
        -:  606:    }
        -:  607:
        -:  608:    // Stop here if no dynamic size was promoted to static.
    68845:  609:    if (!changedType)
branch  0 taken 100% (fallthrough)
branch  1 taken 1%
    68809:  610:      return failure();
        -:  611:
       36:  612:    auto tensorType = RankedTensorType::get(
       36:  613:        staticShape, op.getType().getElementType(), op.getType().getEncoding());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
       36:  614:    auto newOp =
       36:  615:        rewriter.create<EmptyOp>(op.getLoc(), tensorType, dynamicSizes);
call    0 returned 100%
       36:  616:    rewriter.replaceOpWithNewOp<tensor::CastOp>(op, op.getType(), newOp);
call    0 returned 100%
call    1 returned 100%
    68845:  617:    return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  618:  }
        -:  619:};
        -:  620:
        -:  621:/// `tensor.empty` does not define any tensor contents, so a slice of a
        -:  622:/// `tensor.empty` can be canonicalized to a smaller `tensor.empty`.
        -:  623:struct FoldEmptyTensorWithExtractSliceOp
        -:  624:    : public OpRewritePattern<ExtractSliceOp> {
        -:  625:  using OpRewritePattern<ExtractSliceOp>::OpRewritePattern;
        -:  626:
function _ZNK12_GLOBAL__N_133FoldEmptyTensorWithExtractSliceOp15matchAndRewriteEN4mlir6tensor14ExtractSliceOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  627:  LogicalResult matchAndRewrite(ExtractSliceOp sliceOp,
        -:  628:                                PatternRewriter &rewriter) const override {
    #####:  629:    if (!sliceOp.getSource().getDefiningOp<EmptyOp>())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  630:      return failure();
        -:  631:
        -:  632:    // ExtractSliceOp may be rank-reducing; its dynamic sizes must be
        -:  633:    // preserved as well as its result type.
    #####:  634:    auto tensorType = RankedTensorType::get(sliceOp.getType().getShape(),
call    0 never executed
    #####:  635:                                            sliceOp.getType().getElementType(),
call    0 never executed
    #####:  636:                                            sliceOp.getType().getEncoding());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  637:    rewriter.replaceOpWithNewOp<EmptyOp>(sliceOp, tensorType,
    #####:  638:                                         sliceOp.getSizes());
call    0 never executed
call    1 never executed
    #####:  639:    return success();
        -:  640:  }
        -:  641:};
        -:  642:
        -:  643:template <typename ReshapeOp>
        -:  644:struct FoldEmptyTensorWithReshapeOp : public OpRewritePattern<ReshapeOp> {
        -:  645:  using OpRewritePattern<ReshapeOp>::OpRewritePattern;
        -:  646:
     2619:  647:  LogicalResult matchAndRewrite(ReshapeOp reshapeOp,
        -:  648:                                PatternRewriter &rewriter) const override {
     2619:  649:    if (!reshapeOp.getSrc().template getDefiningOp<EmptyOp>())
     2619:  650:      return failure();
     2483:  651:    Location loc = reshapeOp.getLoc();
     4966:  652:    ReifiedRankedShapedTypeDims resultShapes;
        -:  653:    ReifyRankedShapedTypeOpInterface reifyShapedTypeInterface =
     2483:  654:        cast<ReifyRankedShapedTypeOpInterface>(reshapeOp.getOperation());
     2483:  655:    if (failed(reifyShapedTypeInterface.reifyResultShapes(rewriter,
     2483:  656:                                                          resultShapes)) ||
    #####:  657:        !llvm::hasSingleElement(resultShapes))
     2483:  658:      return failure();
        -:  659:    // TODO: Do not drop tensor type encoding.
     4966:  660:    Value emptyTensor =
     2483:  661:        rewriter.create<EmptyOp>(loc, getAsOpFoldResult(resultShapes[0]),
     2483:  662:                                 reshapeOp.getResultType().getElementType());
     2483:  663:    if (emptyTensor.getType() != reshapeOp.getResultType()) {
    #####:  664:      rewriter.replaceOpWithNewOp<tensor::CastOp>(
        -:  665:          reshapeOp, reshapeOp.getResultType(), emptyTensor);
        -:  666:    } else {
     2483:  667:      rewriter.replaceOp(reshapeOp, emptyTensor);
        -:  668:    }
     2483:  669:    return success();
        -:  670:  }
------------------
_ZNK12_GLOBAL__N_128FoldEmptyTensorWithReshapeOpIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_128FoldEmptyTensorWithReshapeOpIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 1400 returned 100% blocks executed 83%
     1400:  647:  LogicalResult matchAndRewrite(ReshapeOp reshapeOp,
        -:  648:                                PatternRewriter &rewriter) const override {
     1400:  649:    if (!reshapeOp.getSrc().template getDefiningOp<EmptyOp>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 5% (fallthrough)
branch  3 taken 95%
     1400:  650:      return failure();
     1325:  651:    Location loc = reshapeOp.getLoc();
call    0 returned 100%
     2650:  652:    ReifiedRankedShapedTypeDims resultShapes;
        -:  653:    ReifyRankedShapedTypeOpInterface reifyShapedTypeInterface =
     1325:  654:        cast<ReifyRankedShapedTypeOpInterface>(reshapeOp.getOperation());
call    0 returned 100%
     1325:  655:    if (failed(reifyShapedTypeInterface.reifyResultShapes(rewriter,
call    0 returned 100%
     1325:  656:                                                          resultShapes)) ||
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    #####:  657:        !llvm::hasSingleElement(resultShapes))
     1325:  658:      return failure();
        -:  659:    // TODO: Do not drop tensor type encoding.
     2650:  660:    Value emptyTensor =
call    0 returned 100%
branch  1 taken 0%
branch  2 taken 100%
call    3 returned 100%
call    4 returned 100%
branch  5 taken 2% (fallthrough)
branch  6 taken 98%
     1325:  661:        rewriter.create<EmptyOp>(loc, getAsOpFoldResult(resultShapes[0]),
call    0 returned 100%
     1325:  662:                                 reshapeOp.getResultType().getElementType());
call    0 returned 100%
     1325:  663:    if (emptyTensor.getType() != reshapeOp.getResultType()) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  664:      rewriter.replaceOpWithNewOp<tensor::CastOp>(
call    0 never executed
call    1 never executed
        -:  665:          reshapeOp, reshapeOp.getResultType(), emptyTensor);
        -:  666:    } else {
     1325:  667:      rewriter.replaceOp(reshapeOp, emptyTensor);
call    0 returned 100%
call    1 returned 100%
        -:  668:    }
     1325:  669:    return success();
call    0 returned 100%
        -:  670:  }
------------------
_ZNK12_GLOBAL__N_128FoldEmptyTensorWithReshapeOpIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_128FoldEmptyTensorWithReshapeOpIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 1219 returned 100% blocks executed 80%
     1219:  647:  LogicalResult matchAndRewrite(ReshapeOp reshapeOp,
        -:  648:                                PatternRewriter &rewriter) const override {
     1219:  649:    if (!reshapeOp.getSrc().template getDefiningOp<EmptyOp>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 5% (fallthrough)
branch  3 taken 95%
     1219:  650:      return failure();
     1158:  651:    Location loc = reshapeOp.getLoc();
call    0 returned 100%
     2316:  652:    ReifiedRankedShapedTypeDims resultShapes;
        -:  653:    ReifyRankedShapedTypeOpInterface reifyShapedTypeInterface =
     1158:  654:        cast<ReifyRankedShapedTypeOpInterface>(reshapeOp.getOperation());
call    0 returned 100%
     1158:  655:    if (failed(reifyShapedTypeInterface.reifyResultShapes(rewriter,
call    0 returned 100%
     1158:  656:                                                          resultShapes)) ||
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    #####:  657:        !llvm::hasSingleElement(resultShapes))
     1158:  658:      return failure();
        -:  659:    // TODO: Do not drop tensor type encoding.
     2316:  660:    Value emptyTensor =
call    0 returned 100%
branch  1 taken 0%
branch  2 taken 100%
call    3 returned 100%
call    4 returned 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
     1158:  661:        rewriter.create<EmptyOp>(loc, getAsOpFoldResult(resultShapes[0]),
call    0 returned 100%
     1158:  662:                                 reshapeOp.getResultType().getElementType());
call    0 returned 100%
     1158:  663:    if (emptyTensor.getType() != reshapeOp.getResultType()) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  664:      rewriter.replaceOpWithNewOp<tensor::CastOp>(
call    0 never executed
call    1 never executed
        -:  665:          reshapeOp, reshapeOp.getResultType(), emptyTensor);
        -:  666:    } else {
     1158:  667:      rewriter.replaceOp(reshapeOp, emptyTensor);
call    0 returned 100%
call    1 returned 100%
        -:  668:    }
     1158:  669:    return success();
call    0 returned 100%
        -:  670:  }
------------------
        -:  671:};
        -:  672:
        -:  673:struct FoldEmptyTensorWithDimOp : public OpRewritePattern<DimOp> {
        -:  674:  using OpRewritePattern<DimOp>::OpRewritePattern;
        -:  675:
function _ZNK12_GLOBAL__N_124FoldEmptyTensorWithDimOp15matchAndRewriteEN4mlir6tensor5DimOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  676:  LogicalResult matchAndRewrite(tensor::DimOp dimOp,
        -:  677:                                PatternRewriter &rewriter) const override {
    #####:  678:    Optional<int64_t> maybeConstantIndex = dimOp.getConstantIndex();
call    0 never executed
    #####:  679:    auto emptyTensorOp = dimOp.getSource().getDefiningOp<EmptyOp>();
call    0 never executed
call    1 never executed
    #####:  680:    if (!emptyTensorOp || !maybeConstantIndex)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  681:      return failure();
    #####:  682:    if (!emptyTensorOp.getType().isDynamicDim(*maybeConstantIndex))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  683:      return failure();
    #####:  684:    rewriter.replaceOp(dimOp,
call    0 never executed
    #####:  685:                       emptyTensorOp.getDynamicSize(*maybeConstantIndex));
call    0 never executed
call    1 never executed
    #####:  686:    return success();
        -:  687:  }
        -:  688:};
        -:  689:
        -:  690:/// Canonicalize
        -:  691:///
        -:  692:/// ```mlir
        -:  693:///   %0 = tensor.empty(%d0, %d1) : tensor<?x?xf32>
        -:  694:///   %1 = tensor.cast %0 : tensor<?x?xf32> to tensor<4x?xf32>
        -:  695:/// ```
        -:  696:///
        -:  697:/// into
        -:  698:///
        -:  699:/// ```mlir
        -:  700:///   %0 = tensor.empty(%d1) : tensor<4x?xf32>
        -:  701:/// ```
        -:  702:///
        -:  703:/// This assumes the input program is correct in terms of its shape. So it is
        -:  704:/// safe to assume that `%d0` is in fact 4.
        -:  705:struct FoldEmptyTensorWithCastOp : public OpRewritePattern<CastOp> {
        -:  706:  using OpRewritePattern<CastOp>::OpRewritePattern;
        -:  707:
function _ZNK12_GLOBAL__N_125FoldEmptyTensorWithCastOp15matchAndRewriteEN4mlir6tensor6CastOpERNS1_15PatternRewriterE called 2091 returned 100% blocks executed 9%
     2091:  708:  LogicalResult matchAndRewrite(CastOp castOp,
        -:  709:                                PatternRewriter &rewriter) const override {
     2091:  710:    if (!canFoldIntoProducerOp(castOp))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
     2091:  711:      return failure();
    #####:  712:    auto producer = castOp.getSource().getDefiningOp<EmptyOp>();
call    0 never executed
call    1 never executed
    #####:  713:    if (!producer)
branch  0 never executed
branch  1 never executed
    #####:  714:      return failure();
        -:  715:
    #####:  716:    auto resultType = castOp->getResult(0).getType().cast<RankedTensorType>();
call    0 never executed
    #####:  717:    ArrayRef<int64_t> resultShape = resultType.getShape();
call    0 never executed
    #####:  718:    SmallVector<OpFoldResult> currMixedSizes = producer.getMixedSizes();
call    0 never executed
    #####:  719:    SmallVector<OpFoldResult> newMixedSizes;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  720:    newMixedSizes.reserve(currMixedSizes.size());
branch  0 never executed
branch  1 never executed
    #####:  721:    assert(resultShape.size() == currMixedSizes.size() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  722:           "mismatch in result shape and sizes of empty op");
    #####:  723:    for (auto it : llvm::zip(resultShape, currMixedSizes)) {
branch  0 never executed
branch  1 never executed
    #####:  724:      int64_t newDim = std::get<0>(it);
call    0 never executed
    #####:  725:      OpFoldResult currDim = std::get<1>(it);
call    0 never executed
        -:  726:      // Case 1: The empty tensor dim is static. Check that the tensor cast
        -:  727:      // result dim matches.
    #####:  728:      if (auto attr = currDim.dyn_cast<Attribute>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  729:        if (ShapedType::isDynamic(newDim) ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  730:            newDim != attr.cast<IntegerAttr>().getInt()) {
call    0 never executed
        -:  731:          // Something is off, the cast result shape cannot be more dynamic
        -:  732:          // than the empty tensor result shape (enforced by
        -:  733:          // `canFoldIntoProducer`). Abort for now.
    #####:  734:          return rewriter.notifyMatchFailure(
        -:  735:              producer, "mismatch in static value of shape of empty tensor "
    #####:  736:                        "result and cast result");
call    0 never executed
        -:  737:        }
    #####:  738:        newMixedSizes.push_back(attr);
call    0 never executed
call    1 never executed
    #####:  739:        continue;
        -:  740:      }
        -:  741:
        -:  742:      // Case 2 : The tensor cast shape is static, but empty tensor result
        -:  743:      // shape is dynamic.
    #####:  744:      if (!ShapedType::isDynamic(newDim)) {
branch  0 never executed
branch  1 never executed
    #####:  745:        newMixedSizes.push_back(rewriter.getIndexAttr(newDim));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  746:        continue;
        -:  747:      }
        -:  748:
        -:  749:      // Case 3 : The tensor cast shape is dynamic and empty tensor result
        -:  750:      // shape is dynamic. Use the dynamic value from the empty tensor op.
    #####:  751:      newMixedSizes.push_back(currDim);
call    0 never executed
        -:  752:    }
        -:  753:
        -:  754:    // TODO: Do not drop tensor encoding.
    #####:  755:    rewriter.replaceOpWithNewOp<EmptyOp>(castOp, newMixedSizes,
    #####:  756:                                         resultType.getElementType());
call    0 never executed
call    1 never executed
    #####:  757:    return success();
branch  0 never executed
branch  1 never executed
        -:  758:  }
        -:  759:};
        -:  760:
        -:  761:} // namespace
        -:  762:
function _ZN4mlir6tensor7EmptyOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1670 returned 100% blocks executed 100%
     1670:  763:void EmptyOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  764:                                          MLIRContext *context) {
     1670:  765:  results.add<FoldEmptyTensorWithCastOp, FoldEmptyTensorWithDimOp,
        -:  766:              FoldEmptyTensorWithExtractSliceOp,
        -:  767:              FoldEmptyTensorWithReshapeOp<tensor::ExpandShapeOp>,
        -:  768:              FoldEmptyTensorWithReshapeOp<tensor::CollapseShapeOp>,
     1670:  769:              ReplaceEmptyTensorStaticShapeDims>(context);
call    0 returned 100%
     1670:  770:}
        -:  771:
        -:  772://===----------------------------------------------------------------------===//
        -:  773:// ExtractOp
        -:  774://===----------------------------------------------------------------------===//
        -:  775:
        -:  776:namespace {
        -:  777:
        -:  778:/// Canonicalizes the pattern of the form
        -:  779:///
        -:  780:/// %val = tensor.cast %source : : tensor<?xi32> to tensor<2xi32>
        -:  781:/// %extracted_element = tensor.extract %val[%c0] : tensor<2xi32>
        -:  782:///
        -:  783:/// to
        -:  784:///
        -:  785:/// %extracted_element = tensor.extract %source[%c0] : tensor<?xi32>
        -:  786:struct ExtractFromTensorCast : public OpRewritePattern<tensor::ExtractOp> {
        -:  787:  using OpRewritePattern<tensor::ExtractOp>::OpRewritePattern;
        -:  788:
function _ZNK12_GLOBAL__N_121ExtractFromTensorCast15matchAndRewriteEN4mlir6tensor9ExtractOpERNS1_15PatternRewriterE called 5180 returned 100% blocks executed 93%
     5180:  789:  LogicalResult matchAndRewrite(tensor::ExtractOp extract,
        -:  790:                                PatternRewriter &rewriter) const final {
     5180:  791:    auto tensorCast = extract.getTensor().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
call    1 returned 100%
     5180:  792:    if (!tensorCast)
branch  0 taken 98% (fallthrough)
branch  1 taken 2%
     5088:  793:      return failure();
       92:  794:    if (!tensorCast.getSource().getType().isa<RankedTensorType>())
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####:  795:      return failure();
       92:  796:    rewriter.replaceOpWithNewOp<tensor::ExtractOp>(
       92:  797:        extract, tensorCast.getSource(), extract.getIndices());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
       92:  798:    return success();
        -:  799:  }
        -:  800:};
        -:  801:
        -:  802:} // namespace
        -:  803:
function _ZN4mlir6tensor9ExtractOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 44912262 returned 100% blocks executed 100%
 44912262:  804:void ExtractOp::getAsmResultNames(
        -:  805:    function_ref<void(Value, StringRef)> setNameFn) {
 44912262:  806:  setNameFn(getResult(), "extracted");
call    0 returned 100%
call    1 returned 100%
 44912262:  807:}
        -:  808:
function _ZN4mlir6tensor9ExtractOp6verifyEv called 47248377 returned 100% blocks executed 62%
 47248377:  809:LogicalResult ExtractOp::verify() {
        -:  810:  // Verify the # indices match if we have a ranked type.
 47248377:  811:  auto tensorType = getTensor().getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
 47248377:  812:  if (tensorType.getRank() != static_cast<int64_t>(getIndices().size()))
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####:  813:    return emitOpError("incorrect number of indices for extract_element");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
 47248377:  814:  return success();
        -:  815:}
        -:  816:
function _ZN4mlir6tensor9ExtractOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 293120 returned 100% blocks executed 92%
   293120:  817:OpFoldResult ExtractOp::fold(ArrayRef<Attribute> operands) {
        -:  818:  // If this is a splat elements attribute, simply return the value. All of
        -:  819:  // the elements of a splat attribute are the same.
   293120:  820:  if (Attribute tensor = operands.front())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
      980:  821:    if (auto splatTensor = tensor.dyn_cast<SplatElementsAttr>())
call    0 returned 100%
branch  1 taken 91% (fallthrough)
branch  2 taken 9%
      891:  822:      return splatTensor.getSplatValue<Attribute>();
call    0 returned 100%
call    1 returned 100%
        -:  823:
        -:  824:  // Collect the constant indices into the tensor.
   292229:  825:  SmallVector<uint64_t, 8> indices;
   834966:  826:  for (Attribute indice : llvm::drop_begin(operands, 1)) {
branch  0 taken 65% (fallthrough)
branch  1 taken 35%
  1085720:  827:    if (!indice || !indice.isa<IntegerAttr>())
branch  0 taken 100% (fallthrough)
branch  1 taken 1%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
      246:  828:      return {};
   542737:  829:    indices.push_back(indice.cast<IntegerAttr>().getInt());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -:  830:  }
        -:  831:
        -:  832:  // Fold extract(from_elements(...)).
   291983:  833:  if (auto fromElementsOp = getTensor().getDefiningOp<FromElementsOp>()) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 1% (fallthrough)
branch  3 taken 99%
     3401:  834:    auto tensorType = fromElementsOp.getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
     3401:  835:    auto rank = tensorType.getRank();
call    0 returned 100%
    3401*:  836:    assert(static_cast<int64_t>(indices.size()) == tensorType.getRank() &&
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
        -:  837:           "rank mismatch");
     3401:  838:    int flatIndex = 0;
     3401:  839:    int stride = 1;
     9830:  840:    for (int i = rank - 1; i >= 0; --i) {
branch  0 taken 65% (fallthrough)
branch  1 taken 35%
     6429:  841:      if (i < rank - 1)
branch  0 taken 47% (fallthrough)
branch  1 taken 53%
     3028:  842:        stride *= tensorType.getDimSize(i);
call    0 returned 100%
     6429:  843:      flatIndex += indices[i] * stride;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  844:    }
        -:  845:    // Prevent out of bounds accesses. This can happen in invalid code that
        -:  846:    // will never execute.
     3401:  847:    if (static_cast<int>(fromElementsOp.getElements().size()) <= flatIndex ||
call    0 returned 100%
branch  1 taken 95% (fallthrough)
branch  2 taken 5%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
        -:  848:        flatIndex < 0)
      162:  849:      return {};
     3239:  850:    return fromElementsOp.getElements()[flatIndex];
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -:  851:  }
        -:  852:
        -:  853:  // If this is an elements attribute, query the value at the given indices.
   288582:  854:  if (Attribute tensor = operands.front()) {
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
       87:  855:    auto elementsAttr = tensor.dyn_cast<ElementsAttr>();
call    0 returned 100%
       87:  856:    if (elementsAttr && elementsAttr.isValidIndex(indices))
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
      174:  857:      return elementsAttr.getValues<Attribute>()[indices];
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -:  858:  }
        -:  859:
   288495:  860:  return {};
        -:  861:}
        -:  862:
function _ZN4mlir6tensor9ExtractOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235:  863:void ExtractOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  864:                                            MLIRContext *context) {
     1235:  865:  results.add<ExtractFromTensorCast>(context);
call    0 returned 100%
     1235:  866:}
        -:  867:
        -:  868://===----------------------------------------------------------------------===//
        -:  869:// FromElementsOp
        -:  870://===----------------------------------------------------------------------===//
        -:  871:
function _ZN4mlir6tensor14FromElementsOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 55769682 returned 100% blocks executed 100%
 55769682:  872:void FromElementsOp::getAsmResultNames(
        -:  873:    function_ref<void(Value, StringRef)> setNameFn) {
 55769682:  874:  setNameFn(getResult(), "from_elements");
call    0 returned 100%
call    1 returned 100%
 55769682:  875:}
        -:  876:
function _ZN4mlir6tensor14FromElementsOp5buildERNS_9OpBuilderERNS_14OperationStateENS_4TypeENS_10ValueRangeE called 0 returned 0% blocks executed 0%
       5*:  877:void FromElementsOp::build(OpBuilder &builder, OperationState &result,
        -:  878:                           Type resultType, ValueRange elements) {
       5*:  879:  result.addOperands(elements);
call    0 returned 100%
call    1 never executed
       5*:  880:  result.addTypes(resultType);
call    0 returned 100%
call    1 returned 100%
call    2 never executed
call    3 never executed
    #####:  881:}
        -:  882:
function _ZN4mlir6tensor14FromElementsOp5buildERNS_9OpBuilderERNS_14OperationStateENS_10ValueRangeE called 0 returned 0% blocks executed 0%
    #####:  883:void FromElementsOp::build(OpBuilder &builder, OperationState &result,
        -:  884:                           ValueRange elements) {
    #####:  885:  assert(!elements.empty() && "expected at least one element");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  886:  Type resultType = RankedTensorType::get(
    #####:  887:      {static_cast<int64_t>(elements.size())}, elements.front().getType());
call    0 never executed
call    1 never executed
    #####:  888:  build(builder, result, resultType, elements);
call    0 never executed
    #####:  889:}
        -:  890:
function _ZN4mlir6tensor14FromElementsOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 257331 returned 100% blocks executed 100%
   257331:  891:OpFoldResult FromElementsOp::fold(ArrayRef<Attribute> operands) {
   257331:  892:  if (!llvm::is_contained(operands, nullptr))
branch  0 taken 62% (fallthrough)
branch  1 taken 38%
   160163:  893:    return DenseElementsAttr::get(getType(), operands);
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
    97168:  894:  return {};
        -:  895:}
        -:  896:
        -:  897:namespace {
        -:  898:
        -:  899:// Pushes the index_casts that occur before extractions to after the extract.
        -:  900:// This minimizes type conversion in some cases and enables the extract
        -:  901:// canonicalizer. This changes:
        -:  902://
        -:  903:// %cast = arith.index_cast %tensor : tensor<1xi32> to tensor<1xindex>
        -:  904:// %extract = tensor.extract %cast[%index] : tensor<1xindex>
        -:  905://
        -:  906:// to the following:
        -:  907://
        -:  908:// %extract = tensor.extract %tensor[%index] : tensor<1xindex>
        -:  909:// %cast = arith.index_cast %extract : i32 to index
        -:  910://
        -:  911:// to just %element.
        -:  912://
        -:  913:// Consider expanding this to a template and handle all tensor cast
        -:  914:// operations.
        -:  915:struct ExtractElementFromIndexCast
        -:  916:    : public OpRewritePattern<tensor::ExtractOp> {
        -:  917:  using OpRewritePattern<tensor::ExtractOp>::OpRewritePattern;
        -:  918:
function _ZNK12_GLOBAL__N_127ExtractElementFromIndexCast15matchAndRewriteEN4mlir6tensor9ExtractOpERNS1_15PatternRewriterE called 5180 returned 100% blocks executed 42%
     5180:  919:  LogicalResult matchAndRewrite(tensor::ExtractOp extract,
        -:  920:                                PatternRewriter &rewriter) const final {
     5180:  921:    Location loc = extract.getLoc();
call    0 returned 100%
     5180:  922:    auto indexCast = extract.getTensor().getDefiningOp<arith::IndexCastOp>();
call    0 returned 100%
call    1 returned 100%
     5180:  923:    if (!indexCast)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     5180:  924:      return failure();
        -:  925:
    #####:  926:    Type elementTy = getElementTypeOrSelf(indexCast.getIn());
call    0 never executed
call    1 never executed
        -:  927:
    #####:  928:    auto newExtract = rewriter.create<tensor::ExtractOp>(
    #####:  929:        loc, elementTy, indexCast.getIn(), extract.getIndices());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  930:
    #####:  931:    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(extract, extract.getType(),
    #####:  932:                                                    newExtract);
call    0 never executed
        -:  933:
    #####:  934:    return success();
        -:  935:  }
        -:  936:};
        -:  937:
        -:  938:} // namespace
        -:  939:
function _ZN4mlir6tensor14FromElementsOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235:  940:void FromElementsOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  941:                                                 MLIRContext *context) {
     1235:  942:  results.add<ExtractElementFromIndexCast>(context);
call    0 returned 100%
     1235:  943:}
        -:  944:
        -:  945://===----------------------------------------------------------------------===//
        -:  946:// GatherOp
        -:  947://===----------------------------------------------------------------------===//
        -:  948:
function _ZN4mlir6tensor8GatherOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####:  949:void GatherOp::getAsmResultNames(
        -:  950:    function_ref<void(Value, StringRef)> setNameFn) {
    #####:  951:  setNameFn(getResult(), "gather");
call    0 never executed
call    1 never executed
    #####:  952:}
        -:  953:
        -:  954:/// Return the inferred result type for a gatherOp where:
        -:  955:///   - sourceType is the type of the source tensor gathered from
        -:  956:///   - indicesType is the type of the indices used to gather
        -:  957:///   - gatherDims are the dims along which the gather occurs.
        -:  958:/// Return a full rank or ranked-reduced variant of the type depending on
        -:  959:/// the value of rankReduced.
        -:  960:///
        -:  961:/// The leading dimensions of the index tensor give the result tensor its
        -:  962:/// leading dimensions.
        -:  963:/// The trailing dimensions of the result tensor are obtained from the source
        -:  964:/// tensor by setting the dimensions specified in gather_dims to `1` (if
        -:  965:/// rankedReduced is false), or skipping them (otherwise).
function _ZN4mlir6tensor8GatherOp15inferResultTypeENS_16RankedTensorTypeES2_N4llvm8ArrayRefIlEEb called 0 returned 0% blocks executed 0%
    #####:  966:RankedTensorType GatherOp::inferResultType(RankedTensorType sourceType,
        -:  967:                                           RankedTensorType indicesType,
        -:  968:                                           ArrayRef<int64_t> gatherDims,
        -:  969:                                           bool rankReduced) {
    #####:  970:  SmallVector<int64_t> resultShape(indicesType.getShape().drop_back());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  971:  resultShape.reserve(resultShape.size() + sourceType.getRank());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  972:  for (int64_t idx : llvm::seq<int64_t>(0, sourceType.getRank())) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  973:    if (std::binary_search(gatherDims.begin(), gatherDims.end(), idx)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  974:      if (!rankReduced)
branch  0 never executed
branch  1 never executed
    #####:  975:        resultShape.push_back(1);
call    0 never executed
    #####:  976:      continue;
        -:  977:    }
    #####:  978:    resultShape.push_back(sourceType.getDimSize(idx));
call    0 never executed
call    1 never executed
        -:  979:  }
    #####:  980:  return RankedTensorType::Builder(sourceType).setShape(resultShape);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
        -:  981:}
        -:  982:
        -:  983:static LogicalResult
function _ZL25verifyGatherOrScatterDimsPN4mlir9OperationEN4llvm8ArrayRefIlEElNS2_9StringRefES5_ called 0 returned 0% blocks executed 0%
    #####:  984:verifyGatherOrScatterDims(Operation *op, ArrayRef<int64_t> dims, int64_t rank,
        -:  985:                          StringRef gatherOrScatter, StringRef sourceOrDest) {
    #####:  986:  if (dims.empty())
branch  0 never executed
branch  1 never executed
    #####:  987:    return op->emitOpError(gatherOrScatter) << "_dims must be non-empty";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -:  988:
    #####:  989:  int64_t numGatherDims = dims.size();
branch  0 never executed
branch  1 never executed
    #####:  990:  if (numGatherDims > rank)
branch  0 never executed
branch  1 never executed
    #####:  991:    return op->emitOpError(gatherOrScatter)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  992:           << "_dims overflow " << sourceOrDest << " rank";
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  993:  for (int64_t val : dims) {
branch  0 never executed
branch  1 never executed
    #####:  994:    if (val < 0)
branch  0 never executed
branch  1 never executed
    #####:  995:      return op->emitOpError(gatherOrScatter)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  996:             << "_dims value must be non-negative";
call    0 never executed
    #####:  997:    if (val >= rank)
branch  0 never executed
branch  1 never executed
    #####:  998:      return op->emitOpError(gatherOrScatter)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  999:             << "_dims value must be smaller than " << sourceOrDest << " rank";
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1000:  }
    #####: 1001:  for (int64_t i = 1; i < numGatherDims; ++i) {
branch  0 never executed
branch  1 never executed
    #####: 1002:    if (dims[i - 1] >= dims[i])
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1003:      return op->emitOpError(gatherOrScatter)
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 1004:             << "_dims values must be strictly increasing";
call    0 never executed
        -: 1005:  }
    #####: 1006:  return success();
        -: 1007:}
        -: 1008:
function _ZN4mlir6tensor8GatherOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 1009:LogicalResult GatherOp::verify() {
    #####: 1010:  int64_t sourceRank = getSourceType().getRank();
call    0 never executed
call    1 never executed
    #####: 1011:  ArrayRef<int64_t> gatherDims = getGatherDims();
call    0 never executed
    #####: 1012:  if (failed(verifyGatherOrScatterDims(getOperation(), gatherDims, sourceRank,
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1013:                                       "gather", "source")))
    #####: 1014:    return failure();
        -: 1015:
    #####: 1016:  RankedTensorType expectedResultType = GatherOp::inferResultType(
    #####: 1017:      getSourceType(), getIndicesType(), gatherDims, /*rankReduced=*/false);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1018:  RankedTensorType expectedRankReducedResultType = GatherOp::inferResultType(
    #####: 1019:      getSourceType(), getIndicesType(), gatherDims, /*rankReduced=*/true);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1020:  if (getResultType() != expectedResultType &&
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1021:      getResultType() != expectedRankReducedResultType) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1022:    return emitOpError("result type "
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1023:                       "mismatch: "
        -: 1024:                       "expected ")
    #####: 1025:           << expectedResultType << " or its rank-reduced variant "
call    0 never executed
call    1 never executed
    #####: 1026:           << expectedRankReducedResultType << " (got: " << getResultType()
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1027:           << ")";
call    0 never executed
        -: 1028:  }
        -: 1029:
    #####: 1030:  return success();
        -: 1031:}
        -: 1032:
        -: 1033://===----------------------------------------------------------------------===//
        -: 1034:// InsertOp
        -: 1035://===----------------------------------------------------------------------===//
        -: 1036:
function _ZN4mlir6tensor8InsertOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 39948941 returned 100% blocks executed 100%
 39948941: 1037:void InsertOp::getAsmResultNames(
        -: 1038:    function_ref<void(Value, StringRef)> setNameFn) {
 39948941: 1039:  setNameFn(getResult(), "inserted");
call    0 returned 100%
call    1 returned 100%
 39948941: 1040:}
        -: 1041:
function _ZN4mlir6tensor8InsertOp6verifyEv called 41799689 returned 100% blocks executed 62%
 41799689: 1042:LogicalResult InsertOp::verify() {
        -: 1043:  // Verify the # indices match if we have a ranked type.
 41799689: 1044:  auto destType = getDest().getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
 41799689: 1045:  if (destType.getRank() != static_cast<int64_t>(getIndices().size()))
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####: 1046:    return emitOpError("incorrect number of indices");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
 41799689: 1047:  return success();
        -: 1048:}
        -: 1049:
function _ZN4mlir6tensor8InsertOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 174806 returned 100% blocks executed 87%
   174806: 1050:OpFoldResult InsertOp::fold(ArrayRef<Attribute> operands) {
   174806: 1051:  Attribute scalar = operands[0];
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   174806: 1052:  Attribute dest = operands[1];
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   174806: 1053:  if (scalar && dest)
branch  0 taken 85% (fallthrough)
branch  1 taken 15%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
      169: 1054:    if (auto splatDest = dest.dyn_cast<SplatElementsAttr>())
call    0 returned 100%
branch  1 taken 73% (fallthrough)
branch  2 taken 27%
      123: 1055:      if (scalar == splatDest.getSplatValue<Attribute>())
call    0 returned 100%
branch  1 taken 33% (fallthrough)
branch  2 taken 67%
       41: 1056:        return dest;
call    0 returned 100%
   174765: 1057:  return {};
        -: 1058:}
        -: 1059:
        -: 1060://===----------------------------------------------------------------------===//
        -: 1061:// GenerateOp
        -: 1062://===----------------------------------------------------------------------===//
        -: 1063:
function _ZN4mlir6tensor10GenerateOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 19997693 returned 100% blocks executed 100%
 19997693: 1064:void GenerateOp::getAsmResultNames(
        -: 1065:    function_ref<void(Value, StringRef)> setNameFn) {
 19997693: 1066:  setNameFn(getResult(), "generated");
call    0 returned 100%
call    1 returned 100%
 19997693: 1067:}
        -: 1068:
function _ZN4mlir6tensor10GenerateOp17reifyResultShapesERNS_9OpBuilderERN4llvm11SmallVectorINS5_INS_5ValueELj6EEELj1EEE called 2260 returned 100% blocks executed 84%
     2260: 1069:LogicalResult GenerateOp::reifyResultShapes(
        -: 1070:    OpBuilder &builder, ReifiedRankedShapedTypeDims &reifiedReturnShapes) {
     2260: 1071:  reifiedReturnShapes.resize(1, SmallVector<Value>(getType().getRank()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
     2260: 1072:  int idx = 0;
    10788: 1073:  for (auto dim : llvm::seq<int64_t>(0, getType().getRank())) {
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 65% (fallthrough)
branch  4 taken 35%
call    5 returned 100%
     4264: 1074:    if (getType().isDynamicDim(dim)) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 76% (fallthrough)
branch  3 taken 24%
     3252: 1075:      reifiedReturnShapes[0][dim] = getOperand(idx++);
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
        -: 1076:    } else {
     2024: 1077:      reifiedReturnShapes[0][dim] = builder.create<arith::ConstantIndexOp>(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
     2024: 1078:          getLoc(), getType().getDimSize(dim));
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -: 1079:    }
        -: 1080:  }
     2260: 1081:  return success();
        -: 1082:}
        -: 1083:
function _ZN4mlir6tensor10GenerateOp6verifyEv called 21238641 returned 100% blocks executed 58%
 21238641: 1084:LogicalResult GenerateOp::verify() {
        -: 1085:  // Ensure that the tensor type has as many dynamic dimensions as are
        -: 1086:  // specified by the operands.
 21238641: 1087:  RankedTensorType resultTy = getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
 21238641: 1088:  if (getNumOperands() != resultTy.getNumDynamicDims())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####: 1089:    return emitError("must have as many index operands as dynamic extents "
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1090:                     "in the result type");
call    0 never executed
        -: 1091:
 21238641: 1092:  return success();
        -: 1093:}
        -: 1094:
function _ZN4mlir6tensor10GenerateOp13verifyRegionsEv called 21237997 returned 100% blocks executed 56%
 21237997: 1095:LogicalResult GenerateOp::verifyRegions() {
 21237997: 1096:  RankedTensorType resultTy = getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
        -: 1097:  // Ensure that region arguments span the index space.
 21237997: 1098:  if (!llvm::all_of(getBody().getArgumentTypes(),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
        -: 1099:                    [](Type ty) { return ty.isIndex(); }))
    #####: 1100:    return emitError("all body arguments must be index");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
 21237997: 1101:  if (getBody().getNumArguments() != resultTy.getRank())
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####: 1102:    return emitError("must have one body argument per input dimension");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1103:
        -: 1104:  // Ensure that the region yields an element of the right type.
 21237997: 1105:  auto yieldOp = cast<YieldOp>(getBody().getBlocks().front().getTerminator());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -: 1106:
 21237997: 1107:  if (yieldOp.getValue().getType() != resultTy.getElementType())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####: 1108:    return emitOpError(
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1109:        "body must be terminated with a `yield` operation of the tensor "
    #####: 1110:        "element type");
call    0 never executed
        -: 1111:
 21237997: 1112:  return success();
        -: 1113:}
        -: 1114:
function _ZN4mlir6tensor10GenerateOp5buildERNS_9OpBuilderERNS_14OperationStateENS_4TypeENS_10ValueRangeEN4llvm12function_refIFvS3_NS_8LocationES7_EEE called 0 returned 0% blocks executed 0%
    #####: 1115:void GenerateOp::build(
        -: 1116:    OpBuilder &b, OperationState &result, Type resultTy,
        -: 1117:    ValueRange dynamicExtents,
        -: 1118:    function_ref<void(OpBuilder &, Location, ValueRange)> bodyBuilder) {
    #####: 1119:  build(b, result, resultTy, dynamicExtents);
call    0 never executed
        -: 1120:
        -: 1121:  // Build and populate body.
    #####: 1122:  OpBuilder::InsertionGuard guard(b);
branch  0 never executed
branch  1 never executed
    #####: 1123:  Region *bodyRegion = result.regions.front().get();
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1124:  auto rank = resultTy.cast<RankedTensorType>().getRank();
call    0 never executed
call    1 never executed
    #####: 1125:  SmallVector<Type, 2> argumentTypes(rank, b.getIndexType());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1126:  SmallVector<Location, 2> argumentLocs(rank, result.location);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1127:  Block *bodyBlock =
call    0 never executed
    #####: 1128:      b.createBlock(bodyRegion, bodyRegion->end(), argumentTypes, argumentLocs);
call    0 never executed
call    1 never executed
    #####: 1129:  bodyBuilder(b, result.location, bodyBlock->getArguments());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1130:}
        -: 1131:
        -: 1132:namespace {
        -: 1133:
        -: 1134:/// Canonicalizes tensor.generate operations with a constant
        -: 1135:/// operand into the equivalent operation with the operand expressed in the
        -: 1136:/// result type, instead. We also insert a type cast to make sure that the
        -: 1137:/// resulting IR is still well-typed.
        -: 1138:struct StaticTensorGenerate : public OpRewritePattern<GenerateOp> {
        -: 1139:  using OpRewritePattern<GenerateOp>::OpRewritePattern;
        -: 1140:
function _ZNK12_GLOBAL__N_120StaticTensorGenerate15matchAndRewriteEN4mlir6tensor10GenerateOpERNS1_15PatternRewriterE called 5641 returned 100% blocks executed 87%
     5641: 1141:  LogicalResult matchAndRewrite(GenerateOp tensorFromElements,
        -: 1142:                                PatternRewriter &rewriter) const final {
     5641: 1143:    auto resultType =
     5641: 1144:        tensorFromElements.getResult().getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1145:
     5641: 1146:    if (resultType.hasStaticShape())
call    0 returned 100%
branch  1 taken 64% (fallthrough)
branch  2 taken 36%
     3617: 1147:      return failure();
        -: 1148:
     2024: 1149:    SmallVector<Value, 4> newOperands;
call    0 returned 100%
     2024: 1150:    SmallVector<int64_t, 4> newShape;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
     2024: 1151:    auto operandsIt = tensorFromElements.getDynamicExtents().begin();
call    0 returned 100%
call    1 returned 100%
        -: 1152:
     6012: 1153:    for (int64_t dim : resultType.getShape()) {
call    0 returned 100%
branch  1 taken 66% (fallthrough)
branch  2 taken 34%
     3988: 1154:      if (!ShapedType::isDynamic(dim)) {
branch  0 taken 29% (fallthrough)
branch  1 taken 71%
     1174: 1155:        newShape.push_back(dim);
call    0 returned 100%
     1174: 1156:        continue;
        -: 1157:      }
     5184: 1158:      APInt index;
call    0 returned 100%
     2814: 1159:      if (!matchPattern(*operandsIt, m_ConstantInt(&index))) {
call    0 returned 100%
branch  1 taken 16% (fallthrough)
branch  2 taken 84%
      444: 1160:        newShape.push_back(ShapedType::kDynamicSize);
call    0 returned 100%
      444: 1161:        newOperands.push_back(*operandsIt++);
call    0 returned 100%
     2062: 1162:        continue;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1163:      }
     2370: 1164:      newShape.push_back(index.getSExtValue());
call    0 returned 100%
call    1 returned 100%
     2370: 1165:      operandsIt++;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1166:    }
        -: 1167:
     2024: 1168:    if (newOperands.size() == tensorFromElements.getDynamicExtents().size())
call    0 returned 100%
branch  1 taken 17% (fallthrough)
branch  2 taken 83%
      343: 1169:      return failure();
        -: 1170:
     1681: 1171:    auto loc = tensorFromElements.getLoc();
call    0 returned 100%
     1681: 1172:    auto newOp = rewriter.create<GenerateOp>(
     1681: 1173:        loc, RankedTensorType::get(newShape, resultType.getElementType()),
call    0 returned 100%
     3362: 1174:        newOperands);
call    0 returned 100%
call    1 returned 100%
     1681: 1175:    rewriter.inlineRegionBefore(tensorFromElements.getBody(), newOp.getBody(),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
     1681: 1176:                                newOp.getBody().begin());
call    0 returned 100%
call    1 returned 100%
     1681: 1177:    rewriter.replaceOpWithNewOp<tensor::CastOp>(tensorFromElements, resultType,
     1681: 1178:                                                newOp);
call    0 returned 100%
     2024: 1179:    return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1180:  }
        -: 1181:};
        -: 1182:
        -: 1183:/// Canonicalizes the pattern of the form
        -: 1184:///
        -: 1185:/// %tensor = tensor.generate %x {
        -: 1186:///   ^bb0(%arg0: index):
        -: 1187:///   <computation>
        -: 1188:///   yield %1 : index
        -: 1189:/// } : tensor<?xindex>
        -: 1190:/// %extracted_element = tensor.extract %tensor[%c0] : tensor<?xi32>
        -: 1191:///
        -: 1192:/// to just <computation> with %arg0 replaced by %c0. We only do this if the
        -: 1193:/// tensor.generate operation has no side-effects.
        -: 1194:struct ExtractFromTensorGenerate : public OpRewritePattern<tensor::ExtractOp> {
        -: 1195:  using OpRewritePattern<tensor::ExtractOp>::OpRewritePattern;
        -: 1196:
function _ZNK12_GLOBAL__N_125ExtractFromTensorGenerate15matchAndRewriteEN4mlir6tensor9ExtractOpERNS1_15PatternRewriterE called 5198 returned 100% blocks executed 88%
     5198: 1197:  LogicalResult matchAndRewrite(tensor::ExtractOp extract,
        -: 1198:                                PatternRewriter &rewriter) const final {
     5198: 1199:    auto tensorFromElements = extract.getTensor().getDefiningOp<GenerateOp>();
call    0 returned 100%
call    1 returned 100%
     5198: 1200:    if (!tensorFromElements || !wouldOpBeTriviallyDead(tensorFromElements))
branch  0 taken 1% (fallthrough)
branch  1 taken 99%
call    2 returned 100%
branch  3 taken 59% (fallthrough)
branch  4 taken 41%
     5180: 1201:      return failure();
        -: 1202:
       36: 1203:    BlockAndValueMapping mapping;
call    0 returned 100%
call    1 returned 100%
       18: 1204:    Block *body = &tensorFromElements.getBody().front();
call    0 returned 100%
call    1 returned 100%
       18: 1205:    mapping.map(body->getArguments(), extract.getIndices());
call    0 returned 100%
call    1 returned 100%
      36*: 1206:    for (auto &op : body->without_terminator())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
call    4 never executed
    #####: 1207:      rewriter.clone(op, mapping);
call    0 never executed
        -: 1208:
       18: 1209:    auto yield = cast<YieldOp>(body->getTerminator());
call    0 returned 100%
call    1 returned 100%
        -: 1210:
       18: 1211:    rewriter.replaceOp(extract, mapping.lookupOrDefault(yield.getValue()));
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
       18: 1212:    return success();
call    0 returned 100%
        -: 1213:  }
        -: 1214:};
        -: 1215:
        -: 1216:} // namespace
        -: 1217:
function _ZN4mlir6tensor10GenerateOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235: 1218:void GenerateOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 1219:                                             MLIRContext *context) {
        -: 1220:  // TODO: Move extract pattern to tensor::ExtractOp.
     1235: 1221:  results.add<ExtractFromTensorGenerate, StaticTensorGenerate>(context);
call    0 returned 100%
     1235: 1222:}
        -: 1223:
        -: 1224://===----------------------------------------------------------------------===//
        -: 1225:// RankOp
        -: 1226://===----------------------------------------------------------------------===//
        -: 1227:
function _ZN4mlir6tensor6RankOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 39647748 returned 100% blocks executed 100%
 39647748: 1228:void RankOp::getAsmResultNames(function_ref<void(Value, StringRef)> setNameFn) {
 39647748: 1229:  setNameFn(getResult(), "rank");
call    0 returned 100%
 39647748: 1230:}
        -: 1231:
function _ZN4mlir6tensor6RankOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 200366 returned 100% blocks executed 86%
   200366: 1232:OpFoldResult RankOp::fold(ArrayRef<Attribute> operands) {
        -: 1233:  // Constant fold rank when the rank of the operand is known.
   200366: 1234:  auto type = getOperand().getType();
call    0 returned 100%
call    1 returned 100%
   200366: 1235:  auto shapedType = type.dyn_cast<ShapedType>();
call    0 returned 100%
   200366: 1236:  if (shapedType && shapedType.hasRank())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
branch  3 taken 100% (fallthrough)
branch  4 taken 0%
   200366: 1237:    return IntegerAttr::get(IndexType::get(getContext()), shapedType.getRank());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
    #####: 1238:  return IntegerAttr();
call    0 never executed
        -: 1239:}
        -: 1240:
        -: 1241://===----------------------------------------------------------------------===//
        -: 1242:// ReshapeOp
        -: 1243://===----------------------------------------------------------------------===//
        -: 1244:
function _ZN4mlir6tensor9ReshapeOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####: 1245:void ReshapeOp::getAsmResultNames(
        -: 1246:    function_ref<void(Value, StringRef)> setNameFn) {
    #####: 1247:  setNameFn(getResult(), "reshape");
call    0 never executed
call    1 never executed
    #####: 1248:}
        -: 1249:
function _ZL14getNumElementsN4mlir10ShapedTypeE called 0 returned 0% blocks executed 0%
    #####: 1250:static int64_t getNumElements(ShapedType type) {
    #####: 1251:  int64_t numElements = 1;
    #####: 1252:  for (auto dim : type.getShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1253:    numElements *= dim;
    #####: 1254:  return numElements;
        -: 1255:}
        -: 1256:
function _ZN4mlir6tensor9ReshapeOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 1257:LogicalResult ReshapeOp::verify() {
    #####: 1258:  TensorType operandType = getSource().getType().cast<TensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1259:  TensorType resultType = getResult().getType().cast<TensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1260:
    #####: 1261:  if (operandType.getElementType() != resultType.getElementType())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1262:    return emitOpError("element types of source and destination tensor "
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1263:                       "types should be the same");
call    0 never executed
        -: 1264:
    #####: 1265:  int64_t shapeSize =
    #####: 1266:      getShape().getType().cast<RankedTensorType>().getDimSize(0);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 1267:  auto resultRankedType = resultType.dyn_cast<RankedTensorType>();
call    0 never executed
    #####: 1268:  auto operandRankedType = operandType.dyn_cast<RankedTensorType>();
call    0 never executed
        -: 1269:
    #####: 1270:  if (resultRankedType) {
branch  0 never executed
branch  1 never executed
    #####: 1271:    if (operandRankedType && resultRankedType.hasStaticShape() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 1272:        operandRankedType.hasStaticShape()) {
call    0 never executed
    #####: 1273:      if (getNumElements(operandRankedType) != getNumElements(resultRankedType))
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1274:        return emitOpError("source and destination tensor should have the "
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1275:                           "same number of elements");
call    0 never executed
        -: 1276:    }
    #####: 1277:    if (ShapedType::isDynamic(shapeSize))
branch  0 never executed
branch  1 never executed
    #####: 1278:      return emitOpError("cannot use shape operand with dynamic length to "
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1279:                         "reshape to statically-ranked tensor type");
call    0 never executed
    #####: 1280:    if (shapeSize != resultRankedType.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1281:      return emitOpError(
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1282:          "length of shape operand differs from the result's tensor rank");
call    0 never executed
        -: 1283:  }
    #####: 1284:  return success();
        -: 1285:}
        -: 1286:
        -: 1287://===----------------------------------------------------------------------===//
        -: 1288:// Reassociative reshape ops
        -: 1289://===----------------------------------------------------------------------===//
        -: 1290:
function _ZN4mlir6tensor15CollapseShapeOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 42950751 returned 100% blocks executed 100%
 42950751: 1291:void CollapseShapeOp::getAsmResultNames(
        -: 1292:    function_ref<void(Value, StringRef)> setNameFn) {
 42950751: 1293:  setNameFn(getResult(), "collapsed");
call    0 returned 100%
call    1 returned 100%
 42950751: 1294:}
        -: 1295:
function _ZN4mlir6tensor13ExpandShapeOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 51062728 returned 100% blocks executed 100%
 51062728: 1296:void ExpandShapeOp::getAsmResultNames(
        -: 1297:    function_ref<void(Value, StringRef)> setNameFn) {
 51062728: 1298:  setNameFn(getResult(), "expanded");
call    0 returned 100%
call    1 returned 100%
 51062728: 1299:}
        -: 1300:
function _ZN4mlir6tensor15CollapseShapeOp20getReassociationMapsEv called 90497663 returned 100% blocks executed 100%
 90497663: 1301:SmallVector<AffineMap, 4> CollapseShapeOp::getReassociationMaps() {
 90497663: 1302:  return getSymbolLessAffineMaps(getReassociationExprs());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1303:}
function _ZN4mlir6tensor15CollapseShapeOp21getReassociationExprsEv called 90497663 returned 100% blocks executed 100%
 90497663: 1304:SmallVector<ReassociationExprs, 4> CollapseShapeOp::getReassociationExprs() {
 90497661: 1305:  return convertReassociationIndicesToExprs(getContext(),
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
180995324: 1306:                                            getReassociationIndices());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -: 1307:}
        -: 1308:
function _ZN4mlir6tensor13ExpandShapeOp20getReassociationMapsEv called 107120826 returned 100% blocks executed 100%
107120826: 1309:SmallVector<AffineMap, 4> ExpandShapeOp::getReassociationMaps() {
107120826: 1310:  return getSymbolLessAffineMaps(getReassociationExprs());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1311:}
function _ZN4mlir6tensor13ExpandShapeOp21getReassociationExprsEv called 107120826 returned 100% blocks executed 100%
107120826: 1312:SmallVector<ReassociationExprs, 4> ExpandShapeOp::getReassociationExprs() {
107120837: 1313:  return convertReassociationIndicesToExprs(getContext(),
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
214241663: 1314:                                            getReassociationIndices());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -: 1315:}
        -: 1316:
        -: 1317:/// Compute the RankedTensorType obtained by applying `reassociation` to
        -: 1318:/// `type`.
        -: 1319:static RankedTensorType
function _ZL33computeTensorReshapeCollapsedTypeN4mlir16RankedTensorTypeEN4llvm8ArrayRefINS_9AffineMapEEE called 98807985 returned 100% blocks executed 91%
 98807985: 1320:computeTensorReshapeCollapsedType(RankedTensorType type,
        -: 1321:                                  ArrayRef<AffineMap> reassociation) {
 98807985: 1322:  auto shape = type.getShape();
call    0 returned 100%
 98807983: 1323:  SmallVector<int64_t, 4> newShape;
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
 98807983: 1324:  newShape.reserve(reassociation.size());
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
        -: 1325:
        -: 1326:  // Use the fact that reassociation is valid to simplify the logic: only use
        -: 1327:  // each map's rank.
98807983*: 1328:  assert(isReassociationValid(reassociation) && "invalid reassociation");
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
 98807979: 1329:  unsigned currentDim = 0;
266336119: 1330:  for (AffineMap m : reassociation) {
branch  0 taken 63% (fallthrough)
branch  1 taken 37%
167528140: 1331:    unsigned dim = m.getNumResults();
call    0 returned 100%
167528142: 1332:    auto band = shape.slice(currentDim, dim);
call    0 returned 100%
167528142: 1333:    int64_t size = 1;
167528142: 1334:    if (llvm::is_contained(band, ShapedType::kDynamicSize))
branch  0 taken 99% (fallthrough)
branch  1 taken 1%
        -: 1335:      size = ShapedType::kDynamicSize;
        -: 1336:    else
428074869: 1337:      for (unsigned d = 0; d < dim; ++d)
branch  0 taken 61% (fallthrough)
branch  1 taken 39%
262819470: 1338:        size *= shape[currentDim + d];
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
167528142: 1339:    newShape.push_back(size);
call    0 returned 100%
167528140: 1340:    currentDim += dim;
        -: 1341:  }
        -: 1342:
 98807979: 1343:  return RankedTensorType::get(newShape, type.getElementType());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
        -: 1344:}
        -: 1345:
function _ZN4mlir6tensor15CollapseShapeOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueEN4llvm8ArrayRefINS7_11SmallVectorIlLj2EEEEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1346:void CollapseShapeOp::build(OpBuilder &b, OperationState &result, Value src,
        -: 1347:                            ArrayRef<ReassociationIndices> reassociation,
        -: 1348:                            ArrayRef<NamedAttribute> attrs) {
    #####: 1349:  auto resultType = computeTensorReshapeCollapsedType(
    #####: 1350:      src.getType().cast<RankedTensorType>(),
    #####: 1351:      getSymbolLessAffineMaps(
call    0 never executed
call    1 never executed
    #####: 1352:          convertReassociationIndicesToExprs(b.getContext(), reassociation)));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
    #####: 1353:  build(b, result, resultType, src, attrs);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1354:  result.addAttribute(getReassociationAttrStrName(),
call    0 never executed
    #####: 1355:                      getReassociationIndicesAttribute(b, reassociation));
call    0 never executed
    #####: 1356:}
        -: 1357:
        -: 1358:// Checks if types are the same, but ignoring encoding on ranked tensors.
function _ZL26isSameTypesWithoutEncodingN4mlir4TypeES0_ called 98807976 returned 100% blocks executed 81%
 98807976: 1359:static bool isSameTypesWithoutEncoding(Type tp1, Type tp2) {
 98807976: 1360:  if (auto rtp1 = tp1.dyn_cast<RankedTensorType>()) {
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
 98807976: 1361:    if (auto rtp2 = tp2.dyn_cast<RankedTensorType>())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
197615940: 1362:      return rtp1.getShape() == rtp2.getShape() &&
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
 98807968: 1363:             rtp1.getElementType() == rtp2.getElementType();
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####: 1364:    return false;
        -: 1365:  }
        -: 1366:  // Default implementation.
    #####: 1367:  return tp1 == tp2;
        -: 1368:}
        -: 1369:
        -: 1370:template <typename TensorReshapeOp, bool isExpansion = std::is_same<
        -: 1371:                                        TensorReshapeOp, ExpandShapeOp>::value>
 98807962: 1372:static LogicalResult verifyTensorReshapeOp(TensorReshapeOp op,
        -: 1373:                                           RankedTensorType expandedType,
        -: 1374:                                           RankedTensorType collapsedType) {
 98807962: 1375:  if (failed(
        -: 1376:          verifyReshapeLikeTypes(op, expandedType, collapsedType, isExpansion)))
 98807971: 1377:    return failure();
        -: 1378:
 98807968: 1379:  auto maps = op.getReassociationMaps();
        -: 1380:  RankedTensorType expectedType =
 98807967: 1381:      computeTensorReshapeCollapsedType(expandedType, maps);
 98807976: 1382:  if (!isSameTypesWithoutEncoding(collapsedType, expectedType))
        -: 1383:    return op.emitOpError("expected collapsed type to be ")
    #####: 1384:           << expectedType << ", but got " << collapsedType;
 98807971: 1385:  return success();
        -: 1386:}
------------------
_Z21verifyTensorReshapeOpIN4mlir6tensor15CollapseShapeOpELb0EENS0_13LogicalResultET_NS0_16RankedTensorTypeES5_:
function _Z21verifyTensorReshapeOpIN4mlir6tensor15CollapseShapeOpELb0EENS0_13LogicalResultET_NS0_16RankedTensorTypeES5_ called 45248244 returned 100% blocks executed 50%
 45248244: 1372:static LogicalResult verifyTensorReshapeOp(TensorReshapeOp op,
        -: 1373:                                           RankedTensorType expandedType,
        -: 1374:                                           RankedTensorType collapsedType) {
 45248244: 1375:  if (failed(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -: 1376:          verifyReshapeLikeTypes(op, expandedType, collapsedType, isExpansion)))
 45248245: 1377:    return failure();
        -: 1378:
 45248246: 1379:  auto maps = op.getReassociationMaps();
call    0 returned 100%
call    1 returned 100%
        -: 1380:  RankedTensorType expectedType =
 45248243: 1381:      computeTensorReshapeCollapsedType(expandedType, maps);
call    0 returned 100%
 45248247: 1382:  if (!isSameTypesWithoutEncoding(collapsedType, expectedType))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -: 1383:    return op.emitOpError("expected collapsed type to be ")
    #####: 1384:           << expectedType << ", but got " << collapsedType;
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
 45248245: 1385:  return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -: 1386:}
------------------
_Z21verifyTensorReshapeOpIN4mlir6tensor13ExpandShapeOpELb1EENS0_13LogicalResultET_NS0_16RankedTensorTypeES5_:
function _Z21verifyTensorReshapeOpIN4mlir6tensor13ExpandShapeOpELb1EENS0_13LogicalResultET_NS0_16RankedTensorTypeES5_ called 53559718 returned 100% blocks executed 55%
 53559718: 1372:static LogicalResult verifyTensorReshapeOp(TensorReshapeOp op,
        -: 1373:                                           RankedTensorType expandedType,
        -: 1374:                                           RankedTensorType collapsedType) {
 53559718: 1375:  if (failed(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -: 1376:          verifyReshapeLikeTypes(op, expandedType, collapsedType, isExpansion)))
 53559726: 1377:    return failure();
        -: 1378:
 53559722: 1379:  auto maps = op.getReassociationMaps();
call    0 returned 100%
call    1 returned 100%
        -: 1380:  RankedTensorType expectedType =
 53559724: 1381:      computeTensorReshapeCollapsedType(expandedType, maps);
call    0 returned 100%
 53559729: 1382:  if (!isSameTypesWithoutEncoding(collapsedType, expectedType))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
        -: 1383:    return op.emitOpError("expected collapsed type to be ")
    #####: 1384:           << expectedType << ", but got " << collapsedType;
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
 53559726: 1385:  return success();
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
        -: 1386:}
------------------
        -: 1387:
function _ZN4mlir6tensor13ExpandShapeOp6verifyEv called 53559725 returned 100% blocks executed 100%
 53559725: 1388:LogicalResult ExpandShapeOp::verify() {
 53559725: 1389:  return verifyTensorReshapeOp(*this, getResultType(), getSrcType());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1390:}
        -: 1391:
function _ZN4mlir6tensor15CollapseShapeOp6verifyEv called 45248245 returned 100% blocks executed 100%
 45248245: 1392:LogicalResult CollapseShapeOp::verify() {
 45248245: 1393:  return verifyTensorReshapeOp(*this, getSrcType(), getResultType());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1394:}
        -: 1395:
        -: 1396:namespace {
        -: 1397:/// Reshape of a splat constant can be replaced with a constant of the result
        -: 1398:/// type.
        -: 1399:template <typename TensorReshapeOp>
        -: 1400:struct FoldReshapeWithConstant : OpRewritePattern<TensorReshapeOp> {
        -: 1401:  using OpRewritePattern<TensorReshapeOp>::OpRewritePattern;
     2905: 1402:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -: 1403:                                PatternRewriter &rewriter) const override {
     2905: 1404:    DenseElementsAttr attr;
     2905: 1405:    if (!matchPattern(reshapeOp.getSrc(), m_Constant(&attr)))
     2905: 1406:      return failure();
    #####: 1407:    if (!attr || !attr.isSplat())
     2905: 1408:      return failure();
    #####: 1409:    DenseElementsAttr newAttr = DenseElementsAttr::getFromRawBuffer(
        -: 1410:        reshapeOp.getResultType(), attr.getRawData());
    #####: 1411:    rewriter.replaceOpWithNewOp<arith::ConstantOp>(reshapeOp, newAttr);
     2905: 1412:    return success();
        -: 1413:  }
------------------
_ZNK12_GLOBAL__N_123FoldReshapeWithConstantIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_123FoldReshapeWithConstantIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 2060 returned 100% blocks executed 33%
     2060: 1402:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -: 1403:                                PatternRewriter &rewriter) const override {
     2060: 1404:    DenseElementsAttr attr;
call    0 returned 100%
     2060: 1405:    if (!matchPattern(reshapeOp.getSrc(), m_Constant(&attr)))
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
     2060: 1406:      return failure();
    #####: 1407:    if (!attr || !attr.isSplat())
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
     2060: 1408:      return failure();
    #####: 1409:    DenseElementsAttr newAttr = DenseElementsAttr::getFromRawBuffer(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1410:        reshapeOp.getResultType(), attr.getRawData());
    #####: 1411:    rewriter.replaceOpWithNewOp<arith::ConstantOp>(reshapeOp, newAttr);
call    0 never executed
     2060: 1412:    return success();
        -: 1413:  }
------------------
_ZNK12_GLOBAL__N_123FoldReshapeWithConstantIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_123FoldReshapeWithConstantIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 845 returned 100% blocks executed 33%
      845: 1402:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -: 1403:                                PatternRewriter &rewriter) const override {
      845: 1404:    DenseElementsAttr attr;
call    0 returned 100%
      845: 1405:    if (!matchPattern(reshapeOp.getSrc(), m_Constant(&attr)))
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
      845: 1406:      return failure();
    #####: 1407:    if (!attr || !attr.isSplat())
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
      845: 1408:      return failure();
    #####: 1409:    DenseElementsAttr newAttr = DenseElementsAttr::getFromRawBuffer(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 1410:        reshapeOp.getResultType(), attr.getRawData());
    #####: 1411:    rewriter.replaceOpWithNewOp<arith::ConstantOp>(reshapeOp, newAttr);
call    0 never executed
      845: 1412:    return success();
        -: 1413:  }
------------------
        -: 1414:};
        -: 1415:
        -: 1416:/// Reshape of a FromElements can be replaced with a FromElements of the
        -: 1417:/// result type
        -: 1418:template <typename TensorReshapeOp>
        -: 1419:struct FoldReshapeWithFromElements : OpRewritePattern<TensorReshapeOp> {
        -: 1420:  using OpRewritePattern<TensorReshapeOp>::OpRewritePattern;
     2905: 1421:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -: 1422:                                PatternRewriter &rewriter) const override {
     2905: 1423:    auto fromElements =
     5810: 1424:        reshapeOp.getSrc().template getDefiningOp<FromElementsOp>();
     2905: 1425:    if (!fromElements)
     2905: 1426:      return failure();
        -: 1427:
       5*: 1428:    auto shapedTy = reshapeOp.getType().template cast<ShapedType>();
        -: 1429:
       5*: 1430:    if (!shapedTy.hasStaticShape())
     2905: 1431:      return failure();
        -: 1432:
       5*: 1433:    rewriter.replaceOpWithNewOp<FromElementsOp>(reshapeOp, reshapeOp.getType(),
        -: 1434:                                                fromElements.getElements());
     2905: 1435:    return success();
        -: 1436:  }
------------------
_ZNK12_GLOBAL__N_127FoldReshapeWithFromElementsIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_127FoldReshapeWithFromElementsIN4mlir6tensor13ExpandShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 2060 returned 100% blocks executed 36%
     2060: 1421:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -: 1422:                                PatternRewriter &rewriter) const override {
     2060: 1423:    auto fromElements =
call    0 returned 100%
     4120: 1424:        reshapeOp.getSrc().template getDefiningOp<FromElementsOp>();
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
     2060: 1425:    if (!fromElements)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     2060: 1426:      return failure();
        -: 1427:
    #####: 1428:    auto shapedTy = reshapeOp.getType().template cast<ShapedType>();
call    0 never executed
        -: 1429:
    #####: 1430:    if (!shapedTy.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
     2060: 1431:      return failure();
        -: 1432:
    #####: 1433:    rewriter.replaceOpWithNewOp<FromElementsOp>(reshapeOp, reshapeOp.getType(),
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1434:                                                fromElements.getElements());
     2060: 1435:    return success();
        -: 1436:  }
------------------
_ZNK12_GLOBAL__N_127FoldReshapeWithFromElementsIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_127FoldReshapeWithFromElementsIN4mlir6tensor15CollapseShapeOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 845 returned 100% blocks executed 93%
      845: 1421:  LogicalResult matchAndRewrite(TensorReshapeOp reshapeOp,
        -: 1422:                                PatternRewriter &rewriter) const override {
      845: 1423:    auto fromElements =
call    0 returned 100%
     1690: 1424:        reshapeOp.getSrc().template getDefiningOp<FromElementsOp>();
call    0 returned 100%
branch  1 taken 99% (fallthrough)
branch  2 taken 1%
      845: 1425:    if (!fromElements)
branch  0 taken 99% (fallthrough)
branch  1 taken 1%
      845: 1426:      return failure();
        -: 1427:
        5: 1428:    auto shapedTy = reshapeOp.getType().template cast<ShapedType>();
call    0 returned 100%
        -: 1429:
        5: 1430:    if (!shapedTy.hasStaticShape())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
      845: 1431:      return failure();
        -: 1432:
        5: 1433:    rewriter.replaceOpWithNewOp<FromElementsOp>(reshapeOp, reshapeOp.getType(),
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -: 1434:                                                fromElements.getElements());
      845: 1435:    return success();
        -: 1436:  }
------------------
        -: 1437:};
        -: 1438:
        -: 1439:// Fold CastOp into CollapseShapeOp when adding static information.
        -: 1440:struct FoldCollapseOfCastOp : public OpRewritePattern<CollapseShapeOp> {
        -: 1441:  using OpRewritePattern<CollapseShapeOp>::OpRewritePattern;
        -: 1442:
function _ZNK12_GLOBAL__N_120FoldCollapseOfCastOp15matchAndRewriteEN4mlir6tensor15CollapseShapeOpERNS1_15PatternRewriterE called 840 returned 100% blocks executed 88%
      840: 1443:  LogicalResult matchAndRewrite(CollapseShapeOp collapseShapeOp,
        -: 1444:                                PatternRewriter &rewriter) const override {
      840: 1445:    auto castOp = collapseShapeOp.getSrc().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
call    1 returned 100%
      840: 1446:    if (!tensor::canFoldIntoConsumerOp(castOp))
call    0 returned 100%
branch  1 taken 98% (fallthrough)
branch  2 taken 2%
      821: 1447:      return failure();
        -: 1448:
       19: 1449:    RankedTensorType srcType =
       19: 1450:        castOp.getSource().getType().cast<RankedTensorType>();
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
       19: 1451:    RankedTensorType newResultType = computeTensorReshapeCollapsedType(
       19: 1452:        srcType, collapseShapeOp.getReassociationMaps());
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -: 1453:
       19: 1454:    if (newResultType == collapseShapeOp.getResultType()) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
function _ZZNK12_GLOBAL__N_120FoldCollapseOfCastOp15matchAndRewriteEN4mlir6tensor15CollapseShapeOpERNS1_15PatternRewriterEENKUlvE_clEv.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1455:      rewriter.updateRootInPlace(collapseShapeOp, [&]() {
call    0 never executed
    #####: 1456:        collapseShapeOp.getSrcMutable().assign(castOp.getSource());
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1457:      });
        -: 1458:    } else {
       19: 1459:      auto newOp = rewriter.create<CollapseShapeOp>(
       38: 1460:          collapseShapeOp.getLoc(), newResultType, castOp.getSource(),
       19: 1461:          collapseShapeOp.getReassociation());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
       19: 1462:      rewriter.replaceOpWithNewOp<tensor::CastOp>(
       19: 1463:          collapseShapeOp, collapseShapeOp.getResultType(), newOp);
call    0 returned 100%
call    1 returned 100%
        -: 1464:    }
       19: 1465:    return success();
        -: 1466:  }
        -: 1467:};
        -: 1468:
        -: 1469:} // namespace
        -: 1470:
function _ZN4mlir6tensor13ExpandShapeOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 2056 returned 100% blocks executed 100%
     2056: 1471:void ExpandShapeOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 1472:                                                MLIRContext *context) {
     2056: 1473:  results.add<ComposeReassociativeReshapeOps<ExpandShapeOp>,
        -: 1474:              ComposeExpandOfCollapseOp<ExpandShapeOp, CollapseShapeOp>,
        -: 1475:              FoldReshapeWithConstant<ExpandShapeOp>,
     2056: 1476:              FoldReshapeWithFromElements<ExpandShapeOp>>(context);
call    0 returned 100%
     2056: 1477:}
        -: 1478:
function _ZN4mlir6tensor15CollapseShapeOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 2056 returned 100% blocks executed 100%
     2056: 1479:void CollapseShapeOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 1480:                                                  MLIRContext *context) {
     2056: 1481:  results
        -: 1482:      .add<ComposeReassociativeReshapeOps<CollapseShapeOp>,
        -: 1483:           ComposeCollapseOfExpandOp<CollapseShapeOp, ExpandShapeOp>,
        -: 1484:           FoldReshapeWithConstant<CollapseShapeOp>,
        -: 1485:           FoldReshapeWithFromElements<CollapseShapeOp>, FoldCollapseOfCastOp>(
     2056: 1486:          context);
call    0 returned 100%
     2056: 1487:}
        -: 1488:
function _ZN4mlir6tensor13ExpandShapeOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 231013 returned 100% blocks executed 100%
   231013: 1489:OpFoldResult ExpandShapeOp::fold(ArrayRef<Attribute> operands) {
   231013: 1490:  return foldReshapeOp<ExpandShapeOp, CollapseShapeOp>(*this, operands);
call    0 returned 100%
        -: 1491:}
function _ZN4mlir6tensor15CollapseShapeOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 196734 returned 100% blocks executed 100%
   196734: 1492:OpFoldResult CollapseShapeOp::fold(ArrayRef<Attribute> operands) {
   196734: 1493:  return foldReshapeOp<CollapseShapeOp, ExpandShapeOp>(*this, operands);
call    0 returned 100%
        -: 1494:}
        -: 1495:
        -: 1496://===----------------------------------------------------------------------===//
        -: 1497:// ExtractSliceOp
        -: 1498://===----------------------------------------------------------------------===//
        -: 1499:
function _ZN4mlir6tensor14ExtractSliceOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####: 1500:void ExtractSliceOp::getAsmResultNames(
        -: 1501:    function_ref<void(Value, StringRef)> setNameFn) {
    #####: 1502:  setNameFn(getResult(), "extracted_slice");
call    0 never executed
call    1 never executed
    #####: 1503:}
        -: 1504:
        -: 1505:/// An extract_slice result type can be inferred, when it is not
        -: 1506:/// rank-reduced, from the source type and the static representation of
        -: 1507:/// offsets, sizes and strides. Special sentinels encode the dynamic case.
function _ZN4mlir6tensor14ExtractSliceOp15inferResultTypeENS_10ShapedTypeEN4llvm8ArrayRefIlEES5_S5_ called 0 returned 0% blocks executed 0%
    #####: 1508:RankedTensorType ExtractSliceOp::inferResultType(
        -: 1509:    ShapedType sourceShapedTensorType, ArrayRef<int64_t> staticOffsets,
        -: 1510:    ArrayRef<int64_t> staticSizes, ArrayRef<int64_t> staticStrides) {
        -: 1511:  // An extract_slice op may specify only a leading subset of offset/sizes/
        -: 1512:  // strides in which case we complete with offset=0, sizes from memref type
        -: 1513:  // and strides=1.
    #####: 1514:  assert(static_cast<int64_t>(staticSizes.size()) ==
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -: 1515:             sourceShapedTensorType.getRank() &&
        -: 1516:         "unexpected staticSizes not equal to rank of source");
    #####: 1517:  return RankedTensorType::get(staticSizes,
    #####: 1518:                               sourceShapedTensorType.getElementType());
call    0 never executed
call    1 never executed
        -: 1519:}
        -: 1520:
function _ZN4mlir6tensor14ExtractSliceOp15inferResultTypeENS_10ShapedTypeEN4llvm8ArrayRefINS_12OpFoldResultEEES6_S6_ called 0 returned 0% blocks executed 0%
    #####: 1521:RankedTensorType ExtractSliceOp::inferResultType(
        -: 1522:    ShapedType sourceShapedTensorType, ArrayRef<OpFoldResult> offsets,
        -: 1523:    ArrayRef<OpFoldResult> sizes, ArrayRef<OpFoldResult> strides) {
    #####: 1524:  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1525:  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1526:  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets,
call    0 never executed
        -: 1527:                             ShapedType::kDynamicStrideOrOffset);
    #####: 1528:  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes,
call    0 never executed
        -: 1529:                             ShapedType::kDynamicSize);
    #####: 1530:  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides,
call    0 never executed
        -: 1531:                             ShapedType::kDynamicStrideOrOffset);
    #####: 1532:  return ExtractSliceOp::inferResultType(sourceShapedTensorType, staticOffsets,
call    0 never executed
    #####: 1533:                                         staticSizes, staticStrides);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1534:}
        -: 1535:
        -: 1536:/// If the rank is reduced (i.e. the desiredResultRank is smaller than the
        -: 1537:/// number of sizes), drop as many size 1 as needed to produce an inferred
        -: 1538:/// type with the desired rank.
        -: 1539:///
        -: 1540:/// Note that there may be multiple ways to compute this rank-reduced type:
        -: 1541:///   e.g. 1x6x1 can rank-reduce to either 1x6 or 6x1 2-D tensors.
        -: 1542:///
        -: 1543:/// To disambiguate, this function always drops the first 1 sizes occurrences.
function _ZN4mlir6tensor14ExtractSliceOp35inferCanonicalRankReducedResultTypeEjNS_16RankedTensorTypeEN4llvm8ArrayRefIlEES5_S5_ called 0 returned 0% blocks executed 0%
    #####: 1544:RankedTensorType ExtractSliceOp::inferCanonicalRankReducedResultType(
        -: 1545:    unsigned desiredResultRank, RankedTensorType sourceRankedTensorType,
        -: 1546:    ArrayRef<int64_t> offsets, ArrayRef<int64_t> sizes,
        -: 1547:    ArrayRef<int64_t> strides) {
        -: 1548:  // Type inferred in the absence of rank-reducing behavior.
    #####: 1549:  auto inferredType =
call    0 never executed
    #####: 1550:      inferResultType(sourceRankedTensorType, offsets, sizes, strides)
call    0 never executed
call    1 never executed
    #####: 1551:          .cast<RankedTensorType>();
    #####: 1552:  int rankDiff = inferredType.getRank() - desiredResultRank;
call    0 never executed
    #####: 1553:  if (rankDiff > 0) {
branch  0 never executed
branch  1 never executed
    #####: 1554:    auto shape = inferredType.getShape();
call    0 never executed
    #####: 1555:    llvm::SmallBitVector dimsToProject =
    #####: 1556:        getPositionsOfShapeOne(rankDiff, shape);
call    0 never executed
    #####: 1557:    SmallVector<int64_t> projectedShape;
call    0 never executed
        -: 1558:    // Best effort rank-reducing: drop 1s in order.
    #####: 1559:    for (unsigned pos = 0, e = shape.size(); pos < e; ++pos)
branch  0 never executed
branch  1 never executed
    #####: 1560:      if (!dimsToProject.test(pos))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1561:        projectedShape.push_back(shape[pos]);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1562:    inferredType =
    #####: 1563:        RankedTensorType::get(projectedShape, inferredType.getElementType());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1564:  }
    #####: 1565:  return inferredType;
        -: 1566:}
        -: 1567:
function _ZN4mlir6tensor14ExtractSliceOp35inferCanonicalRankReducedResultTypeEjNS_16RankedTensorTypeEN4llvm8ArrayRefINS_12OpFoldResultEEES6_S6_ called 0 returned 0% blocks executed 0%
    #####: 1568:RankedTensorType ExtractSliceOp::inferCanonicalRankReducedResultType(
        -: 1569:    unsigned desiredResultRank, RankedTensorType sourceRankedTensorType,
        -: 1570:    ArrayRef<OpFoldResult> offsets, ArrayRef<OpFoldResult> sizes,
        -: 1571:    ArrayRef<OpFoldResult> strides) {
    #####: 1572:  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1573:  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1574:  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets,
call    0 never executed
        -: 1575:                             ShapedType::kDynamicStrideOrOffset);
    #####: 1576:  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes,
call    0 never executed
        -: 1577:                             ShapedType::kDynamicSize);
    #####: 1578:  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides,
call    0 never executed
        -: 1579:                             ShapedType::kDynamicStrideOrOffset);
    #####: 1580:  return ExtractSliceOp::inferCanonicalRankReducedResultType(
call    0 never executed
        -: 1581:      desiredResultRank, sourceRankedTensorType, staticOffsets, staticSizes,
    #####: 1582:      staticStrides);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1583:}
        -: 1584:
        -: 1585:/// Build an ExtractSliceOp with mixed static and dynamic entries and custom
        -: 1586:/// result type. If the type passed is nullptr, it is inferred.
function _ZN4mlir6tensor14ExtractSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_16RankedTensorTypeENS_5ValueEN4llvm8ArrayRefINS_12OpFoldResultEEESB_SB_NS9_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1587:void ExtractSliceOp::build(OpBuilder &b, OperationState &result,
        -: 1588:                           RankedTensorType resultType, Value source,
        -: 1589:                           ArrayRef<OpFoldResult> offsets,
        -: 1590:                           ArrayRef<OpFoldResult> sizes,
        -: 1591:                           ArrayRef<OpFoldResult> strides,
        -: 1592:                           ArrayRef<NamedAttribute> attrs) {
    #####: 1593:  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1594:  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1595:  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets,
call    0 never executed
        -: 1596:                             ShapedType::kDynamicStrideOrOffset);
    #####: 1597:  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes,
call    0 never executed
        -: 1598:                             ShapedType::kDynamicSize);
    #####: 1599:  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides,
call    0 never executed
        -: 1600:                             ShapedType::kDynamicStrideOrOffset);
    #####: 1601:  auto sourceRankedTensorType = source.getType().cast<RankedTensorType>();
call    0 never executed
        -: 1602:  // Structuring implementation this way avoids duplication between builders.
    #####: 1603:  if (!resultType) {
branch  0 never executed
branch  1 never executed
    #####: 1604:    resultType =
call    0 never executed
    #####: 1605:        ExtractSliceOp::inferResultType(sourceRankedTensorType, staticOffsets,
    #####: 1606:                                        staticSizes, staticStrides)
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1607:            .cast<RankedTensorType>();
        -: 1608:  }
    #####: 1609:  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
        -: 1610:        dynamicStrides, b.getI64ArrayAttr(staticOffsets),
        -: 1611:        b.getI64ArrayAttr(staticSizes), b.getI64ArrayAttr(staticStrides));
    #####: 1612:  result.addAttributes(attrs);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1613:}
        -: 1614:
        -: 1615:/// Build an ExtractSliceOp with mixed static and dynamic entries and inferred
        -: 1616:/// result type.
function _ZN4mlir6tensor14ExtractSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueEN4llvm8ArrayRefINS_12OpFoldResultEEESA_SA_NS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1617:void ExtractSliceOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 1618:                           ArrayRef<OpFoldResult> offsets,
        -: 1619:                           ArrayRef<OpFoldResult> sizes,
        -: 1620:                           ArrayRef<OpFoldResult> strides,
        -: 1621:                           ArrayRef<NamedAttribute> attrs) {
    #####: 1622:  build(b, result, RankedTensorType(), source, offsets, sizes, strides, attrs);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1623:}
        -: 1624:
        -: 1625:/// Build an ExtractSliceOp with mixed static and dynamic entries packed into
        -: 1626:/// a Range vector.
function _ZN4mlir6tensor14ExtractSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueEN4llvm8ArrayRefINS_5RangeEEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1627:void ExtractSliceOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 1628:                           ArrayRef<Range> ranges,
        -: 1629:                           ArrayRef<NamedAttribute> attrs) {
    #####: 1630:  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
call    0 never executed
call    1 never executed
    #####: 1631:  build(b, result, RankedTensorType(), source, offsets, sizes, strides, attrs);
call    0 never executed
call    1 never executed
    #####: 1632:}
        -: 1633:
        -: 1634:/// Build an ExtractSliceOp with dynamic entries and custom result type. If
        -: 1635:/// the type passed is nullptr, it is inferred.
function _ZN4mlir6tensor14ExtractSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_16RankedTensorTypeENS_5ValueENS_10ValueRangeES8_S8_N4llvm8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1636:void ExtractSliceOp::build(OpBuilder &b, OperationState &result,
        -: 1637:                           RankedTensorType resultType, Value source,
        -: 1638:                           ValueRange offsets, ValueRange sizes,
        -: 1639:                           ValueRange strides, ArrayRef<NamedAttribute> attrs) {
    #####: 1640:  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 1641:      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1642:  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 1643:      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1644:  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 1645:      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1646:  build(b, result, resultType, source, offsetValues, sizeValues, strideValues);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1647:}
        -: 1648:
        -: 1649:/// Build an ExtractSliceOp with dynamic entries and inferred result type.
function _ZN4mlir6tensor14ExtractSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueENS_10ValueRangeES7_S7_N4llvm8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1650:void ExtractSliceOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 1651:                           ValueRange offsets, ValueRange sizes,
        -: 1652:                           ValueRange strides, ArrayRef<NamedAttribute> attrs) {
    #####: 1653:  build(b, result, RankedTensorType(), source, offsets, sizes, strides, attrs);
call    0 never executed
    #####: 1654:}
        -: 1655:
        -: 1656:template <typename OpTy>
    #####: 1657:static LogicalResult produceSliceErrorMsg(SliceVerificationResult result,
        -: 1658:                                          OpTy op, Type expectedType) {
    #####: 1659:  auto memrefType = expectedType.cast<ShapedType>();
    #####: 1660:  switch (result) {
    #####: 1661:  case SliceVerificationResult::Success:
    #####: 1662:    return success();
    #####: 1663:  case SliceVerificationResult::RankTooLarge:
        -: 1664:    return op.emitError("expected rank to be smaller or equal to ")
    #####: 1665:           << "the other rank. ";
    #####: 1666:  case SliceVerificationResult::SizeMismatch:
        -: 1667:    return op.emitError("expected type to be ")
    #####: 1668:           << expectedType << " or a rank-reduced version. (size mismatch) ";
    #####: 1669:  case SliceVerificationResult::ElemTypeMismatch:
        -: 1670:    return op.emitError("expected element type to be ")
    #####: 1671:           << memrefType.getElementType();
    #####: 1672:  default:
    #####: 1673:    llvm_unreachable("unexpected extract_slice op verification result");
        -: 1674:  }
        -: 1675:}
------------------
_Z20produceSliceErrorMsgIN4mlir6tensor21ParallelInsertSliceOpEENS0_13LogicalResultENS0_23SliceVerificationResultET_NS0_4TypeE:
function _Z20produceSliceErrorMsgIN4mlir6tensor21ParallelInsertSliceOpEENS0_13LogicalResultENS0_23SliceVerificationResultET_NS0_4TypeE called 0 returned 0% blocks executed 0%
    #####: 1657:static LogicalResult produceSliceErrorMsg(SliceVerificationResult result,
call    0 never executed
        -: 1658:                                          OpTy op, Type expectedType) {
    #####: 1659:  auto memrefType = expectedType.cast<ShapedType>();
    #####: 1660:  switch (result) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1661:  case SliceVerificationResult::Success:
    #####: 1662:    return success();
    #####: 1663:  case SliceVerificationResult::RankTooLarge:
        -: 1664:    return op.emitError("expected rank to be smaller or equal to ")
    #####: 1665:           << "the other rank. ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1666:  case SliceVerificationResult::SizeMismatch:
        -: 1667:    return op.emitError("expected type to be ")
    #####: 1668:           << expectedType << " or a rank-reduced version. (size mismatch) ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1669:  case SliceVerificationResult::ElemTypeMismatch:
        -: 1670:    return op.emitError("expected element type to be ")
    #####: 1671:           << memrefType.getElementType();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1672:  default:
    #####: 1673:    llvm_unreachable("unexpected extract_slice op verification result");
call    0 never executed
        -: 1674:  }
        -: 1675:}
------------------
_Z20produceSliceErrorMsgIN4mlir6tensor13InsertSliceOpEENS0_13LogicalResultENS0_23SliceVerificationResultET_NS0_4TypeE:
function _Z20produceSliceErrorMsgIN4mlir6tensor13InsertSliceOpEENS0_13LogicalResultENS0_23SliceVerificationResultET_NS0_4TypeE called 0 returned 0% blocks executed 0%
    #####: 1657:static LogicalResult produceSliceErrorMsg(SliceVerificationResult result,
call    0 never executed
        -: 1658:                                          OpTy op, Type expectedType) {
    #####: 1659:  auto memrefType = expectedType.cast<ShapedType>();
    #####: 1660:  switch (result) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1661:  case SliceVerificationResult::Success:
    #####: 1662:    return success();
    #####: 1663:  case SliceVerificationResult::RankTooLarge:
        -: 1664:    return op.emitError("expected rank to be smaller or equal to ")
    #####: 1665:           << "the other rank. ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1666:  case SliceVerificationResult::SizeMismatch:
        -: 1667:    return op.emitError("expected type to be ")
    #####: 1668:           << expectedType << " or a rank-reduced version. (size mismatch) ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1669:  case SliceVerificationResult::ElemTypeMismatch:
        -: 1670:    return op.emitError("expected element type to be ")
    #####: 1671:           << memrefType.getElementType();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1672:  default:
    #####: 1673:    llvm_unreachable("unexpected extract_slice op verification result");
call    0 never executed
        -: 1674:  }
        -: 1675:}
------------------
_Z20produceSliceErrorMsgIN4mlir6tensor14ExtractSliceOpEENS0_13LogicalResultENS0_23SliceVerificationResultET_NS0_4TypeE:
function _Z20produceSliceErrorMsgIN4mlir6tensor14ExtractSliceOpEENS0_13LogicalResultENS0_23SliceVerificationResultET_NS0_4TypeE called 0 returned 0% blocks executed 0%
    #####: 1657:static LogicalResult produceSliceErrorMsg(SliceVerificationResult result,
call    0 never executed
        -: 1658:                                          OpTy op, Type expectedType) {
    #####: 1659:  auto memrefType = expectedType.cast<ShapedType>();
    #####: 1660:  switch (result) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1661:  case SliceVerificationResult::Success:
    #####: 1662:    return success();
    #####: 1663:  case SliceVerificationResult::RankTooLarge:
        -: 1664:    return op.emitError("expected rank to be smaller or equal to ")
    #####: 1665:           << "the other rank. ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1666:  case SliceVerificationResult::SizeMismatch:
        -: 1667:    return op.emitError("expected type to be ")
    #####: 1668:           << expectedType << " or a rank-reduced version. (size mismatch) ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1669:  case SliceVerificationResult::ElemTypeMismatch:
        -: 1670:    return op.emitError("expected element type to be ")
    #####: 1671:           << memrefType.getElementType();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
    #####: 1672:  default:
    #####: 1673:    llvm_unreachable("unexpected extract_slice op verification result");
call    0 never executed
        -: 1674:  }
        -: 1675:}
------------------
        -: 1676:
        -: 1677:/// Verifier for ExtractSliceOp.
function _ZN4mlir6tensor14ExtractSliceOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 1678:LogicalResult ExtractSliceOp::verify() {
        -: 1679:  // Verify result type against inferred type.
    #####: 1680:  RankedTensorType expectedType = ExtractSliceOp::inferResultType(
    #####: 1681:      getSourceType(), getMixedOffsets(), getMixedSizes(), getMixedStrides());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
branch 10 never executed
branch 11 never executed
    #####: 1682:  SliceVerificationResult result = isRankReducedType(expectedType, getType());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 1683:  return produceSliceErrorMsg(result, *this, expectedType);
call    0 never executed
        -: 1684:}
        -: 1685:
function _ZN4mlir6tensor14ExtractSliceOp14getDroppedDimsEv called 0 returned 0% blocks executed 0%
    #####: 1686:llvm::SmallBitVector ExtractSliceOp::getDroppedDims() {
    #####: 1687:  ArrayRef<int64_t> resultShape = getType().getShape();
call    0 never executed
call    1 never executed
    #####: 1688:  SmallVector<OpFoldResult> mixedSizes = getMixedSizes();
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1689:  llvm::SmallBitVector droppedDims(mixedSizes.size());
call    0 never executed
    #####: 1690:  unsigned shapePos = 0;
    #####: 1691:  for (const auto &size : enumerate(mixedSizes)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1692:    Optional<int64_t> sizeVal = getConstantIntValue(size.value());
call    0 never executed
        -: 1693:    // If the size is not 1, or if the current matched dimension of the result
        -: 1694:    // is the same static shape as the size value (which is 1), then the
        -: 1695:    // dimension is preserved.
    #####: 1696:    if (!sizeVal || *sizeVal != 1 ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1697:        (shapePos < resultShape.size() && resultShape[shapePos] == 1)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1698:      shapePos++;
    #####: 1699:      continue;
        -: 1700:    }
    #####: 1701:    droppedDims.set(size.index());
call    0 never executed
        -: 1702:  }
    #####: 1703:  return droppedDims;
branch  0 never executed
branch  1 never executed
        -: 1704:}
        -: 1705:
function _ZN4mlir6tensor14ExtractSliceOp17reifyResultShapesERNS_9OpBuilderERN4llvm11SmallVectorINS5_INS_5ValueELj6EEELj1EEE called 0 returned 0% blocks executed 0%
    #####: 1706:LogicalResult ExtractSliceOp::reifyResultShapes(
        -: 1707:    OpBuilder &builder, ReifiedRankedShapedTypeDims &reifiedReturnShapes) {
    #####: 1708:  reifiedReturnShapes.resize(1);
call    0 never executed
    #####: 1709:  reifiedReturnShapes[0].reserve(getType().getRank());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1710:  SmallVector<OpFoldResult> mixedSizes = getMixedSizes();
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1711:  llvm::SmallBitVector droppedDims = getDroppedDims();
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1712:  Location loc = getLoc();
    #####: 1713:  for (const auto &size : enumerate(mixedSizes)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1714:    if (droppedDims.test(size.index()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1715:      continue;
    #####: 1716:    if (auto attr = size.value().dyn_cast<Attribute>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1717:      reifiedReturnShapes[0].push_back(builder.create<arith::ConstantIndexOp>(
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####: 1718:          loc, attr.cast<IntegerAttr>().getInt()));
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1719:      continue;
        -: 1720:    }
    #####: 1721:    reifiedReturnShapes[0].push_back(size.value().get<Value>());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -: 1722:  }
    #####: 1723:  return success();
call    0 never executed
        -: 1724:}
        -: 1725:
        -: 1726:namespace {
        -: 1727:/// Pattern to rewrite an extract_slice op with tensor::Cast arguments.
        -: 1728:/// This essentially pushes memref_cast past its consuming slice when
        -: 1729:/// `canFoldIntoConsumerOp` is true.
        -: 1730:///
        -: 1731:/// Example:
        -: 1732:/// ```
        -: 1733:///   %0 = tensor.cast %V : tensor<16x16xf32> to tensor<?x?xf32>
        -: 1734:///   %1 = tensor.extract_slice %0[0, 0][3, 4][1, 1] : tensor<?x?xf32> to
        -: 1735:///   tensor<3x4xf32>
        -: 1736:/// ```
        -: 1737:/// is rewritten into:
        -: 1738:/// ```
        -: 1739:///   %0 = tensor.extract_slice %V[0, 0][3, 4][1, 1] : tensor<16x16xf32> to
        -: 1740:///   tensor<3x4xf32> %1 = tensor.cast %0: tensor<3x4xf32> to tensor<3x4xf32>
        -: 1741:/// ```
        -: 1742:class ExtractSliceOpCastFolder final : public OpRewritePattern<ExtractSliceOp> {
        -: 1743:public:
        -: 1744:  using OpRewritePattern<ExtractSliceOp>::OpRewritePattern;
        -: 1745:
function _ZNK12_GLOBAL__N_124ExtractSliceOpCastFolder15matchAndRewriteEN4mlir6tensor14ExtractSliceOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1746:  LogicalResult matchAndRewrite(ExtractSliceOp sliceOp,
        -: 1747:                                PatternRewriter &rewriter) const override {
        -: 1748:    // Any constant operand, just return to let the constant folder kick in.
    #####: 1749:    if (llvm::any_of(sliceOp.getOperands(), [](Value operand) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1750:          return matchPattern(operand, matchConstantIndex());
        -: 1751:        }))
    #####: 1752:      return failure();
        -: 1753:
    #####: 1754:    auto castOp = sliceOp.getSource().getDefiningOp<tensor::CastOp>();
call    0 never executed
call    1 never executed
    #####: 1755:    if (!castOp)
branch  0 never executed
branch  1 never executed
    #####: 1756:      return failure();
        -: 1757:
    #####: 1758:    if (!canFoldIntoConsumerOp(castOp))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1759:      return failure();
        -: 1760:
        -: 1761:    /// Deduce the type of the result to use for the canonicalized operation.
    #####: 1762:    RankedTensorType resultType =
        -: 1763:        ExtractSliceOp::inferCanonicalRankReducedResultType(
    #####: 1764:            sliceOp.getType().getRank(), sliceOp.getSourceType(),
call    0 never executed
call    1 never executed
    #####: 1765:            sliceOp.getMixedOffsets(), sliceOp.getMixedSizes(),
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1766:            sliceOp.getMixedStrides());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
branch 10 never executed
    #####: 1767:    Value newSlice = rewriter.create<ExtractSliceOp>(
    #####: 1768:        sliceOp.getLoc(), resultType, castOp.getSource(), sliceOp.getOffsets(),
call    0 never executed
    #####: 1769:        sliceOp.getSizes(), sliceOp.getStrides(), sliceOp.getStaticOffsets(),
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1770:        sliceOp.getStaticSizes(), sliceOp.getStaticStrides());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 1771:    rewriter.replaceOpWithNewOp<tensor::CastOp>(sliceOp, sliceOp.getType(),
    #####: 1772:                                                newSlice);
call    0 never executed
call    1 never executed
    #####: 1773:    return success();
        -: 1774:  }
        -: 1775:};
        -: 1776:
        -: 1777:/// Slice elements from `values` into `outValues`. `counts` represents the
        -: 1778:/// numbers of elements to stride in the original values for each dimension.
        -: 1779:/// The output values can be used to construct a DenseElementsAttr.
        -: 1780:template <typename IterTy, typename ElemTy>
    #####: 1781:static void sliceElements(IterTy values, ArrayRef<int64_t> counts,
        -: 1782:                          ArrayRef<int64_t> offsets, ArrayRef<int64_t> sizes,
        -: 1783:                          ArrayRef<int64_t> strides,
        -: 1784:                          llvm::SmallVectorImpl<ElemTy> *outValues) {
    #####: 1785:  assert(offsets.size() == sizes.size());
    #####: 1786:  assert(offsets.size() == strides.size());
    #####: 1787:  if (offsets.empty())
        -: 1788:    return;
        -: 1789:
    #####: 1790:  int64_t offset = offsets.front();
    #####: 1791:  int64_t size = sizes.front();
    #####: 1792:  int64_t stride = strides.front();
    #####: 1793:  if (offsets.size() == 1) {
    #####: 1794:    for (int64_t i = 0; i < size; ++i, offset += stride)
    #####: 1795:      outValues->push_back(*(values + offset));
        -: 1796:
        -: 1797:    return;
        -: 1798:  }
        -: 1799:
    #####: 1800:  for (int64_t i = 0; i < size; ++i, offset += stride) {
    #####: 1801:    auto begin = values + offset * counts.front();
    #####: 1802:    sliceElements<IterTy, ElemTy>(begin, counts.drop_front(),
        -: 1803:                                  offsets.drop_front(), sizes.drop_front(),
        -: 1804:                                  strides.drop_front(), outValues);
        -: 1805:  }
        -: 1806:}
------------------
_ZN12_GLOBAL__N_1L13sliceElementsIN4mlir17DenseElementsAttr20FloatElementIteratorEN4llvm7APFloatEEEvT_NS4_8ArrayRefIlEES8_S8_S8_PNS4_15SmallVectorImplIT0_EE:
function _ZN12_GLOBAL__N_1L13sliceElementsIN4mlir17DenseElementsAttr20FloatElementIteratorEN4llvm7APFloatEEEvT_NS4_8ArrayRefIlEES8_S8_S8_PNS4_15SmallVectorImplIT0_EE called 0 returned 0% blocks executed 0%
    #####: 1781:static void sliceElements(IterTy values, ArrayRef<int64_t> counts,
        -: 1782:                          ArrayRef<int64_t> offsets, ArrayRef<int64_t> sizes,
        -: 1783:                          ArrayRef<int64_t> strides,
        -: 1784:                          llvm::SmallVectorImpl<ElemTy> *outValues) {
    #####: 1785:  assert(offsets.size() == sizes.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1786:  assert(offsets.size() == strides.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1787:  if (offsets.empty())
branch  0 never executed
branch  1 never executed
        -: 1788:    return;
        -: 1789:
    #####: 1790:  int64_t offset = offsets.front();
branch  0 never executed
branch  1 never executed
    #####: 1791:  int64_t size = sizes.front();
branch  0 never executed
branch  1 never executed
    #####: 1792:  int64_t stride = strides.front();
    #####: 1793:  if (offsets.size() == 1) {
branch  0 never executed
branch  1 never executed
    #####: 1794:    for (int64_t i = 0; i < size; ++i, offset += stride)
branch  0 never executed
branch  1 never executed
    #####: 1795:      outValues->push_back(*(values + offset));
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1796:
        -: 1797:    return;
        -: 1798:  }
        -: 1799:
    #####: 1800:  for (int64_t i = 0; i < size; ++i, offset += stride) {
branch  0 never executed
branch  1 never executed
    #####: 1801:    auto begin = values + offset * counts.front();
call    0 never executed
    #####: 1802:    sliceElements<IterTy, ElemTy>(begin, counts.drop_front(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -: 1803:                                  offsets.drop_front(), sizes.drop_front(),
        -: 1804:                                  strides.drop_front(), outValues);
        -: 1805:  }
        -: 1806:}
------------------
_ZN12_GLOBAL__N_1L13sliceElementsIN4mlir17DenseElementsAttr18IntElementIteratorEN4llvm5APIntEEEvT_NS4_8ArrayRefIlEES8_S8_S8_PNS4_15SmallVectorImplIT0_EE:
function _ZN12_GLOBAL__N_1L13sliceElementsIN4mlir17DenseElementsAttr18IntElementIteratorEN4llvm5APIntEEEvT_NS4_8ArrayRefIlEES8_S8_S8_PNS4_15SmallVectorImplIT0_EE called 0 returned 0% blocks executed 0%
    #####: 1781:static void sliceElements(IterTy values, ArrayRef<int64_t> counts,
        -: 1782:                          ArrayRef<int64_t> offsets, ArrayRef<int64_t> sizes,
        -: 1783:                          ArrayRef<int64_t> strides,
        -: 1784:                          llvm::SmallVectorImpl<ElemTy> *outValues) {
    #####: 1785:  assert(offsets.size() == sizes.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1786:  assert(offsets.size() == strides.size());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1787:  if (offsets.empty())
branch  0 never executed
branch  1 never executed
        -: 1788:    return;
        -: 1789:
    #####: 1790:  int64_t offset = offsets.front();
branch  0 never executed
branch  1 never executed
    #####: 1791:  int64_t size = sizes.front();
branch  0 never executed
branch  1 never executed
    #####: 1792:  int64_t stride = strides.front();
    #####: 1793:  if (offsets.size() == 1) {
branch  0 never executed
branch  1 never executed
    #####: 1794:    for (int64_t i = 0; i < size; ++i, offset += stride)
branch  0 never executed
branch  1 never executed
    #####: 1795:      outValues->push_back(*(values + offset));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1796:
        -: 1797:    return;
        -: 1798:  }
        -: 1799:
    #####: 1800:  for (int64_t i = 0; i < size; ++i, offset += stride) {
branch  0 never executed
branch  1 never executed
    #####: 1801:    auto begin = values + offset * counts.front();
call    0 never executed
    #####: 1802:    sliceElements<IterTy, ElemTy>(begin, counts.drop_front(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -: 1803:                                  offsets.drop_front(), sizes.drop_front(),
        -: 1804:                                  strides.drop_front(), outValues);
        -: 1805:  }
        -: 1806:}
------------------
        -: 1807:
        -: 1808:/// Fold arith.constant and tensor.extract_slice into arith.constant. The
        -: 1809:/// folded operation might introduce more constant data; Users can control
        -: 1810:/// their heuristics by the control function.
        -: 1811:class ConstantOpExtractSliceFolder final
        -: 1812:    : public OpRewritePattern<ExtractSliceOp> {
        -: 1813:public:
        -: 1814:  using OpRewritePattern<ExtractSliceOp>::OpRewritePattern;
        -: 1815:
function _ZN12_GLOBAL__N_128ConstantOpExtractSliceFolderC2EPN4mlir11MLIRContextESt8functionIFbNS1_6tensor14ExtractSliceOpEEE called 0 returned 0% blocks executed 0%
    #####: 1816:  ConstantOpExtractSliceFolder(MLIRContext *context,
        -: 1817:                               ControlConstantExtractSliceFusionFn controlFn)
    #####: 1818:      : OpRewritePattern<ExtractSliceOp>(context),
    #####: 1819:        controlFn(std::move(controlFn)) {}
call    0 never executed
call    1 never executed
        -: 1820:
function _ZNK12_GLOBAL__N_128ConstantOpExtractSliceFolder15matchAndRewriteEN4mlir6tensor14ExtractSliceOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1821:  LogicalResult matchAndRewrite(ExtractSliceOp op,
        -: 1822:                                PatternRewriter &rewriter) const override {
    #####: 1823:    DenseElementsAttr attr;
    #####: 1824:    if (!matchPattern(op.getSource(), m_Constant(&attr)))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1825:      return failure();
        -: 1826:
        -: 1827:    // A constant splat is handled by fold().
    #####: 1828:    if (attr.isSplat())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1829:      return failure();
        -: 1830:
        -: 1831:    // Dynamic result shape is not supported.
    #####: 1832:    auto sourceType = op.getSource().getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1833:    auto resultType = op.getResult().getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1834:    if (!sourceType.hasStaticShape() || !resultType.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1835:      return failure();
        -: 1836:
        -: 1837:    // Customized control over the folding.
    #####: 1838:    if (!controlFn(op))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1839:      return failure();
        -: 1840:
    #####: 1841:    int64_t count = sourceType.getNumElements();
call    0 never executed
    #####: 1842:    if (count == 0)
branch  0 never executed
branch  1 never executed
    #####: 1843:      return failure();
        -: 1844:
        -: 1845:    // Check if there are any dynamic parts, which are not supported.
    #####: 1846:    auto offsets = extractFromI64ArrayAttr(op.getStaticOffsets());
call    0 never executed
call    1 never executed
    #####: 1847:    if (llvm::is_contained(offsets, ShapedType::kDynamicStrideOrOffset))
branch  0 never executed
branch  1 never executed
    #####: 1848:      return failure();
    #####: 1849:    auto sizes = extractFromI64ArrayAttr(op.getStaticSizes());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1850:    if (llvm::is_contained(sizes, ShapedType::kDynamicSize))
branch  0 never executed
branch  1 never executed
    #####: 1851:      return failure();
    #####: 1852:    auto strides = extractFromI64ArrayAttr(op.getStaticStrides());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1853:    if (llvm::is_contained(strides, ShapedType::kDynamicStrideOrOffset))
branch  0 never executed
branch  1 never executed
    #####: 1854:      return failure();
        -: 1855:
        -: 1856:    // Compute the stride for each dimension.
    #####: 1857:    SmallVector<int64_t> counts;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1858:    ArrayRef<int64_t> shape = sourceType.getShape();
call    0 never executed
    #####: 1859:    counts.reserve(shape.size());
branch  0 never executed
branch  1 never executed
    #####: 1860:    for (int64_t v : shape) {
branch  0 never executed
branch  1 never executed
    #####: 1861:      count = count / v;
    #####: 1862:      counts.push_back(count);
call    0 never executed
        -: 1863:    }
        -: 1864:
        -: 1865:    // New attribute constructed by the sliced values.
    #####: 1866:    DenseElementsAttr newAttr;
        -: 1867:
    #####: 1868:    if (auto elems = attr.dyn_cast<DenseIntElementsAttr>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1869:      SmallVector<APInt> outValues;
call    0 never executed
    #####: 1870:      outValues.reserve(sourceType.getNumElements());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1871:      sliceElements<DenseElementsAttr::IntElementIterator, APInt>(
call    0 never executed
call    1 never executed
        -: 1872:          elems.begin(), counts, offsets, sizes, strides, &outValues);
    #####: 1873:      newAttr = DenseElementsAttr::get(resultType, outValues);
call    0 never executed
call    1 never executed
    #####: 1874:    } else if (auto elems = attr.dyn_cast<DenseFPElementsAttr>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1875:      SmallVector<APFloat> outValues;
call    0 never executed
    #####: 1876:      outValues.reserve(sourceType.getNumElements());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1877:      sliceElements<DenseElementsAttr::FloatElementIterator, APFloat>(
call    0 never executed
call    1 never executed
        -: 1878:          elems.begin(), counts, offsets, sizes, strides, &outValues);
    #####: 1879:      newAttr = DenseElementsAttr::get(resultType, outValues);
call    0 never executed
call    1 never executed
        -: 1880:    }
        -: 1881:
    #####: 1882:    if (newAttr) {
branch  0 never executed
branch  1 never executed
    #####: 1883:      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, resultType, newAttr);
call    0 never executed
    #####: 1884:      return success();
        -: 1885:    }
        -: 1886:
    #####: 1887:    return failure();
branch  0 never executed
branch  1 never executed
        -: 1888:  }
        -: 1889:
        -: 1890:private:
        -: 1891:  /// This additionally controls whether the fold happens or not. Users can
        -: 1892:  /// impose their heuristics in the function.
        -: 1893:  ControlConstantExtractSliceFusionFn controlFn;
        -: 1894:};
        -: 1895:
        -: 1896:} // namespace
        -: 1897:
function _ZN4mlir6tensor40populateFoldConstantExtractSlicePatternsERNS_17RewritePatternSetERKSt8functionIFbNS0_14ExtractSliceOpEEE called 0 returned 0% blocks executed 0%
    #####: 1898:void mlir::tensor::populateFoldConstantExtractSlicePatterns(
        -: 1899:    RewritePatternSet &patterns,
        -: 1900:    const ControlConstantExtractSliceFusionFn &controlFn) {
    #####: 1901:  patterns.add<ConstantOpExtractSliceFolder>(patterns.getContext(), controlFn);
call    0 never executed
    #####: 1902:}
        -: 1903:
        -: 1904:/// Return the canonical type of the result of an extract_slice op.
        -: 1905:struct SliceReturnTypeCanonicalizer {
        -: 1906:  RankedTensorType operator()(ExtractSliceOp op,
        -: 1907:                              ArrayRef<OpFoldResult> mixedOffsets,
        -: 1908:                              ArrayRef<OpFoldResult> mixedSizes,
        -: 1909:                              ArrayRef<OpFoldResult> mixedStrides) {
        -: 1910:    return ExtractSliceOp::inferCanonicalRankReducedResultType(
        -: 1911:        op.getType().getRank(), op.getSourceType(), mixedOffsets, mixedSizes,
        -: 1912:        mixedStrides);
        -: 1913:  }
        -: 1914:};
        -: 1915:
        -: 1916:/// A canonicalizer wrapper to replace ExtractSliceOps.
        -: 1917:struct SliceCanonicalizer {
function _ZN18SliceCanonicalizerclERN4mlir15PatternRewriterENS0_6tensor14ExtractSliceOpES4_ called 0 returned 0% blocks executed 0%
    #####: 1918:  void operator()(PatternRewriter &rewriter, ExtractSliceOp op,
        -: 1919:                  ExtractSliceOp newOp) {
    #####: 1920:    Value replacement = newOp.getResult();
call    0 never executed
    #####: 1921:    if (replacement.getType() != op.getType())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1922:      replacement = rewriter.create<tensor::CastOp>(op.getLoc(), op.getType(),
    #####: 1923:                                                    replacement);
call    0 never executed
call    1 never executed
    #####: 1924:    rewriter.replaceOp(op, replacement);
call    0 never executed
call    1 never executed
    #####: 1925:  }
        -: 1926:};
        -: 1927:
function _ZN4mlir6tensor14ExtractSliceOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1261 returned 100% blocks executed 100%
     1261: 1928:void ExtractSliceOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 1929:                                                 MLIRContext *context) {
     1261: 1930:  results.add<
        -: 1931:      OpWithOffsetSizesAndStridesConstantArgumentFolder<
        -: 1932:          ExtractSliceOp, SliceReturnTypeCanonicalizer, SliceCanonicalizer>,
     1261: 1933:      ExtractSliceOpCastFolder>(context);
call    0 returned 100%
     1261: 1934:}
        -: 1935:
        -: 1936://
        -: 1937:static LogicalResult
function _ZL42foldIdentityOffsetSizeAndStrideOpInterfaceN4mlir30OffsetSizeAndStrideOpInterfaceENS_10ShapedTypeE called 0 returned 0% blocks executed 0%
    #####: 1938:foldIdentityOffsetSizeAndStrideOpInterface(OffsetSizeAndStrideOpInterface op,
        -: 1939:                                           ShapedType shapedType) {
    #####: 1940:  OpBuilder b(op.getContext());
call    0 never executed
call    1 never executed
    #####: 1941:  for (OpFoldResult ofr : op.getMixedOffsets())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1942:    if (getConstantIntValue(ofr) != static_cast<int64_t>(0))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1943:      return failure();
branch  0 never executed
branch  1 never executed
        -: 1944:  // Rank-reducing noops only need to inspect the leading dimensions:
        -: 1945:  // llvm::zip is appropriate.
    #####: 1946:  auto shape = shapedType.getShape();
call    0 never executed
    #####: 1947:  for (auto it : llvm::zip(op.getMixedSizes(), shape))
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 1948:    if (getConstantIntValue(std::get<0>(it)) != std::get<1>(it))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1949:      return failure();
branch  0 never executed
branch  1 never executed
    #####: 1950:  for (OpFoldResult ofr : op.getMixedStrides())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1951:    if (getConstantIntValue(ofr) != static_cast<int64_t>(1))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1952:      return failure();
branch  0 never executed
branch  1 never executed
    #####: 1953:  return success();
        -: 1954:}
        -: 1955:
        -: 1956:/// If we have an ExtractSliceOp consuming an InsertSliceOp with the same
        -: 1957:/// slice, we can return the InsertSliceOp's source directly.
        -: 1958:// TODO: This only checks the immediate producer; extend to go up the
        -: 1959:// insert/extract chain if the slices are disjoint.
function _ZL27foldExtractAfterInsertSliceN4mlir6tensor14ExtractSliceOpE called 0 returned 0% blocks executed 0%
    #####: 1960:static Value foldExtractAfterInsertSlice(ExtractSliceOp extractOp) {
    #####: 1961:  auto insertOp = extractOp.getSource().getDefiningOp<InsertSliceOp>();
call    0 never executed
call    1 never executed
        -: 1962:
    #####: 1963:  auto isSame = [](OpFoldResult a, OpFoldResult b) { return a == b; };
    #####: 1964:  if (insertOp && insertOp.getSource().getType() == extractOp.getType() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####: 1965:      insertOp.isSameAs(extractOp, isSame))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1966:    return insertOp.getSource();
call    0 never executed
        -: 1967:
    #####: 1968:  return {};
        -: 1969:}
        -: 1970:
function _ZN4mlir6tensor14ExtractSliceOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 0 returned 0% blocks executed 0%
    #####: 1971:OpFoldResult ExtractSliceOp::fold(ArrayRef<Attribute> operands) {
    #####: 1972:  if (auto splat = operands[0].dyn_cast_or_null<SplatElementsAttr>()) {
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1973:    auto resultType = getResult().getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1974:    if (resultType.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1975:      return splat.resizeSplat(resultType);
call    0 never executed
call    1 never executed
        -: 1976:  }
    #####: 1977:  if (getSourceType() == getType() &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1978:      succeeded(foldIdentityOffsetSizeAndStrideOpInterface(*this, getType())))
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 1979:    return this->getSource();
call    0 never executed
call    1 never executed
    #####: 1980:  if (Value slice = foldExtractAfterInsertSlice(*this))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1981:    return slice;
call    0 never executed
        -: 1982:
    #####: 1983:  return OpFoldResult();
        -: 1984:}
        -: 1985:
function _ZN4mlir6tensor41createCanonicalRankReducingExtractSliceOpERNS_9OpBuilderENS_8LocationENS_5ValueENS_16RankedTensorTypeE called 0 returned 0% blocks executed 0%
    #####: 1986:Value mlir::tensor::createCanonicalRankReducingExtractSliceOp(
        -: 1987:    OpBuilder &b, Location loc, Value tensor, RankedTensorType targetType) {
    #####: 1988:  auto rankedTensorType = tensor.getType().cast<RankedTensorType>();
call    0 never executed
    #####: 1989:  unsigned rank = rankedTensorType.getRank();
call    0 never executed
    #####: 1990:  SmallVector<OpFoldResult> offsets(rank, b.getIndexAttr(0));
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1991:  SmallVector<OpFoldResult> sizes = getMixedSizes(b, loc, tensor);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1992:  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1993:  return b.createOrFold<tensor::ExtractSliceOp>(loc, targetType, tensor,
    #####: 1994:                                                offsets, sizes, strides);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1995:}
        -: 1996:
        -: 1997://===----------------------------------------------------------------------===//
        -: 1998:// InsertSliceOp
        -: 1999://===----------------------------------------------------------------------===//
        -: 2000:
function _ZN4mlir6tensor13InsertSliceOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####: 2001:void InsertSliceOp::getAsmResultNames(
        -: 2002:    function_ref<void(Value, StringRef)> setNameFn) {
    #####: 2003:  setNameFn(getResult(), "inserted_slice");
call    0 never executed
call    1 never executed
    #####: 2004:}
        -: 2005:
        -: 2006:// Build a InsertSliceOp with mixed static and dynamic entries.
function _ZN4mlir6tensor13InsertSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_N4llvm8ArrayRefINS_12OpFoldResultEEESA_SA_NS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2007:void InsertSliceOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 2008:                          Value dest, ArrayRef<OpFoldResult> offsets,
        -: 2009:                          ArrayRef<OpFoldResult> sizes,
        -: 2010:                          ArrayRef<OpFoldResult> strides,
        -: 2011:                          ArrayRef<NamedAttribute> attrs) {
    #####: 2012:  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2013:  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2014:  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets,
call    0 never executed
        -: 2015:                             ShapedType::kDynamicStrideOrOffset);
    #####: 2016:  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes,
call    0 never executed
        -: 2017:                             ShapedType::kDynamicSize);
    #####: 2018:  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides,
call    0 never executed
        -: 2019:                             ShapedType::kDynamicStrideOrOffset);
    #####: 2020:  build(b, result, dest.getType(), source, dest, dynamicOffsets, dynamicSizes,
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
        -: 2021:        dynamicStrides, b.getI64ArrayAttr(staticOffsets),
        -: 2022:        b.getI64ArrayAttr(staticSizes), b.getI64ArrayAttr(staticStrides));
    #####: 2023:  result.addAttributes(attrs);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2024:}
        -: 2025:
        -: 2026:/// Build an InsertSliceOp with mixed static and dynamic entries packed into a
        -: 2027:/// Range vector.
function _ZN4mlir6tensor13InsertSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_N4llvm8ArrayRefINS_5RangeEEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2028:void InsertSliceOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 2029:                          Value dest, ArrayRef<Range> ranges,
        -: 2030:                          ArrayRef<NamedAttribute> attrs) {
    #####: 2031:  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
call    0 never executed
call    1 never executed
    #####: 2032:  build(b, result, source, dest, offsets, sizes, strides, attrs);
call    0 never executed
call    1 never executed
    #####: 2033:}
        -: 2034:
        -: 2035:// Build a InsertSliceOp with dynamic entries.
function _ZN4mlir6tensor13InsertSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_NS_10ValueRangeES7_S7_N4llvm8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2036:void InsertSliceOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 2037:                          Value dest, ValueRange offsets, ValueRange sizes,
        -: 2038:                          ValueRange strides, ArrayRef<NamedAttribute> attrs) {
    #####: 2039:  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 2040:      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2041:  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 2042:      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2043:  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 2044:      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2045:  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2046:}
        -: 2047:
        -: 2048:/// Rank-reducing type verification for both InsertSliceOp and
        -: 2049:/// ParallelInsertSliceOp.
        -: 2050:static SliceVerificationResult
function _ZL19verifyInsertSliceOpN4mlir10ShapedTypeES0_NS_9ArrayAttrES1_S1_PS0_ called 0 returned 0% blocks executed 0%
    #####: 2051:verifyInsertSliceOp(ShapedType srcType, ShapedType dstType,
        -: 2052:                    ArrayAttr staticOffsets, ArrayAttr staticSizes,
        -: 2053:                    ArrayAttr staticStrides,
        -: 2054:                    ShapedType *expectedType = nullptr) {
        -: 2055:  // insert_slice is the inverse of extract_slice, use the same type
        -: 2056:  // inference.
    #####: 2057:  RankedTensorType expected = ExtractSliceOp::inferResultType(
    #####: 2058:      dstType, extractFromI64ArrayAttr(staticOffsets),
call    0 never executed
    #####: 2059:      extractFromI64ArrayAttr(staticSizes),
call    0 never executed
call    1 never executed
    #####: 2060:      extractFromI64ArrayAttr(staticStrides));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
    #####: 2061:  if (expectedType)
branch  0 never executed
branch  1 never executed
    #####: 2062:    *expectedType = expected;
call    0 never executed
    #####: 2063:  return isRankReducedType(expected, srcType);
call    0 never executed
call    1 never executed
        -: 2064:}
        -: 2065:
        -: 2066:/// Verifier for InsertSliceOp.
function _ZN4mlir6tensor13InsertSliceOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 2067:LogicalResult InsertSliceOp::verify() {
    #####: 2068:  ShapedType expectedType;
call    0 never executed
    #####: 2069:  SliceVerificationResult result =
call    0 never executed
    #####: 2070:      verifyInsertSliceOp(getSourceType(), getType(), getStaticOffsets(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
        -: 2071:                          getStaticSizes(), getStaticStrides(), &expectedType);
    #####: 2072:  return produceSliceErrorMsg(result, *this, expectedType);
call    0 never executed
        -: 2073:}
        -: 2074:
        -: 2075:/// If we have two consecutive InsertSliceOp writing to the same slice, we
        -: 2076:/// can mutate the second InsertSliceOp's destination to the first one's.
        -: 2077:///
        -: 2078:/// Example:
        -: 2079:///
        -: 2080:/// ```mlir
        -: 2081:///   %0 = tensor.insert_slice %slice0 into %input[0, 0] [64, 64] [1, 1]
        -: 2082:///   %1 = tensor.insert_slice %slice1 into %0[0, 0] [64, 64] [1, 1]
        -: 2083:/// ```
        -: 2084:///
        -: 2085:/// folds into:
        -: 2086:///
        -: 2087:/// ```mlir
        -: 2088:///   %1 = tensor.insert_slice %slice1 into %input[0, 0] [64, 64] [1, 1]
        -: 2089:/// ```
        -: 2090:///
        -: 2091:/// This pattern works with both InsertSliceOp and ParallelInsertSliceOp.
function _ZL26foldInsertAfterInsertSliceN4mlir6tensor13InsertSliceOpE called 0 returned 0% blocks executed 0%
    #####: 2092:static LogicalResult foldInsertAfterInsertSlice(InsertSliceOp insertOp) {
    #####: 2093:  auto prevInsertOp = insertOp.getDest().getDefiningOp<InsertSliceOp>();
call    0 never executed
call    1 never executed
        -: 2094:
    #####: 2095:  auto isSame = [](OpFoldResult a, OpFoldResult b) { return a == b; };
    #####: 2096:  if (!prevInsertOp ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2097:      prevInsertOp.getSource().getType() != insertOp.getSource().getType() ||
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2098:      !prevInsertOp.isSameAs(insertOp, isSame))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2099:    return failure();
        -: 2100:
    #####: 2101:  insertOp.getDestMutable().assign(prevInsertOp.getDest());
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2102:  return success();
        -: 2103:}
        -: 2104:
        -: 2105:/// Folds round-trip extract/insert slice op pairs.
        -: 2106:/// Example:
        -: 2107:/// ```mlir
        -: 2108:/// %0 = tensor.extract_slice %val[0, 0, 0, 0] [1, 1, 2, 4] [1, 1, 1, 1]
        -: 2109:/// %1 = tensor.insert_slice %0 into %val[0, 0, 0, 0] [1, 1, 2, 4] [1, 1, 1, 1]
        -: 2110:/// ```
        -: 2111:/// can be folded into %val.
function _ZL27foldInsertAfterExtractSliceN4mlir6tensor13InsertSliceOpE called 0 returned 0% blocks executed 0%
    #####: 2112:static Value foldInsertAfterExtractSlice(InsertSliceOp insertOp) {
    #####: 2113:  auto extractOp = insertOp.getSource().getDefiningOp<ExtractSliceOp>();
call    0 never executed
call    1 never executed
        -: 2114:
    #####: 2115:  auto isSame = [](OpFoldResult a, OpFoldResult b) { return a == b; };
    #####: 2116:  if (!extractOp || extractOp.getSource() != insertOp.getDest() ||
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2117:      !extractOp.isSameAs(insertOp, isSame))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2118:    return nullptr;
        -: 2119:
    #####: 2120:  return extractOp.getSource();
call    0 never executed
        -: 2121:}
        -: 2122:
function _ZN4mlir6tensor13InsertSliceOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2123:OpFoldResult InsertSliceOp::fold(ArrayRef<Attribute>) {
    #####: 2124:  if (getSourceType().hasStaticShape() && getType().hasStaticShape() &&
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2125:      getSourceType() == getType() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2126:      succeeded(foldIdentityOffsetSizeAndStrideOpInterface(*this, getType())))
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2127:    return this->getSource();
call    0 never executed
call    1 never executed
    #####: 2128:  if (succeeded(foldInsertAfterInsertSlice(*this)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2129:    return getResult();
call    0 never executed
call    1 never executed
    #####: 2130:  if (auto result = foldInsertAfterExtractSlice(*this))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2131:    return result;
call    0 never executed
    #####: 2132:  return OpFoldResult();
        -: 2133:}
        -: 2134:
function _ZN4mlir6tensor13InsertSliceOp17reifyResultShapesERNS_9OpBuilderERN4llvm11SmallVectorINS5_INS_5ValueELj6EEELj1EEE called 0 returned 0% blocks executed 0%
    #####: 2135:LogicalResult InsertSliceOp::reifyResultShapes(
        -: 2136:    OpBuilder &builder, ReifiedRankedShapedTypeDims &reifiedReturnShapes) {
    #####: 2137:  reifiedReturnShapes.resize(1, SmallVector<Value>(getType().getRank()));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2138:  for (auto dim : llvm::seq<int64_t>(0, getType().getRank())) {
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2139:    reifiedReturnShapes[0][dim] =
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2140:        builder.createOrFold<tensor::DimOp>(getLoc(), getDest(), dim);
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2141:  }
    #####: 2142:  return success();
        -: 2143:}
        -: 2144:
        -: 2145:namespace {
        -: 2146:/// Pattern to rewrite a insert_slice op with constant arguments.
        -: 2147:///
        -: 2148:/// This pattern works with both InsertSliceOp and ParallelInsertSliceOp.
        -: 2149:template <typename InsertOpTy>
        -: 2150:class InsertSliceOpConstantArgumentFolder final
        -: 2151:    : public OpRewritePattern<InsertOpTy> {
        -: 2152:public:
        -: 2153:  using OpRewritePattern<InsertOpTy>::OpRewritePattern;
        -: 2154:
    #####: 2155:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
        -: 2156:                                PatternRewriter &rewriter) const override {
        -: 2157:    // No constant operand, just return.
    #####: 2158:    if (llvm::none_of(insertSliceOp.getOperands(), [](Value operand) {
        -: 2159:          return matchPattern(operand, matchConstantIndex());
        -: 2160:        }))
    #####: 2161:      return failure();
        -: 2162:
        -: 2163:    // At least one of offsets/sizes/strides is a new constant.
        -: 2164:    // Form the new list of operands and constant attributes from the
        -: 2165:    // existing.
    #####: 2166:    SmallVector<OpFoldResult> mixedOffsets(insertSliceOp.getMixedOffsets());
    #####: 2167:    SmallVector<OpFoldResult> mixedSizes(insertSliceOp.getMixedSizes());
    #####: 2168:    SmallVector<OpFoldResult> mixedStrides(insertSliceOp.getMixedStrides());
    #####: 2169:    canonicalizeSubViewPart(mixedOffsets, ShapedType::isDynamicStrideOrOffset);
    #####: 2170:    canonicalizeSubViewPart(mixedSizes, ShapedType::isDynamic);
    #####: 2171:    canonicalizeSubViewPart(mixedStrides, ShapedType::isDynamicStrideOrOffset);
        -: 2172:
        -: 2173:    // Create the new op in canonical form.
    #####: 2174:    auto sourceType = ExtractSliceOp::inferCanonicalRankReducedResultType(
    #####: 2175:        insertSliceOp.getSourceType().getRank(), insertSliceOp.getDestType(),
        -: 2176:        mixedOffsets, mixedSizes, mixedStrides);
    #####: 2177:    Value toInsert = insertSliceOp.getSource();
    #####: 2178:    if (sourceType != insertSliceOp.getSourceType()) {
    #####: 2179:      OpBuilder::InsertionGuard g(rewriter);
        -: 2180:      // The only difference between InsertSliceOp and ParallelInsertSliceOp
        -: 2181:      // is the the insertion point is just before the ParallelCombiningOp in
        -: 2182:      // the parallel case.
        -: 2183:      if (std::is_same<InsertOpTy, ParallelInsertSliceOp>::value)
    #####: 2184:        rewriter.setInsertionPoint(insertSliceOp->getParentOp());
    #####: 2185:      toInsert = rewriter.create<tensor::CastOp>(insertSliceOp.getLoc(),
        -: 2186:                                                 sourceType, toInsert);
        -: 2187:    }
    #####: 2188:    rewriter.replaceOpWithNewOp<InsertOpTy>(
        -: 2189:        insertSliceOp, toInsert, insertSliceOp.getDest(), mixedOffsets,
        -: 2190:        mixedSizes, mixedStrides);
    #####: 2191:    return success();
        -: 2192:  }
------------------
_ZNK12_GLOBAL__N_135InsertSliceOpConstantArgumentFolderIN4mlir6tensor13InsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_135InsertSliceOpConstantArgumentFolderIN4mlir6tensor13InsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2155:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
call    0 never executed
        -: 2156:                                PatternRewriter &rewriter) const override {
        -: 2157:    // No constant operand, just return.
    #####: 2158:    if (llvm::none_of(insertSliceOp.getOperands(), [](Value operand) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2159:          return matchPattern(operand, matchConstantIndex());
        -: 2160:        }))
    #####: 2161:      return failure();
        -: 2162:
        -: 2163:    // At least one of offsets/sizes/strides is a new constant.
        -: 2164:    // Form the new list of operands and constant attributes from the
        -: 2165:    // existing.
    #####: 2166:    SmallVector<OpFoldResult> mixedOffsets(insertSliceOp.getMixedOffsets());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2167:    SmallVector<OpFoldResult> mixedSizes(insertSliceOp.getMixedSizes());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 2168:    SmallVector<OpFoldResult> mixedStrides(insertSliceOp.getMixedStrides());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2169:    canonicalizeSubViewPart(mixedOffsets, ShapedType::isDynamicStrideOrOffset);
call    0 never executed
call    1 never executed
    #####: 2170:    canonicalizeSubViewPart(mixedSizes, ShapedType::isDynamic);
call    0 never executed
call    1 never executed
    #####: 2171:    canonicalizeSubViewPart(mixedStrides, ShapedType::isDynamicStrideOrOffset);
call    0 never executed
call    1 never executed
        -: 2172:
        -: 2173:    // Create the new op in canonical form.
    #####: 2174:    auto sourceType = ExtractSliceOp::inferCanonicalRankReducedResultType(
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2175:        insertSliceOp.getSourceType().getRank(), insertSliceOp.getDestType(),
call    0 never executed
        -: 2176:        mixedOffsets, mixedSizes, mixedStrides);
    #####: 2177:    Value toInsert = insertSliceOp.getSource();
call    0 never executed
    #####: 2178:    if (sourceType != insertSliceOp.getSourceType()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2179:      OpBuilder::InsertionGuard g(rewriter);
call    0 never executed
        -: 2180:      // The only difference between InsertSliceOp and ParallelInsertSliceOp
        -: 2181:      // is the the insertion point is just before the ParallelCombiningOp in
        -: 2182:      // the parallel case.
        -: 2183:      if (std::is_same<InsertOpTy, ParallelInsertSliceOp>::value)
        -: 2184:        rewriter.setInsertionPoint(insertSliceOp->getParentOp());
    #####: 2185:      toInsert = rewriter.create<tensor::CastOp>(insertSliceOp.getLoc(),
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2186:                                                 sourceType, toInsert);
        -: 2187:    }
    #####: 2188:    rewriter.replaceOpWithNewOp<InsertOpTy>(
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 2189:        insertSliceOp, toInsert, insertSliceOp.getDest(), mixedOffsets,
        -: 2190:        mixedSizes, mixedStrides);
    #####: 2191:    return success();
branch  0 never executed
branch  1 never executed
        -: 2192:  }
------------------
_ZNK12_GLOBAL__N_135InsertSliceOpConstantArgumentFolderIN4mlir6tensor21ParallelInsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_135InsertSliceOpConstantArgumentFolderIN4mlir6tensor21ParallelInsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2155:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
call    0 never executed
        -: 2156:                                PatternRewriter &rewriter) const override {
        -: 2157:    // No constant operand, just return.
    #####: 2158:    if (llvm::none_of(insertSliceOp.getOperands(), [](Value operand) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2159:          return matchPattern(operand, matchConstantIndex());
        -: 2160:        }))
    #####: 2161:      return failure();
        -: 2162:
        -: 2163:    // At least one of offsets/sizes/strides is a new constant.
        -: 2164:    // Form the new list of operands and constant attributes from the
        -: 2165:    // existing.
    #####: 2166:    SmallVector<OpFoldResult> mixedOffsets(insertSliceOp.getMixedOffsets());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2167:    SmallVector<OpFoldResult> mixedSizes(insertSliceOp.getMixedSizes());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 2168:    SmallVector<OpFoldResult> mixedStrides(insertSliceOp.getMixedStrides());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2169:    canonicalizeSubViewPart(mixedOffsets, ShapedType::isDynamicStrideOrOffset);
call    0 never executed
call    1 never executed
    #####: 2170:    canonicalizeSubViewPart(mixedSizes, ShapedType::isDynamic);
call    0 never executed
call    1 never executed
    #####: 2171:    canonicalizeSubViewPart(mixedStrides, ShapedType::isDynamicStrideOrOffset);
call    0 never executed
call    1 never executed
        -: 2172:
        -: 2173:    // Create the new op in canonical form.
    #####: 2174:    auto sourceType = ExtractSliceOp::inferCanonicalRankReducedResultType(
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2175:        insertSliceOp.getSourceType().getRank(), insertSliceOp.getDestType(),
call    0 never executed
        -: 2176:        mixedOffsets, mixedSizes, mixedStrides);
    #####: 2177:    Value toInsert = insertSliceOp.getSource();
call    0 never executed
    #####: 2178:    if (sourceType != insertSliceOp.getSourceType()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2179:      OpBuilder::InsertionGuard g(rewriter);
branch  0 never executed
branch  1 never executed
        -: 2180:      // The only difference between InsertSliceOp and ParallelInsertSliceOp
        -: 2181:      // is the the insertion point is just before the ParallelCombiningOp in
        -: 2182:      // the parallel case.
        -: 2183:      if (std::is_same<InsertOpTy, ParallelInsertSliceOp>::value)
    #####: 2184:        rewriter.setInsertionPoint(insertSliceOp->getParentOp());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2185:      toInsert = rewriter.create<tensor::CastOp>(insertSliceOp.getLoc(),
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2186:                                                 sourceType, toInsert);
        -: 2187:    }
    #####: 2188:    rewriter.replaceOpWithNewOp<InsertOpTy>(
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 2189:        insertSliceOp, toInsert, insertSliceOp.getDest(), mixedOffsets,
        -: 2190:        mixedSizes, mixedStrides);
    #####: 2191:    return success();
branch  0 never executed
branch  1 never executed
        -: 2192:  }
------------------
        -: 2193:};
        -: 2194:
        -: 2195:/// Fold tensor_casts with insert_slice operations. If the source or
        -: 2196:/// destination tensor is a tensor_cast that removes static type information,
        -: 2197:/// the cast is folded into the insert_slice operation. E.g.:
        -: 2198:///
        -: 2199:/// ```mlir
        -: 2200:///   %1 = tensor.cast %0 : tensor<8x16xf32> to tensor<?x?xf32>
        -: 2201:///   %2 = tensor.insert_slice %1 into ... : tensor<?x?xf32> into ...
        -: 2202:/// ```
        -: 2203:///
        -: 2204:/// folds into:
        -: 2205:///
        -: 2206:/// ```mlir
        -: 2207:///   %2 = tensor.insert_slice %0 into ... : tensor<8x16xf32> into ...
        -: 2208:/// ```
        -: 2209:///
        -: 2210:/// Note: When folding a cast on the destination tensor, the result of the
        -: 2211:/// insert_slice operation is casted to ensure that the type of the result did
        -: 2212:/// not change.
        -: 2213:///
        -: 2214:/// This pattern works with both InsertSliceOp and ParallelInsertSliceOp.
        -: 2215:template <typename InsertOpTy>
        -: 2216:struct InsertSliceOpCastFolder final : public OpRewritePattern<InsertOpTy> {
        -: 2217:  using OpRewritePattern<InsertOpTy>::OpRewritePattern;
        -: 2218:
    #####: 2219:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
        -: 2220:                                PatternRewriter &rewriter) const override {
    #####: 2221:    if (llvm::any_of(insertSliceOp.getOperands(), [](Value operand) {
        -: 2222:          return matchPattern(operand, matchConstantIndex());
        -: 2223:        }))
    #####: 2224:      return failure();
        -: 2225:
    #####: 2226:    auto getSourceOfCastOp = [](Value v) -> Optional<Value> {
    #####: 2227:      auto castOp = v.getDefiningOp<tensor::CastOp>();
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2228:      if (!castOp || !canFoldIntoConsumerOp(castOp))
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
call    7 never executed
branch  8 never executed
branch  9 never executed
    #####: 2229:        return llvm::None;
    #####: 2230:      return castOp.getSource();
call    0 never executed
call    1 never executed
        -: 2231:    };
        -: 2232:    Optional<Value> sourceCastSource =
    #####: 2233:        getSourceOfCastOp(insertSliceOp.getSource());
    #####: 2234:    Optional<Value> destCastSource = getSourceOfCastOp(insertSliceOp.getDest());
    #####: 2235:    if (!sourceCastSource && !destCastSource)
    #####: 2236:      return failure();
        -: 2237:
    #####: 2238:    auto src =
    #####: 2239:        (sourceCastSource ? *sourceCastSource : insertSliceOp.getSource());
    #####: 2240:    auto dst = (destCastSource ? *destCastSource : insertSliceOp.getDest());
    #####: 2241:    auto srcType = src.getType().template cast<ShapedType>();
    #####: 2242:    auto dstType = dst.getType().template cast<ShapedType>();
    #####: 2243:    if (verifyInsertSliceOp(srcType, dstType, insertSliceOp.getStaticOffsets(),
        -: 2244:                            insertSliceOp.getStaticSizes(),
        -: 2245:                            insertSliceOp.getStaticStrides()) !=
        -: 2246:        SliceVerificationResult::Success)
    #####: 2247:      return failure();
        -: 2248:
    #####: 2249:    Operation *replacement = rewriter.create<InsertOpTy>(
        -: 2250:        insertSliceOp.getLoc(), src, dst, insertSliceOp.getMixedOffsets(),
        -: 2251:        insertSliceOp.getMixedSizes(), insertSliceOp.getMixedStrides());
        -: 2252:
        -: 2253:    // In the parallel case there is no result and so nothing to cast.
    #####: 2254:    bool isParallelInsert =
        -: 2255:        std::is_same<InsertOpTy, ParallelInsertSliceOp>::value;
    #####: 2256:    if (!isParallelInsert && dst.getType() != insertSliceOp.getDestType()) {
    #####: 2257:      replacement = rewriter.create<tensor::CastOp>(insertSliceOp.getLoc(),
        -: 2258:                                                    insertSliceOp.getDestType(),
    #####: 2259:                                                    replacement->getResult(0));
        -: 2260:    }
    #####: 2261:    rewriter.replaceOp(insertSliceOp, replacement->getResults());
    #####: 2262:    return success();
        -: 2263:  }
------------------
_ZNK12_GLOBAL__N_123InsertSliceOpCastFolderIN4mlir6tensor13InsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_123InsertSliceOpCastFolderIN4mlir6tensor13InsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2219:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
call    0 never executed
        -: 2220:                                PatternRewriter &rewriter) const override {
    #####: 2221:    if (llvm::any_of(insertSliceOp.getOperands(), [](Value operand) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2222:          return matchPattern(operand, matchConstantIndex());
        -: 2223:        }))
    #####: 2224:      return failure();
        -: 2225:
        -: 2226:    auto getSourceOfCastOp = [](Value v) -> Optional<Value> {
        -: 2227:      auto castOp = v.getDefiningOp<tensor::CastOp>();
        -: 2228:      if (!castOp || !canFoldIntoConsumerOp(castOp))
        -: 2229:        return llvm::None;
        -: 2230:      return castOp.getSource();
        -: 2231:    };
        -: 2232:    Optional<Value> sourceCastSource =
    #####: 2233:        getSourceOfCastOp(insertSliceOp.getSource());
call    0 never executed
call    1 never executed
    #####: 2234:    Optional<Value> destCastSource = getSourceOfCastOp(insertSliceOp.getDest());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2235:    if (!sourceCastSource && !destCastSource)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2236:      return failure();
        -: 2237:
    #####: 2238:    auto src =
    #####: 2239:        (sourceCastSource ? *sourceCastSource : insertSliceOp.getSource());
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2240:    auto dst = (destCastSource ? *destCastSource : insertSliceOp.getDest());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2241:    auto srcType = src.getType().template cast<ShapedType>();
call    0 never executed
    #####: 2242:    auto dstType = dst.getType().template cast<ShapedType>();
call    0 never executed
call    1 never executed
    #####: 2243:    if (verifyInsertSliceOp(srcType, dstType, insertSliceOp.getStaticOffsets(),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -: 2244:                            insertSliceOp.getStaticSizes(),
        -: 2245:                            insertSliceOp.getStaticStrides()) !=
        -: 2246:        SliceVerificationResult::Success)
    #####: 2247:      return failure();
        -: 2248:
    #####: 2249:    Operation *replacement = rewriter.create<InsertOpTy>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
        -: 2250:        insertSliceOp.getLoc(), src, dst, insertSliceOp.getMixedOffsets(),
        -: 2251:        insertSliceOp.getMixedSizes(), insertSliceOp.getMixedStrides());
        -: 2252:
        -: 2253:    // In the parallel case there is no result and so nothing to cast.
    #####: 2254:    bool isParallelInsert =
call    0 never executed
        -: 2255:        std::is_same<InsertOpTy, ParallelInsertSliceOp>::value;
    #####: 2256:    if (!isParallelInsert && dst.getType() != insertSliceOp.getDestType()) {
branch  0 never executed
branch  1 never executed
    #####: 2257:      replacement = rewriter.create<tensor::CastOp>(insertSliceOp.getLoc(),
call    0 never executed
call    1 never executed
        -: 2258:                                                    insertSliceOp.getDestType(),
    #####: 2259:                                                    replacement->getResult(0));
call    0 never executed
        -: 2260:    }
    #####: 2261:    rewriter.replaceOp(insertSliceOp, replacement->getResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####: 2262:    return success();
        -: 2263:  }
------------------
_ZNK12_GLOBAL__N_123InsertSliceOpCastFolderIN4mlir6tensor21ParallelInsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_123InsertSliceOpCastFolderIN4mlir6tensor21ParallelInsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2219:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
call    0 never executed
        -: 2220:                                PatternRewriter &rewriter) const override {
    #####: 2221:    if (llvm::any_of(insertSliceOp.getOperands(), [](Value operand) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2222:          return matchPattern(operand, matchConstantIndex());
        -: 2223:        }))
    #####: 2224:      return failure();
        -: 2225:
        -: 2226:    auto getSourceOfCastOp = [](Value v) -> Optional<Value> {
        -: 2227:      auto castOp = v.getDefiningOp<tensor::CastOp>();
        -: 2228:      if (!castOp || !canFoldIntoConsumerOp(castOp))
        -: 2229:        return llvm::None;
        -: 2230:      return castOp.getSource();
        -: 2231:    };
        -: 2232:    Optional<Value> sourceCastSource =
    #####: 2233:        getSourceOfCastOp(insertSliceOp.getSource());
call    0 never executed
call    1 never executed
    #####: 2234:    Optional<Value> destCastSource = getSourceOfCastOp(insertSliceOp.getDest());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2235:    if (!sourceCastSource && !destCastSource)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2236:      return failure();
        -: 2237:
    #####: 2238:    auto src =
    #####: 2239:        (sourceCastSource ? *sourceCastSource : insertSliceOp.getSource());
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2240:    auto dst = (destCastSource ? *destCastSource : insertSliceOp.getDest());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2241:    auto srcType = src.getType().template cast<ShapedType>();
call    0 never executed
    #####: 2242:    auto dstType = dst.getType().template cast<ShapedType>();
call    0 never executed
call    1 never executed
    #####: 2243:    if (verifyInsertSliceOp(srcType, dstType, insertSliceOp.getStaticOffsets(),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -: 2244:                            insertSliceOp.getStaticSizes(),
        -: 2245:                            insertSliceOp.getStaticStrides()) !=
        -: 2246:        SliceVerificationResult::Success)
    #####: 2247:      return failure();
        -: 2248:
    #####: 2249:    Operation *replacement = rewriter.create<InsertOpTy>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
        -: 2250:        insertSliceOp.getLoc(), src, dst, insertSliceOp.getMixedOffsets(),
        -: 2251:        insertSliceOp.getMixedSizes(), insertSliceOp.getMixedStrides());
        -: 2252:
        -: 2253:    // In the parallel case there is no result and so nothing to cast.
    #####: 2254:    bool isParallelInsert =
        -: 2255:        std::is_same<InsertOpTy, ParallelInsertSliceOp>::value;
        -: 2256:    if (!isParallelInsert && dst.getType() != insertSliceOp.getDestType()) {
        -: 2257:      replacement = rewriter.create<tensor::CastOp>(insertSliceOp.getLoc(),
        -: 2258:                                                    insertSliceOp.getDestType(),
        -: 2259:                                                    replacement->getResult(0));
        -: 2260:    }
    #####: 2261:    rewriter.replaceOp(insertSliceOp, replacement->getResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####: 2262:    return success();
        -: 2263:  }
------------------
        -: 2264:};
        -: 2265:
        -: 2266:/// If additional static type information can be deduced from a insert_slice's
        -: 2267:/// size operands, insert an explicit cast of the op's source operand. This
        -: 2268:/// enables other canonicalization patterns that are matching for tensor_cast
        -: 2269:/// ops such as `ForOpTensorCastFolder` in SCF.
        -: 2270:///
        -: 2271:/// Example:
        -: 2272:///
        -: 2273:/// ```mlir
        -: 2274:///   %r = tensor.insert_slice %0 into %1[...] [64, 64] [1, 1]
        -: 2275:///       : tensor<?x?xf32> into ...
        -: 2276:/// ```
        -: 2277:///
        -: 2278:/// folds into:
        -: 2279:///
        -: 2280:/// ```mlir
        -: 2281:///   %tmp = tensor.cast %0 : tensor<?x?xf32> to tensor<64x64xf32>
        -: 2282:///   %r = tensor.insert_slice %tmp into %1[...] [64, 64] [1, 1]
        -: 2283:///       : tensor<64x64xf32> into ...
        -: 2284:/// ```
        -: 2285:///
        -: 2286:/// This patterns works with both InsertSliceOp and ParallelInsertSliceOp.
        -: 2287:template <typename InsertOpTy>
        -: 2288:struct InsertSliceOpSourceCastInserter final
        -: 2289:    : public OpRewritePattern<InsertOpTy> {
        -: 2290:  using OpRewritePattern<InsertOpTy>::OpRewritePattern;
        -: 2291:
    #####: 2292:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
        -: 2293:                                PatternRewriter &rewriter) const override {
    #####: 2294:    RankedTensorType srcType = insertSliceOp.getSourceType();
    #####: 2295:    if (srcType.getRank() != insertSliceOp.getDestType().getRank())
    #####: 2296:      return failure();
    #####: 2297:    SmallVector<int64_t> newSrcShape(srcType.getShape().begin(),
    #####: 2298:                                     srcType.getShape().end());
    #####: 2299:    for (int64_t i = 0; i < srcType.getRank(); ++i) {
    #####: 2300:      if (Optional<int64_t> constInt =
    #####: 2301:              getConstantIntValue(insertSliceOp.getMixedSizes()[i]))
    #####: 2302:        newSrcShape[i] = *constInt;
        -: 2303:    }
        -: 2304:
        -: 2305:    RankedTensorType newSrcType =
    #####: 2306:        RankedTensorType::get(newSrcShape, srcType.getElementType());
    #####: 2307:    if (srcType == newSrcType ||
    #####: 2308:        !preservesStaticInformation(srcType, newSrcType) ||
    #####: 2309:        !tensor::CastOp::areCastCompatible(srcType, newSrcType))
    #####: 2310:      return failure();
        -: 2311:
        -: 2312:    // newSrcType is:
        -: 2313:    //   1) Different from srcType.
        -: 2314:    //   2) "More static" than srcType.
        -: 2315:    //   3) Cast-compatible with srcType.
        -: 2316:    // Insert the cast.
    #####: 2317:    OpBuilder::InsertionGuard g(rewriter);
        -: 2318:    // The only difference between InsertSliceOp and ParallelInsertSliceOp is
        -: 2319:    // the the insertion point is just before the ParallelCombiningOp in the
        -: 2320:    // parallel case.
        -: 2321:    if (std::is_same<InsertOpTy, ParallelInsertSliceOp>::value)
    #####: 2322:      rewriter.setInsertionPoint(insertSliceOp->getParentOp());
    #####: 2323:    Value cast = rewriter.create<tensor::CastOp>(
        -: 2324:        insertSliceOp.getLoc(), newSrcType, insertSliceOp.getSource());
    #####: 2325:    rewriter.replaceOpWithNewOp<InsertOpTy>(
        -: 2326:        insertSliceOp, cast, insertSliceOp.getDest(),
        -: 2327:        insertSliceOp.getMixedOffsets(), insertSliceOp.getMixedSizes(),
        -: 2328:        insertSliceOp.getMixedStrides());
    #####: 2329:    cast.getDefiningOp()->getParentOfType<ModuleOp>().dump();
    #####: 2330:    return success();
        -: 2331:  }
------------------
_ZNK12_GLOBAL__N_131InsertSliceOpSourceCastInserterIN4mlir6tensor13InsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_131InsertSliceOpSourceCastInserterIN4mlir6tensor13InsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2292:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
        -: 2293:                                PatternRewriter &rewriter) const override {
    #####: 2294:    RankedTensorType srcType = insertSliceOp.getSourceType();
call    0 never executed
    #####: 2295:    if (srcType.getRank() != insertSliceOp.getDestType().getRank())
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2296:      return failure();
    #####: 2297:    SmallVector<int64_t> newSrcShape(srcType.getShape().begin(),
call    0 never executed
call    1 never executed
    #####: 2298:                                     srcType.getShape().end());
call    0 never executed
    #####: 2299:    for (int64_t i = 0; i < srcType.getRank(); ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2300:      if (Optional<int64_t> constInt =
branch  0 never executed
branch  1 never executed
    #####: 2301:              getConstantIntValue(insertSliceOp.getMixedSizes()[i]))
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2302:        newSrcShape[i] = *constInt;
branch  0 never executed
branch  1 never executed
        -: 2303:    }
        -: 2304:
        -: 2305:    RankedTensorType newSrcType =
    #####: 2306:        RankedTensorType::get(newSrcShape, srcType.getElementType());
call    0 never executed
call    1 never executed
    #####: 2307:    if (srcType == newSrcType ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2308:        !preservesStaticInformation(srcType, newSrcType) ||
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2309:        !tensor::CastOp::areCastCompatible(srcType, newSrcType))
call    0 never executed
call    1 never executed
    #####: 2310:      return failure();
        -: 2311:
        -: 2312:    // newSrcType is:
        -: 2313:    //   1) Different from srcType.
        -: 2314:    //   2) "More static" than srcType.
        -: 2315:    //   3) Cast-compatible with srcType.
        -: 2316:    // Insert the cast.
    #####: 2317:    OpBuilder::InsertionGuard g(rewriter);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2318:    // The only difference between InsertSliceOp and ParallelInsertSliceOp is
        -: 2319:    // the the insertion point is just before the ParallelCombiningOp in the
        -: 2320:    // parallel case.
        -: 2321:    if (std::is_same<InsertOpTy, ParallelInsertSliceOp>::value)
        -: 2322:      rewriter.setInsertionPoint(insertSliceOp->getParentOp());
    #####: 2323:    Value cast = rewriter.create<tensor::CastOp>(
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2324:        insertSliceOp.getLoc(), newSrcType, insertSliceOp.getSource());
    #####: 2325:    rewriter.replaceOpWithNewOp<InsertOpTy>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
branch 10 never executed
        -: 2326:        insertSliceOp, cast, insertSliceOp.getDest(),
        -: 2327:        insertSliceOp.getMixedOffsets(), insertSliceOp.getMixedSizes(),
        -: 2328:        insertSliceOp.getMixedStrides());
    #####: 2329:    cast.getDefiningOp()->getParentOfType<ModuleOp>().dump();
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2330:    return success();
branch  0 never executed
branch  1 never executed
        -: 2331:  }
------------------
_ZNK12_GLOBAL__N_131InsertSliceOpSourceCastInserterIN4mlir6tensor21ParallelInsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE:
function _ZNK12_GLOBAL__N_131InsertSliceOpSourceCastInserterIN4mlir6tensor21ParallelInsertSliceOpEE15matchAndRewriteES3_RNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2292:  LogicalResult matchAndRewrite(InsertOpTy insertSliceOp,
        -: 2293:                                PatternRewriter &rewriter) const override {
    #####: 2294:    RankedTensorType srcType = insertSliceOp.getSourceType();
call    0 never executed
    #####: 2295:    if (srcType.getRank() != insertSliceOp.getDestType().getRank())
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2296:      return failure();
    #####: 2297:    SmallVector<int64_t> newSrcShape(srcType.getShape().begin(),
call    0 never executed
call    1 never executed
    #####: 2298:                                     srcType.getShape().end());
call    0 never executed
    #####: 2299:    for (int64_t i = 0; i < srcType.getRank(); ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2300:      if (Optional<int64_t> constInt =
branch  0 never executed
branch  1 never executed
    #####: 2301:              getConstantIntValue(insertSliceOp.getMixedSizes()[i]))
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2302:        newSrcShape[i] = *constInt;
branch  0 never executed
branch  1 never executed
        -: 2303:    }
        -: 2304:
        -: 2305:    RankedTensorType newSrcType =
    #####: 2306:        RankedTensorType::get(newSrcShape, srcType.getElementType());
call    0 never executed
call    1 never executed
    #####: 2307:    if (srcType == newSrcType ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2308:        !preservesStaticInformation(srcType, newSrcType) ||
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2309:        !tensor::CastOp::areCastCompatible(srcType, newSrcType))
call    0 never executed
call    1 never executed
    #####: 2310:      return failure();
        -: 2311:
        -: 2312:    // newSrcType is:
        -: 2313:    //   1) Different from srcType.
        -: 2314:    //   2) "More static" than srcType.
        -: 2315:    //   3) Cast-compatible with srcType.
        -: 2316:    // Insert the cast.
    #####: 2317:    OpBuilder::InsertionGuard g(rewriter);
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -: 2318:    // The only difference between InsertSliceOp and ParallelInsertSliceOp is
        -: 2319:    // the the insertion point is just before the ParallelCombiningOp in the
        -: 2320:    // parallel case.
        -: 2321:    if (std::is_same<InsertOpTy, ParallelInsertSliceOp>::value)
    #####: 2322:      rewriter.setInsertionPoint(insertSliceOp->getParentOp());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2323:    Value cast = rewriter.create<tensor::CastOp>(
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2324:        insertSliceOp.getLoc(), newSrcType, insertSliceOp.getSource());
    #####: 2325:    rewriter.replaceOpWithNewOp<InsertOpTy>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
branch 10 never executed
        -: 2326:        insertSliceOp, cast, insertSliceOp.getDest(),
        -: 2327:        insertSliceOp.getMixedOffsets(), insertSliceOp.getMixedSizes(),
        -: 2328:        insertSliceOp.getMixedStrides());
    #####: 2329:    cast.getDefiningOp()->getParentOfType<ModuleOp>().dump();
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2330:    return success();
branch  0 never executed
branch  1 never executed
        -: 2331:  }
------------------
        -: 2332:};
        -: 2333:} // namespace
        -: 2334:
function _ZN4mlir6tensor13InsertSliceOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1261 returned 100% blocks executed 100%
     1261: 2335:void InsertSliceOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 2336:                                                MLIRContext *context) {
     1261: 2337:  results.add<InsertSliceOpConstantArgumentFolder<InsertSliceOp>,
        -: 2338:              InsertSliceOpCastFolder<InsertSliceOp>,
     1261: 2339:              InsertSliceOpSourceCastInserter<InsertSliceOp>>(context);
call    0 returned 100%
     1261: 2340:}
        -: 2341:
function _ZN4mlir6tensor40createCanonicalRankReducingInsertSliceOpERNS_9OpBuilderENS_8LocationENS_5ValueES4_ called 0 returned 0% blocks executed 0%
    #####: 2342:Value mlir::tensor::createCanonicalRankReducingInsertSliceOp(OpBuilder &b,
        -: 2343:                                                             Location loc,
        -: 2344:                                                             Value tensor,
        -: 2345:                                                             Value dest) {
    #####: 2346:  auto rankedTensorType = dest.getType().cast<RankedTensorType>();
call    0 never executed
    #####: 2347:  unsigned rank = rankedTensorType.getRank();
call    0 never executed
    #####: 2348:  SmallVector<OpFoldResult> offsets(rank, b.getIndexAttr(0));
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2349:  SmallVector<OpFoldResult> sizes = getMixedSizes(b, loc, dest);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2350:  SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2351:  return b.createOrFold<tensor::InsertSliceOp>(loc, tensor, dest, offsets,
    #####: 2352:                                               sizes, strides);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2353:}
        -: 2354:
        -: 2355://===----------------------------------------------------------------------===//
        -: 2356:// PadOp
        -: 2357://===----------------------------------------------------------------------===//
        -: 2358:
function _ZN4mlir6tensor5PadOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####: 2359:void PadOp::getAsmResultNames(function_ref<void(Value, StringRef)> setNameFn) {
    #####: 2360:  setNameFn(getResult(), "padded");
call    0 never executed
call    1 never executed
    #####: 2361:}
        -: 2362:
        -: 2363:// TODO: Replace custom<InferType> directive with AllTypesMatch as soon as it
        -: 2364:// supports optional types.
function _Z14printInferTypeRN4mlir12OpAsmPrinterEPNS_9OperationENS_5ValueENS_4TypeES5_ called 0 returned 0% blocks executed 0%
    #####: 2365:void printInferType(OpAsmPrinter &printer, Operation *op, Value optOperand,
    #####: 2366:                    Type typeToInfer, Type typeToInferFrom) {}
        -: 2367:
function _Z14parseInferTypeRN4mlir11OpAsmParserEN4llvm8OptionalINS0_17UnresolvedOperandEEERNS_4TypeES6_ called 0 returned 0% blocks executed 0%
    #####: 2368:ParseResult parseInferType(OpAsmParser &parser,
        -: 2369:                           Optional<OpAsmParser::UnresolvedOperand> optOperand,
        -: 2370:                           Type &typeToInfer, Type typeToInferFrom) {
    #####: 2371:  if (optOperand)
branch  0 never executed
branch  1 never executed
    #####: 2372:    typeToInfer = typeToInferFrom;
    #####: 2373:  return success();
        -: 2374:}
        -: 2375:
function _ZN4mlir6tensor5PadOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 2376:LogicalResult PadOp::verify() {
    #####: 2377:  auto sourceType = getSource().getType().cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2378:  auto resultType = getResult().getType().cast<RankedTensorType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2379:  auto expectedType = PadOp::inferResultType(
    #####: 2380:      sourceType, extractFromI64ArrayAttr(getStaticLow()),
call    0 never executed
call    1 never executed
    #####: 2381:      extractFromI64ArrayAttr(getStaticHigh()));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2382:  for (int i = 0, e = sourceType.getRank(); i < e; ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2383:    if (resultType.getDimSize(i) == expectedType.getDimSize(i))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2384:      continue;
    #####: 2385:    if (expectedType.isDynamicDim(i))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2386:      continue;
    #####: 2387:    return emitError("specified type ")
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 2388:           << resultType << " does not match the inferred type "
call    0 never executed
call    1 never executed
    #####: 2389:           << expectedType;
call    0 never executed
        -: 2390:  }
        -: 2391:
    #####: 2392:  return success();
        -: 2393:}
        -: 2394:
function _ZN4mlir6tensor5PadOp13verifyRegionsEv called 0 returned 0% blocks executed 0%
    #####: 2395:LogicalResult PadOp::verifyRegions() {
    #####: 2396:  auto &region = getRegion();
call    0 never executed
    #####: 2397:  unsigned rank = getResult().getType().cast<RankedTensorType>().getRank();
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 2398:  Block &block = region.front();
call    0 never executed
    #####: 2399:  if (block.getNumArguments() != rank)
branch  0 never executed
branch  1 never executed
    #####: 2400:    return emitError("expected the block to have ") << rank << " arguments";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
        -: 2401:
        -: 2402:  // Note: the number and type of yield values are checked in the YieldOp.
    #####: 2403:  for (const auto &en : llvm::enumerate(block.getArgumentTypes())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 2404:    if (!en.value().isIndex())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2405:      return emitOpError("expected block argument ")
call    0 never executed
call    1 never executed
    #####: 2406:             << (en.index() + 1) << " to be an index";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 2407:  }
        -: 2408:
        -: 2409:  // Ensure that the region yields an element of the right type.
    #####: 2410:  auto yieldOp = llvm::cast<YieldOp>(block.getTerminator());
call    0 never executed
call    1 never executed
    #####: 2411:  if (yieldOp.getValue().getType() !=
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2412:      getType().cast<ShapedType>().getElementType())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2413:    return emitOpError("expected yield type to match shape element type");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 2414:
    #####: 2415:  return success();
        -: 2416:}
        -: 2417:
function _ZN4mlir6tensor5PadOp15inferResultTypeENS_16RankedTensorTypeEN4llvm8ArrayRefIlEES5_S5_ called 0 returned 0% blocks executed 0%
    #####: 2418:RankedTensorType PadOp::inferResultType(RankedTensorType sourceType,
        -: 2419:                                        ArrayRef<int64_t> staticLow,
        -: 2420:                                        ArrayRef<int64_t> staticHigh,
        -: 2421:                                        ArrayRef<int64_t> resultShape) {
    #####: 2422:  unsigned rank = sourceType.getRank();
call    0 never executed
    #####: 2423:  assert(staticLow.size() == rank && "unexpected staticLow size mismatch");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2424:  assert(staticHigh.size() == rank && "unexpected staticHigh size mismatch");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2425:  assert((resultShape.empty() || resultShape.size() == rank) &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
        -: 2426:         "unexpected resultShape size mismatch");
        -: 2427:
    #####: 2428:  SmallVector<int64_t, 4> inferredShape;
call    0 never executed
    #####: 2429:  for (auto i : llvm::seq<unsigned>(0, rank)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 2430:    if (sourceType.isDynamicDim(i) ||
call    0 never executed
    #####: 2431:        staticLow[i] == ShapedType::kDynamicSize ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2432:        staticHigh[i] == ShapedType::kDynamicSize) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2433:      inferredShape.push_back(resultShape.empty() ? ShapedType::kDynamicSize
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2434:                                                  : resultShape[i]);
branch  0 never executed
branch  1 never executed
        -: 2435:    } else {
    #####: 2436:      int64_t size = sourceType.getDimSize(i) + staticLow[i] + staticHigh[i];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2437:      assert((resultShape.empty() || size == resultShape[i] ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
        -: 2438:              resultShape[i] == ShapedType::kDynamicSize) &&
        -: 2439:             "mismatch between inferred shape and result shape");
    #####: 2440:      inferredShape.push_back(size);
call    0 never executed
        -: 2441:    }
        -: 2442:  }
        -: 2443:
    #####: 2444:  return RankedTensorType::get(inferredShape, sourceType.getElementType());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 2445:}
        -: 2446:
function _ZN4mlir6tensor5PadOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueEN4llvm8ArrayRefIlEES9_NS_10ValueRangeESA_bNS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2447:void PadOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 2448:                  ArrayRef<int64_t> staticLow, ArrayRef<int64_t> staticHigh,
        -: 2449:                  ValueRange low, ValueRange high, bool nofold,
        -: 2450:                  ArrayRef<NamedAttribute> attrs) {
    #####: 2451:  auto sourceType = source.getType().cast<RankedTensorType>();
call    0 never executed
    #####: 2452:  auto resultType = inferResultType(sourceType, staticLow, staticHigh);
call    0 never executed
    #####: 2453:  build(b, result, resultType, source, low, high, b.getI64ArrayAttr(staticLow),
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 2454:        b.getI64ArrayAttr(staticHigh), nofold ? b.getUnitAttr() : UnitAttr());
call    0 never executed
    #####: 2455:  result.addAttributes(attrs);
call    0 never executed
    #####: 2456:}
        -: 2457:
function _ZN4mlir6tensor5PadOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueENS_10ValueRangeES7_bN4llvm8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2458:void PadOp::build(OpBuilder &b, OperationState &result, Value source,
        -: 2459:                  ValueRange low, ValueRange high, bool nofold,
        -: 2460:                  ArrayRef<NamedAttribute> attrs) {
    #####: 2461:  auto sourceType = source.getType().cast<RankedTensorType>();
call    0 never executed
    #####: 2462:  unsigned rank = sourceType.getRank();
call    0 never executed
    #####: 2463:  SmallVector<int64_t, 4> staticVector(rank, ShapedType::kDynamicSize);
call    0 never executed
    #####: 2464:  build(b, result, source, staticVector, staticVector, low, high, nofold,
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2465:        attrs);
    #####: 2466:}
        -: 2467:
function _ZN4mlir6tensor5PadOp5buildERNS_9OpBuilderERNS_14OperationStateENS_4TypeENS_5ValueEN4llvm8ArrayRefINS_12OpFoldResultEEESB_bNS9_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2468:void PadOp::build(OpBuilder &b, OperationState &result, Type resultType,
        -: 2469:                  Value source, ArrayRef<OpFoldResult> low,
        -: 2470:                  ArrayRef<OpFoldResult> high, bool nofold,
        -: 2471:                  ArrayRef<NamedAttribute> attrs) {
    #####: 2472:  auto sourceType = source.getType().cast<RankedTensorType>();
call    0 never executed
    #####: 2473:  SmallVector<Value, 4> dynamicLow, dynamicHigh;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2474:  SmallVector<int64_t, 4> staticLow, staticHigh;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -: 2475:  // staticLow and staticHigh have full information of the padding config.
        -: 2476:  // This will grow staticLow and staticHigh with 1 value. If the config is
        -: 2477:  // dynamic (ie not a constant), dynamicLow and dynamicHigh will grow with 1
        -: 2478:  // value as well.
    #####: 2479:  dispatchIndexOpFoldResults(low, dynamicLow, staticLow,
call    0 never executed
        -: 2480:                             ShapedType::kDynamicSize);
    #####: 2481:  dispatchIndexOpFoldResults(high, dynamicHigh, staticHigh,
call    0 never executed
        -: 2482:                             ShapedType::kDynamicSize);
    #####: 2483:  if (!resultType) {
branch  0 never executed
branch  1 never executed
    #####: 2484:    resultType = PadOp::inferResultType(sourceType, staticLow, staticHigh);
call    0 never executed
        -: 2485:  }
    #####: 2486:  assert(resultType.isa<RankedTensorType>());
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 2487:  build(b, result, resultType, source, dynamicLow, dynamicHigh,
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
        -: 2488:        b.getI64ArrayAttr(staticLow), b.getI64ArrayAttr(staticHigh),
    #####: 2489:        nofold ? b.getUnitAttr() : UnitAttr());
call    0 never executed
    #####: 2490:  result.addAttributes(attrs);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2491:}
        -: 2492:
function _ZN4mlir6tensor5PadOp5buildERNS_9OpBuilderERNS_14OperationStateENS_4TypeENS_5ValueEN4llvm8ArrayRefINS_12OpFoldResultEEESB_S7_bNS9_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2493:void PadOp::build(OpBuilder &b, OperationState &result, Type resultType,
        -: 2494:                  Value source, ArrayRef<OpFoldResult> low,
        -: 2495:                  ArrayRef<OpFoldResult> high, Value constantPadValue,
        -: 2496:                  bool nofold, ArrayRef<NamedAttribute> attrs) {
    #####: 2497:  build(b, result, resultType, source, low, high, nofold, attrs);
call    0 never executed
        -: 2498:
        -: 2499:  // Add a region and a block to yield the pad value.
    #####: 2500:  Region *region = result.regions[0].get();
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2501:  int sourceRank = source.getType().cast<RankedTensorType>().getRank();
call    0 never executed
call    1 never executed
    #####: 2502:  SmallVector<Type> blockArgTypes(sourceRank, b.getIndexType());
call    0 never executed
call    1 never executed
    #####: 2503:  SmallVector<Location> blockArgLocs(sourceRank, result.location);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2504:
        -: 2505:  // `builder.createBlock` changes the insertion point within the block. Create
        -: 2506:  // a guard to reset the insertion point of the builder after it is destroyed.
    #####: 2507:  OpBuilder::InsertionGuard guard(b);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2508:  b.createBlock(region, region->end(), blockArgTypes, blockArgLocs);
call    0 never executed
call    1 never executed
    #####: 2509:  b.create<tensor::YieldOp>(result.location, constantPadValue);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2510:}
        -: 2511:
function _ZN4mlir6tensor5PadOp13getPaddedDimsEv called 0 returned 0% blocks executed 0%
    #####: 2512:llvm::SmallBitVector PadOp::getPaddedDims() {
    #####: 2513:  llvm::SmallBitVector paddedDims(getSourceType().getRank());
call    0 never executed
call    1 never executed
call    2 never executed
function _ZZN4mlir6tensor5PadOp13getPaddedDimsEvENKUlN4llvm8ArrayRefINS_12OpFoldResultEEEE_clES5_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 2514:  auto extractPaddedDims = [&](ArrayRef<OpFoldResult> paddingWidths) {
    #####: 2515:    for (const auto &en : enumerate(paddingWidths))
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2516:      if (getConstantIntValue(en.value()) != static_cast<int64_t>(0))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2517:        paddedDims.set(en.index());
call    0 never executed
    #####: 2518:  };
    #####: 2519:  extractPaddedDims(getMixedLowPad());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2520:  extractPaddedDims(getMixedHighPad());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2521:  return paddedDims;
        -: 2522:}
        -: 2523:
        -: 2524:namespace {
        -: 2525:// Folds tensor.pad when padding is static zeros and the attribute
        -: 2526:// doesn't request otherwise.
        -: 2527:struct FoldStaticZeroPadding : public OpRewritePattern<PadOp> {
        -: 2528:  using OpRewritePattern<PadOp>::OpRewritePattern;
        -: 2529:
function _ZNK12_GLOBAL__N_121FoldStaticZeroPadding15matchAndRewriteEN4mlir6tensor5PadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2530:  LogicalResult matchAndRewrite(PadOp padTensorOp,
        -: 2531:                                PatternRewriter &rewriter) const override {
    #####: 2532:    if (!padTensorOp.hasZeroLowPad() || !padTensorOp.hasZeroHighPad())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2533:      return failure();
    #####: 2534:    if (padTensorOp.getNofold())
call    0 never executed
    #####: 2535:      return failure();
    #####: 2536:    rewriter.replaceOpWithNewOp<tensor::CastOp>(
    #####: 2537:        padTensorOp, padTensorOp.getResult().getType(),
call    0 never executed
call    1 never executed
    #####: 2538:        padTensorOp.getSource());
call    0 never executed
call    1 never executed
    #####: 2539:    return success();
        -: 2540:  }
        -: 2541:};
        -: 2542:
        -: 2543:// Fold CastOp into PadOp when adding static information.
        -: 2544:struct FoldSourceTensorCast : public OpRewritePattern<PadOp> {
        -: 2545:  using OpRewritePattern<PadOp>::OpRewritePattern;
        -: 2546:
function _ZNK12_GLOBAL__N_120FoldSourceTensorCast15matchAndRewriteEN4mlir6tensor5PadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2547:  LogicalResult matchAndRewrite(PadOp padTensorOp,
        -: 2548:                                PatternRewriter &rewriter) const override {
    #####: 2549:    auto castOp = padTensorOp.getSource().getDefiningOp<tensor::CastOp>();
call    0 never executed
call    1 never executed
    #####: 2550:    if (!tensor::canFoldIntoConsumerOp(castOp))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2551:      return failure();
        -: 2552:
    #####: 2553:    auto newResultType = PadOp::inferResultType(
    #####: 2554:        castOp.getSource().getType().cast<RankedTensorType>(),
call    0 never executed
call    1 never executed
    #####: 2555:        extractFromI64ArrayAttr(padTensorOp.getStaticLow()),
call    0 never executed
call    1 never executed
    #####: 2556:        extractFromI64ArrayAttr(padTensorOp.getStaticHigh()),
call    0 never executed
call    1 never executed
    #####: 2557:        padTensorOp.getResultType().getShape());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
        -: 2558:
    #####: 2559:    if (newResultType == padTensorOp.getResultType()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
function _ZZNK12_GLOBAL__N_120FoldSourceTensorCast15matchAndRewriteEN4mlir6tensor5PadOpERNS1_15PatternRewriterEENKUlvE_clEv.isra.0 called 0 returned 0% blocks executed 0%
    #####: 2560:      rewriter.updateRootInPlace(padTensorOp, [&]() {
call    0 never executed
    #####: 2561:        padTensorOp.getSourceMutable().assign(castOp.getSource());
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2562:      });
        -: 2563:    } else {
    #####: 2564:      auto newOp = rewriter.create<PadOp>(
    #####: 2565:          padTensorOp->getLoc(), newResultType, padTensorOp.getSource(),
    #####: 2566:          padTensorOp.getLow(), padTensorOp.getHigh(),
call    0 never executed
call    1 never executed
    #####: 2567:          padTensorOp.getStaticLow(), padTensorOp.getStaticHigh(),
call    0 never executed
call    1 never executed
    #####: 2568:          padTensorOp.getNofold());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2569:      BlockAndValueMapping mapper;
call    0 never executed
    #####: 2570:      padTensorOp.getRegion().cloneInto(&newOp.getRegion(), mapper);
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2571:
    #####: 2572:      rewriter.replaceOpWithNewOp<tensor::CastOp>(
    #####: 2573:          padTensorOp, padTensorOp.getResultType(), newOp);
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2574:    }
    #####: 2575:    return success();
        -: 2576:  }
        -: 2577:};
        -: 2578:
        -: 2579:// Fold CastOp using the result of PadOp back into the latter if it adds
        -: 2580:// static information.
        -: 2581:struct FoldTargetTensorCast : public OpRewritePattern<PadOp> {
        -: 2582:  using OpRewritePattern<PadOp>::OpRewritePattern;
        -: 2583:
function _ZNK12_GLOBAL__N_120FoldTargetTensorCast15matchAndRewriteEN4mlir6tensor5PadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2584:  LogicalResult matchAndRewrite(PadOp padTensorOp,
        -: 2585:                                PatternRewriter &rewriter) const override {
    #####: 2586:    if (!padTensorOp.getResult().hasOneUse())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2587:      return failure();
    #####: 2588:    auto tensorCastOp =
    #####: 2589:        dyn_cast<tensor::CastOp>(*padTensorOp->getUsers().begin());
call    0 never executed
call    1 never executed
    #####: 2590:    if (!tensorCastOp)
branch  0 never executed
branch  1 never executed
    #####: 2591:      return failure();
    #####: 2592:    if (!tensor::preservesStaticInformation(padTensorOp.getResult().getType(),
call    0 never executed
call    1 never executed
    #####: 2593:                                            tensorCastOp.getDest().getType()))
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2594:      return failure();
        -: 2595:
    #####: 2596:    auto replacementOp = rewriter.create<PadOp>(
    #####: 2597:        padTensorOp.getLoc(), tensorCastOp.getDest().getType(),
call    0 never executed
call    1 never executed
    #####: 2598:        padTensorOp.getSource(), padTensorOp.getLow(), padTensorOp.getHigh(),
call    0 never executed
call    1 never executed
    #####: 2599:        padTensorOp.getStaticLow(), padTensorOp.getStaticHigh(),
call    0 never executed
call    1 never executed
    #####: 2600:        padTensorOp.getNofold());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2601:    replacementOp.getRegion().takeBody(padTensorOp.getRegion());
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2602:
    #####: 2603:    rewriter.replaceOp(padTensorOp, replacementOp.getResult());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2604:    rewriter.replaceOp(tensorCastOp, replacementOp.getResult());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2605:    return success();
        -: 2606:  }
        -: 2607:};
        -: 2608:
        -: 2609:/// Fold chains of tensor::ExtractSliceOp, tensor::PadOp pairs that pad
        -: 2610:/// different dimensions. The pattern applies if the following preconditions
        -: 2611:/// hold:
        -: 2612:///   1) the tensor::ExtractSliceOps are not rank-reducing,
        -: 2613:///   2) the tensor::ExtractSliceOps have only unit-strides,
        -: 2614:///   3) the tensor::PadOps perform only high-padding,
        -: 2615:///   4) the tensor::PadOps have the same constant padding value,
        -: 2616:///   5) the tensor::PadOps do not have common padding dimensions,
        -: 2617:///   6) one tensor::ExtractSliceOp, tensor::PadOp pair has zero-padding and
        -: 2618:///      zero-offset for every dimension.
        -: 2619:///   7) the tensor::ExtractSliceOp sizes match the source tensor sizes for
        -: 2620:///   the
        -: 2621:///      padded source dimensions.
        -: 2622:///
        -: 2623:/// Example:
        -: 2624:///
        -: 2625:/// ```mlir
        -: 2626:///   %0 = tensor.extract_slice %input[16, 0] [%sz0, 64] [1, 1]
        -: 2627:///       : tensor<64x64xf32> to tensor<?x64xf32>
        -: 2628:///   %1 = tensor.pad %0 low[0, 0] high[%pw0, 0] { ...
        -: 2629:///     } : tensor<?x64xf32> to tensor<8x64xf32>
        -: 2630:///   %2 = tensor.extract_slice %1[0, 4] [8, %sz1] [1, 1]
        -: 2631:///        : tensor<8x64xf32> to tensor<8x?xf32>
        -: 2632:///   %res = tensor.pad %2 nofold low[0, 0] high[0, %pw1] { ...
        -: 2633:///     } : tensor<8x?xf32> to tensor<8x4xf32>
        -: 2634:/// ```
        -: 2635:///
        -: 2636:/// folds into:
        -: 2637:///
        -: 2638:/// ```mlir
        -: 2639:///   %0 = tensor.extract_slice %input[16, 4] [%sz0, %sz1] [1, 1]
        -: 2640:///        : tensor<64x64xf32> to tensor<?x?xf32>
        -: 2641:///   %res = tensor.pad %0 nofold low[0, 0] high[%pw0, %pw1] { ...
        -: 2642:///     } : tensor<?x?xf32> to tensor<8x4xf32>
        -: 2643:/// ```
        -: 2644:struct FoldOrthogonalPaddings : public OpRewritePattern<PadOp> {
        -: 2645:  using OpRewritePattern<PadOp>::OpRewritePattern;
        -: 2646:
function _ZNK12_GLOBAL__N_122FoldOrthogonalPaddings15matchAndRewriteEN4mlir6tensor5PadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 2647:  LogicalResult matchAndRewrite(PadOp padOp,
        -: 2648:                                PatternRewriter &rewriter) const override {
    #####: 2649:    auto innerSliceOp = padOp.getSource().getDefiningOp<ExtractSliceOp>();
call    0 never executed
call    1 never executed
    #####: 2650:    if (!innerSliceOp)
branch  0 never executed
branch  1 never executed
    #####: 2651:      return failure();
    #####: 2652:    auto outerPadOp = innerSliceOp.getSource().getDefiningOp<PadOp>();
call    0 never executed
call    1 never executed
    #####: 2653:    if (!outerPadOp || outerPadOp.getNofold())
branch  0 never executed
branch  1 never executed
    #####: 2654:      return failure();
    #####: 2655:    auto outerSliceOp = outerPadOp.getSource().getDefiningOp<ExtractSliceOp>();
call    0 never executed
call    1 never executed
    #####: 2656:    if (!outerSliceOp)
branch  0 never executed
branch  1 never executed
    #####: 2657:      return failure();
        -: 2658:
        -: 2659:    // 1) Fail if the chain is rank-reducing.
    #####: 2660:    int64_t rank = padOp.getSourceType().getRank();
call    0 never executed
call    1 never executed
    #####: 2661:    if (outerSliceOp.getSourceType().getRank() != rank) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2662:      return rewriter.notifyMatchFailure(padOp,
    #####: 2663:                                         "cannot fold rank-reducing chain");
call    0 never executed
        -: 2664:    }
        -: 2665:
        -: 2666:    // 2) Fail if the tensor::ExtractSliceOps have non-unit strides.
    #####: 2667:    if (!innerSliceOp.hasUnitStride() || !outerSliceOp.hasUnitStride()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2668:      return rewriter.notifyMatchFailure(
    #####: 2669:          padOp, "cannot fold non-unit stride ExtractSliceOps");
call    0 never executed
        -: 2670:    }
        -: 2671:
        -: 2672:    // 3) Fail if the tensor::PadOps have non-zero low padding.
    #####: 2673:    if (!padOp.hasZeroLowPad() || !outerPadOp.hasZeroLowPad()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2674:      return rewriter.notifyMatchFailure(padOp,
    #####: 2675:                                         "cannot fold PadOps with low padding");
call    0 never executed
        -: 2676:    }
        -: 2677:
        -: 2678:    // 4) Fail if the tensor::PadOps padding values do not match.
    #####: 2679:    Attribute innerAttr, outerAttr;
    #####: 2680:    Value innerValue = padOp.getConstantPaddingValue();
call    0 never executed
    #####: 2681:    Value outerValue = outerPadOp.getConstantPaddingValue();
call    0 never executed
    #####: 2682:    if (!innerValue || !outerValue ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2683:        !matchPattern(innerValue, m_Constant(&innerAttr)) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2684:        !matchPattern(outerValue, m_Constant(&outerAttr)) ||
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2685:        innerAttr != outerAttr) {
branch  0 never executed
branch  1 never executed
    #####: 2686:      return rewriter.notifyMatchFailure(
    #####: 2687:          padOp, "cannot fold PadOps with different padding values");
call    0 never executed
        -: 2688:    }
        -: 2689:
        -: 2690:    // 5) Fail if a dimension is padded by both tensor::PadOps.
    #####: 2691:    llvm::SmallBitVector innerDims = padOp.getPaddedDims();
call    0 never executed
call    1 never executed
    #####: 2692:    llvm::SmallBitVector outerDims = outerPadOp.getPaddedDims();
call    0 never executed
    #####: 2693:    if (innerDims.anyCommon(outerDims)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2694:      return rewriter.notifyMatchFailure(
    #####: 2695:          padOp, "cannot fold PadOps with common padding dimensions");
call    0 never executed
        -: 2696:    }
        -: 2697:
        -: 2698:    // 6) Combine the offsets of the two tensor::ExtractSliceOps. Find the
        -: 2699:    // zero-offset and zero-padding tensor::ExtractSliceOp, tensor::PadOp pair
        -: 2700:    // for every dimension, and use the offset the other pair. Fail if no
        -: 2701:    // zero-offset and zero-padding tensor::ExtractSliceOp, tensor::PadOp pair
        -: 2702:    // exists.
    #####: 2703:    SmallVector<OpFoldResult> newOffsets(rank, rewriter.getIndexAttr(0));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 2704:    for (auto &en : enumerate(newOffsets)) {
branch  0 never executed
branch  1 never executed
    #####: 2705:      OpFoldResult innerOffset = innerSliceOp.getMixedOffsets()[en.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2706:      OpFoldResult outerOffset = outerSliceOp.getMixedOffsets()[en.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2707:      if (!innerDims.test(en.index()) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2708:          (getConstantIntValue(innerOffset) == static_cast<int64_t>(0))) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2709:        en.value() = outerOffset;
    #####: 2710:        continue;
call    0 never executed
        -: 2711:      }
    #####: 2712:      if (!outerDims.test(en.index()) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2713:          (getConstantIntValue(outerOffset) == static_cast<int64_t>(0))) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2714:        en.value() = innerOffset;
    #####: 2715:        continue;
        -: 2716:      }
    #####: 2717:      return rewriter.notifyMatchFailure(
    #####: 2718:          padOp, "cannot find zero-offset and zero-padding pair");
call    0 never executed
        -: 2719:    }
        -: 2720:
        -: 2721:    // 7) Combine the sizes of the two tensor::ExtractSliceOps. Take the size
        -: 2722:    // of the outer tensor::ExtractSliceOp for the dimensions padded by the
        -: 2723:    // outer tensor::PadOp and fail if the size of the inner
        -: 2724:    // tensor::ExtractSliceOp does not match the size of the padded dimension.
        -: 2725:    // Otherwise, take the size of the inner tensor::ExtractSliceOp.
    #####: 2726:    SmallVector<OpFoldResult> newSizes = innerSliceOp.getMixedSizes();
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 2727:    for (auto &en : enumerate(newSizes)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2728:      if (!outerDims.test(en.index()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2729:        continue;
    #####: 2730:      OpFoldResult sliceSize = innerSliceOp.getMixedSizes()[en.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2731:      int64_t sourceSize = innerSliceOp.getSourceType().getShape()[en.index()];
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2732:      assert(!ShapedType::isDynamic(sourceSize) &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -: 2733:             "expected padded dimension to have a static size");
    #####: 2734:      if (getConstantIntValue(sliceSize) != sourceSize) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2735:        return rewriter.notifyMatchFailure(
        -: 2736:            padOp, "cannot fold since the inner ExtractSliceOp size does not "
    #####: 2737:                   "match the size of the outer padding");
call    0 never executed
        -: 2738:      }
    #####: 2739:      en.value() = outerSliceOp.getMixedSizes()[en.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -: 2740:    }
        -: 2741:
        -: 2742:    // Combine the high paddings of the two tensor::PadOps.
    #####: 2743:    SmallVector<OpFoldResult> newHighPad(rank, rewriter.getIndexAttr(0));
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2744:    for (auto &en : enumerate(newHighPad)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 2745:      if (innerDims.test(en.index()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2746:        newHighPad[en.index()] = padOp.getMixedHighPad()[en.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 2747:      if (outerDims.test(en.index()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2748:        newHighPad[en.index()] = outerPadOp.getMixedHighPad()[en.index()];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
        -: 2749:    }
        -: 2750:
        -: 2751:    // Create a new tensor::ExtractSliceOp, tensor::PadOp pair that performs
        -: 2752:    // the two paddings in one step.
    #####: 2753:    auto newSliceOp = rewriter.create<ExtractSliceOp>(
    #####: 2754:        padOp.getLoc(), outerSliceOp.getSource(), newOffsets, newSizes,
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2755:        innerSliceOp.getMixedStrides());
call    0 never executed
call    1 never executed
    #####: 2756:    auto newPadOp = rewriter.create<PadOp>(
    #####: 2757:        padOp.getLoc(), padOp.getResultType(), newSliceOp.getResult(),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2758:        padOp.getMixedLowPad(), newHighPad, padOp.getNofold());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2759:    rewriter.inlineRegionBefore(padOp.getRegion(), newPadOp.getRegion(),
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2760:                                newPadOp.getRegion().begin());
call    0 never executed
call    1 never executed
    #####: 2761:    rewriter.replaceOp(padOp, newPadOp.getResult());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2762:    return success();
branch  0 never executed
branch  1 never executed
        -: 2763:  }
        -: 2764:};
        -: 2765:
        -: 2766:} // namespace
        -: 2767:
function _ZN4mlir6tensor5PadOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1261 returned 100% blocks executed 100%
     1261: 2768:void PadOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -: 2769:                                        MLIRContext *context) {
     1261: 2770:  results.add<FoldStaticZeroPadding, FoldSourceTensorCast, FoldTargetTensorCast,
     1261: 2771:              FoldOrthogonalPaddings>(context);
call    0 returned 100%
     1261: 2772:}
        -: 2773:
        -: 2774:/// Return the padding value of the PadOp if it constant. In this context,
        -: 2775:/// "constant" means an actual constant or "defined outside of the block".
        -: 2776:///
        -: 2777:/// Values are considered constant in three cases:
        -: 2778:///  - A ConstantLike value.
        -: 2779:///  - A basic block argument from a different block.
        -: 2780:///  - A value defined outside of the block.
        -: 2781:///
        -: 2782:/// If the padding value is not constant, an empty Value is returned.
function _ZN4mlir6tensor5PadOp23getConstantPaddingValueEv called 0 returned 0% blocks executed 0%
    #####: 2783:Value PadOp::getConstantPaddingValue() {
    #####: 2784:  auto yieldOp = dyn_cast<YieldOp>(getRegion().front().getTerminator());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####: 2785:  if (!yieldOp)
branch  0 never executed
branch  1 never executed
    #####: 2786:    return {};
    #####: 2787:  Value padValue = yieldOp.getValue();
call    0 never executed
        -: 2788:  // Check if yield value is a constant.
    #####: 2789:  if (matchPattern(padValue, m_Constant()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2790:    return padValue;
        -: 2791:  // Check if yield value is defined inside the PadOp block.
    #####: 2792:  if (padValue.getParentBlock() == &getRegion().front())
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2793:    return {};
        -: 2794:  // Else: Yield value defined outside of the PadOp block.
    #####: 2795:  return padValue;
        -: 2796:}
        -: 2797:
function _ZN4mlir6tensor5PadOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2798:OpFoldResult PadOp::fold(ArrayRef<Attribute>) {
    #####: 2799:  if (getResultType().hasStaticShape() && getResultType() == getSourceType() &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
    #####: 2800:      !getNofold())
    #####: 2801:    return getSource();
call    0 never executed
call    1 never executed
    #####: 2802:  return {};
        -: 2803:}
        -: 2804:
        -: 2805://===----------------------------------------------------------------------===//
        -: 2806:// ParallelInsertSliceOp
        -: 2807://===----------------------------------------------------------------------===//
        -: 2808:
function _ZN4mlir6tensor21ParallelInsertSliceOp15getTiedOpResultEv called 0 returned 0% blocks executed 0%
    #####: 2809:OpResult ParallelInsertSliceOp::getTiedOpResult() {
    #####: 2810:  ParallelCombiningOpInterface parallelCombiningParent =
    #####: 2811:      getParallelCombiningParent();
call    0 never executed
    #####: 2812:  for (const auto &it :
    #####: 2813:       llvm::enumerate(parallelCombiningParent.getYieldingOps())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 2814:    Operation &nextOp = it.value();
call    0 never executed
    #####: 2815:    if (&nextOp == getOperation())
branch  0 never executed
branch  1 never executed
    #####: 2816:      return parallelCombiningParent.getParentResult(it.index());
call    0 never executed
        -: 2817:  }
    #####: 2818:  llvm_unreachable("ParallelInsertSliceOp no tied OpResult found");
        -: 2819:}
        -: 2820:
        -: 2821:// Build a ParallelInsertSliceOp with mixed static and dynamic entries.
function _ZN4mlir6tensor21ParallelInsertSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_N4llvm8ArrayRefINS_12OpFoldResultEEESA_SA_NS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2822:void ParallelInsertSliceOp::build(OpBuilder &b, OperationState &result,
        -: 2823:                                  Value source, Value dest,
        -: 2824:                                  ArrayRef<OpFoldResult> offsets,
        -: 2825:                                  ArrayRef<OpFoldResult> sizes,
        -: 2826:                                  ArrayRef<OpFoldResult> strides,
        -: 2827:                                  ArrayRef<NamedAttribute> attrs) {
    #####: 2828:  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2829:  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2830:  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets,
call    0 never executed
        -: 2831:                             ShapedType::kDynamicStrideOrOffset);
    #####: 2832:  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes,
call    0 never executed
        -: 2833:                             ShapedType::kDynamicSize);
    #####: 2834:  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides,
call    0 never executed
        -: 2835:                             ShapedType::kDynamicStrideOrOffset);
    #####: 2836:  build(b, result, {}, source, dest, dynamicOffsets, dynamicSizes,
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
        -: 2837:        dynamicStrides, b.getI64ArrayAttr(staticOffsets),
        -: 2838:        b.getI64ArrayAttr(staticSizes), b.getI64ArrayAttr(staticStrides));
    #####: 2839:  result.addAttributes(attrs);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2840:}
        -: 2841:
        -: 2842:/// Build an ParallelInsertSliceOp with mixed static and dynamic entries
        -: 2843:/// packed into a Range vector.
function _ZN4mlir6tensor21ParallelInsertSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_N4llvm8ArrayRefINS_5RangeEEENS8_INS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2844:void ParallelInsertSliceOp::build(OpBuilder &b, OperationState &result,
        -: 2845:                                  Value source, Value dest,
        -: 2846:                                  ArrayRef<Range> ranges,
        -: 2847:                                  ArrayRef<NamedAttribute> attrs) {
    #####: 2848:  auto [offsets, sizes, strides] = getOffsetsSizesAndStrides(ranges);
call    0 never executed
call    1 never executed
    #####: 2849:  build(b, result, source, dest, offsets, sizes, strides, attrs);
call    0 never executed
call    1 never executed
    #####: 2850:}
        -: 2851:
        -: 2852:// Build a ParallelInsertSliceOp with dynamic entries.
function _ZN4mlir6tensor21ParallelInsertSliceOp5buildERNS_9OpBuilderERNS_14OperationStateENS_5ValueES6_NS_10ValueRangeES7_S7_N4llvm8ArrayRefINS_14NamedAttributeEEE called 0 returned 0% blocks executed 0%
    #####: 2853:void ParallelInsertSliceOp::build(OpBuilder &b, OperationState &result,
        -: 2854:                                  Value source, Value dest, ValueRange offsets,
        -: 2855:                                  ValueRange sizes, ValueRange strides,
        -: 2856:                                  ArrayRef<NamedAttribute> attrs) {
    #####: 2857:  SmallVector<OpFoldResult> offsetValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 2858:      llvm::map_range(offsets, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 2859:  SmallVector<OpFoldResult> sizeValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 2860:      llvm::map_range(sizes, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2861:  SmallVector<OpFoldResult> strideValues = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
    #####: 2862:      llvm::map_range(strides, [](Value v) -> OpFoldResult { return v; }));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####: 2863:  build(b, result, source, dest, offsetValues, sizeValues, strideValues);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2864:}
        -: 2865:
function _ZN4mlir6tensor21ParallelInsertSliceOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 2866:LogicalResult ParallelInsertSliceOp::verify() {
    #####: 2867:  if (!isa<ParallelCombiningOpInterface>(getOperation()->getParentOp()))
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2868:    return this->emitError("expected ParallelCombiningOpInterface parent, got:")
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2869:           << *(getOperation()->getParentOp());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -: 2870:
    #####: 2871:  ShapedType expectedType;
call    0 never executed
    #####: 2872:  SliceVerificationResult result =
call    0 never executed
    #####: 2873:      verifyInsertSliceOp(getSourceType(), getDestType(), getStaticOffsets(),
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
        -: 2874:                          getStaticSizes(), getStaticStrides(), &expectedType);
    #####: 2875:  return produceSliceErrorMsg(result, *this, expectedType);
call    0 never executed
        -: 2876:}
        -: 2877:
function _ZN4mlir6tensor21ParallelInsertSliceOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235: 2878:void ParallelInsertSliceOp::getCanonicalizationPatterns(
        -: 2879:    RewritePatternSet &results, MLIRContext *context) {
     1235: 2880:  results.add<InsertSliceOpConstantArgumentFolder<ParallelInsertSliceOp>,
        -: 2881:              InsertSliceOpCastFolder<ParallelInsertSliceOp>,
     1235: 2882:              InsertSliceOpSourceCastInserter<ParallelInsertSliceOp>>(context);
call    0 returned 100%
     1235: 2883:}
        -: 2884:
        -: 2885://===----------------------------------------------------------------------===//
        -: 2886:// ScatterOp
        -: 2887://===----------------------------------------------------------------------===//
        -: 2888:
function _ZN4mlir6tensor9ScatterOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 0 returned 0% blocks executed 0%
    #####: 2889:void ScatterOp::getAsmResultNames(
        -: 2890:    function_ref<void(Value, StringRef)> setNameFn) {
    #####: 2891:  setNameFn(getResult(), "scatter");
call    0 never executed
call    1 never executed
    #####: 2892:}
        -: 2893:
function _ZN4mlir6tensor9ScatterOp6verifyEv called 0 returned 0% blocks executed 0%
    #####: 2894:LogicalResult ScatterOp::verify() {
    #####: 2895:  int64_t destRank = getDestType().getRank();
call    0 never executed
call    1 never executed
    #####: 2896:  ArrayRef<int64_t> scatterDims = getScatterDims();
call    0 never executed
    #####: 2897:  if (failed(verifyGatherOrScatterDims(getOperation(), scatterDims, destRank,
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 2898:                                       "scatter", "dest")))
    #####: 2899:    return failure();
        -: 2900:
    #####: 2901:  if (!getUnique())
call    0 never executed
    #####: 2902:    return emitOpError("requires 'unique' attribute to be set");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -: 2903:  // TODO: we could also check statically that there are fewer leading index
        -: 2904:  // tensor dims than the dest dims. If this is not the case, the unique
        -: 2905:  // attribute cannot be true.
        -: 2906:
        -: 2907:  // Use the GatherOp::inferResultType on the `dest` type and verify the
        -: 2908:  // expected type matches the source type.
    #####: 2909:  RankedTensorType expectedSourceType = GatherOp::inferResultType(
    #####: 2910:      getDestType(), getIndicesType(), scatterDims, /*rankReduced=*/false);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2911:  RankedTensorType expectedRankReducedSourceType = GatherOp::inferResultType(
    #####: 2912:      getDestType(), getIndicesType(), scatterDims, /*rankReduced=*/true);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 2913:  if (getSourceType() != expectedSourceType &&
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 2914:      getSourceType() != expectedRankReducedSourceType) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 2915:    return emitOpError("source type "
call    0 never executed
call    1 never executed
call    2 never executed
        -: 2916:                       "mismatch: "
        -: 2917:                       "expected ")
    #####: 2918:           << expectedSourceType << " or its rank-reduced variant "
call    0 never executed
call    1 never executed
    #####: 2919:           << expectedRankReducedSourceType << " (got: " << getSourceType()
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####: 2920:           << ")";
call    0 never executed
        -: 2921:  }
        -: 2922:
    #####: 2923:  return success();
        -: 2924:}
        -: 2925:
        -: 2926://===----------------------------------------------------------------------===//
        -: 2927:// SplatOp
        -: 2928://===----------------------------------------------------------------------===//
        -: 2929:
function _ZN4mlir6tensor7SplatOp17getAsmResultNamesEN4llvm12function_refIFvNS_5ValueENS2_9StringRefEEEE called 40659214 returned 100% blocks executed 100%
 40659214: 2930:void SplatOp::getAsmResultNames(
        -: 2931:    function_ref<void(Value, StringRef)> setNameFn) {
 40659214: 2932:  setNameFn(getResult(), "splat");
call    0 returned 100%
 40659214: 2933:}
        -: 2934:
function _ZN4mlir6tensor7SplatOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 191286 returned 100% blocks executed 92%
   191286: 2935:OpFoldResult SplatOp::fold(ArrayRef<Attribute> operands) {
   191286: 2936:  auto constOperand = operands.front();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
   357356: 2937:  if (!constOperand.isa_and_nonnull<IntegerAttr, FloatAttr>())
branch  0 taken 87% (fallthrough)
branch  1 taken 13%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    25216: 2938:    return {};
        -: 2939:
        -: 2940:  // SplatElementsAttr::get treats single value for second arg as being a
        -: 2941:  // splat.
   166070: 2942:  return SplatElementsAttr::get(getType(), {constOperand});
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
        -: 2943:}
        -: 2944:
        -: 2945://===----------------------------------------------------------------------===//
        -: 2946:// TableGen'd op method definitions
        -: 2947://===----------------------------------------------------------------------===//
        -: 2948:
        -: 2949:#define GET_OP_CLASSES
        -: 2950:#include "mlir/Dialect/Tensor/IR/TensorOps.cpp.inc"
