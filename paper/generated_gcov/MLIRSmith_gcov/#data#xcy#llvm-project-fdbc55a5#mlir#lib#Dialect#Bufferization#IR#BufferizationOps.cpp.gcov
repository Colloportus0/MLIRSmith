        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Bufferization/IR/BufferizationOps.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Bufferization/IR/CMakeFiles/obj.MLIRBufferizationDialect.dir/BufferizationOps.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Bufferization/IR/CMakeFiles/obj.MLIRBufferizationDialect.dir/BufferizationOps.cpp.gcda
        -:    0:Runs:116157
        -:    1://===----------------------------------------------------------------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8:
        -:    9:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   10:#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
        -:   11:#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
        -:   12:#include "mlir/Dialect/Func/IR/FuncOps.h"
        -:   13:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   14:#include "mlir/Dialect/MemRef/Utils/MemRefUtils.h"
        -:   15:#include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
        -:   16:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   17:#include "mlir/IR/Matchers.h"
        -:   18:
        -:   19:using namespace mlir;
        -:   20:using namespace mlir::bufferization;
        -:   21:
        -:   22://===----------------------------------------------------------------------===//
        -:   23:// Helper functions
        -:   24://===----------------------------------------------------------------------===//
        -:   25:
        -:   26:FailureOr<Value>
function _ZN4mlir13bufferization24castOrReallocMemRefValueERNS_9OpBuilderENS_5ValueENS_10MemRefTypeE called 0 returned 0% blocks executed 0%
    #####:   27:mlir::bufferization::castOrReallocMemRefValue(OpBuilder &b, Value value,
        -:   28:                                              MemRefType destType) {
    #####:   29:  auto srcType = value.getType().cast<MemRefType>();
call    0 never executed
        -:   30:
        -:   31:  // Element type, rank and memory space must match.
    #####:   32:  if (srcType.getElementType() != destType.getElementType())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   33:    return failure();
    #####:   34:  if (srcType.getMemorySpaceAsInt() != destType.getMemorySpaceAsInt())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   35:    return failure();
    #####:   36:  if (srcType.getRank() != destType.getRank())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   37:    return failure();
        -:   38:
        -:   39:  // In case the affine maps are different, we may need to use a copy if we go
        -:   40:  // from dynamic to static offset or stride (the canonicalization cannot know
        -:   41:  // at this point that it is really cast compatible).
function _ZZN4mlir13bufferization24castOrReallocMemRefValueERNS_9OpBuilderENS_5ValueENS_10MemRefTypeEENKUlS4_S4_E_clES4_S4_.isra.0 called 0 returned 0% blocks executed 0%
    #####:   42:  auto isGuaranteedCastCompatible = [](MemRefType source, MemRefType target) {
    #####:   43:    int64_t sourceOffset, targetOffset;
    #####:   44:    SmallVector<int64_t, 4> sourceStrides, targetStrides;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   45:    if (failed(getStridesAndOffset(source, sourceStrides, sourceOffset)) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   46:        failed(getStridesAndOffset(target, targetStrides, targetOffset)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   47:      return false;
    #####:   48:    auto dynamicToStatic = [](int64_t a, int64_t b) {
    #####:   49:      return a == MemRefType::getDynamicStrideOrOffset() &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
        -:   50:             b != MemRefType::getDynamicStrideOrOffset();
        -:   51:    };
    #####:   52:    if (dynamicToStatic(sourceOffset, targetOffset))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:   53:      return false;
    #####:   54:    for (auto it : zip(sourceStrides, targetStrides))
branch  0 never executed
branch  1 never executed
    #####:   55:      if (dynamicToStatic(std::get<0>(it), std::get<1>(it)))
branch  0 never executed
branch  1 never executed
    #####:   56:        return false;
    #####:   57:    return true;
        -:   58:  };
        -:   59:
        -:   60:  // Note: If `areCastCompatible`, a cast is valid, but may fail at runtime. To
        -:   61:  // ensure that we only generate casts that always succeed at runtime, we check
        -:   62:  // a fix extra conditions in `isGuaranteedCastCompatible`.
    #####:   63:  if (memref::CastOp::areCastCompatible(srcType, destType) &&
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:   64:      isGuaranteedCastCompatible(srcType, destType)) {
call    0 never executed
    #####:   65:    Value casted = b.create<memref::CastOp>(value.getLoc(), destType, value);
call    0 never executed
call    1 never executed
    #####:   66:    return casted;
        -:   67:  }
        -:   68:
    #####:   69:  auto loc = value.getLoc();
call    0 never executed
    #####:   70:  SmallVector<Value, 4> dynamicOperands;
    #####:   71:  for (int i = 0; i < destType.getRank(); ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   72:    if (destType.getShape()[i] != ShapedType::kDynamicSize)
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:   73:      continue;
    #####:   74:    auto index = b.createOrFold<arith::ConstantIndexOp>(loc, i);
call    0 never executed
    #####:   75:    Value size = b.create<memref::DimOp>(loc, value, index);
call    0 never executed
call    1 never executed
    #####:   76:    dynamicOperands.push_back(size);
call    0 never executed
        -:   77:  }
        -:   78:  // TODO: Use alloc/memcpy callback from BufferizationOptions if called via
        -:   79:  // BufferizableOpInterface impl of ToMemrefOp.
    #####:   80:  Value copy = b.create<memref::AllocOp>(loc, destType, dynamicOperands);
call    0 never executed
call    1 never executed
    #####:   81:  b.create<memref::CopyOp>(loc, value, copy);
call    0 never executed
    #####:   82:  return copy;
branch  0 never executed
branch  1 never executed
        -:   83:}
        -:   84:
        -:   85:/// Try to fold to_memref(to_tensor(x)). If x's type and the result type of the
        -:   86:/// to_memref op are different, a memref.cast is needed.
        -:   87:LogicalResult
function _ZN4mlir13bufferization24foldToMemrefToTensorPairERNS_12RewriterBaseENS0_10ToMemrefOpE called 104603 returned 100% blocks executed 31%
   104603:   88:mlir::bufferization::foldToMemrefToTensorPair(RewriterBase &rewriter,
        -:   89:                                              ToMemrefOp toMemref) {
   104603:   90:  auto memrefToTensor = toMemref.getTensor().getDefiningOp<ToTensorOp>();
call    0 returned 100%
call    1 returned 100%
   104603:   91:  if (!memrefToTensor)
branch  0 taken 94% (fallthrough)
branch  1 taken 6%
    98749:   92:    return failure();
        -:   93:
     5854:   94:  Type srcType = memrefToTensor.getMemref().getType();
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
     5854:   95:  Type destType = toMemref.getType();
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        -:   96:
        -:   97:  // Directly rewrite if the type did not change.
     5854:   98:  if (srcType == destType) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
     5854:   99:    rewriter.replaceOp(toMemref, memrefToTensor.getMemref());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
     5854:  100:    return success();
        -:  101:  }
        -:  102:
    #####:  103:  auto rankedSrcType = srcType.dyn_cast<MemRefType>();
call    0 never executed
    #####:  104:  auto rankedDestType = destType.dyn_cast<MemRefType>();
call    0 never executed
    #####:  105:  auto unrankedSrcType = srcType.dyn_cast<UnrankedMemRefType>();
call    0 never executed
        -:  106:
        -:  107:  // Ranked memref -> Ranked memref cast.
    #####:  108:  if (rankedSrcType && rankedDestType) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  109:    FailureOr<Value> replacement = castOrReallocMemRefValue(
call    0 never executed
    #####:  110:        rewriter, memrefToTensor.getMemref(), rankedDestType);
call    0 never executed
    #####:  111:    if (failed(replacement))
branch  0 never executed
branch  1 never executed
    #####:  112:      return failure();
        -:  113:
    #####:  114:    rewriter.replaceOp(toMemref, *replacement);
call    0 never executed
call    1 never executed
    #####:  115:    return success();
        -:  116:  }
        -:  117:
        -:  118:  // Unranked memref -> Ranked memref cast: May require a copy.
        -:  119:  // TODO: Not implemented at the moment.
    #####:  120:  if (unrankedSrcType && rankedDestType)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  121:    return failure();
        -:  122:
        -:  123:  // Unranked memref -> unranked memref cast
        -:  124:  // Ranked memref -> unranked memref cast: No copy needed.
    #####:  125:  assert(memref::CastOp::areCastCompatible(srcType, destType) &&
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
        -:  126:         "expected that types are cast compatible");
    #####:  127:  rewriter.replaceOpWithNewOp<memref::CastOp>(toMemref, destType,
call    0 never executed
    #####:  128:                                              memrefToTensor.getMemref());
call    0 never executed
    #####:  129:  return success();
        -:  130:}
        -:  131:
function _ZN4mlir13bufferization23populateDynamicDimSizesERNS_9OpBuilderENS_8LocationENS_5ValueERN4llvm11SmallVectorIS4_Lj6EEE called 12520 returned 100% blocks executed 95%
    12520:  132:void mlir::bufferization::populateDynamicDimSizes(
        -:  133:    OpBuilder &b, Location loc, Value shapedValue,
        -:  134:    SmallVector<Value> &dynamicDims) {
    12520:  135:  auto shapedType = shapedValue.getType().cast<ShapedType>();
call    0 returned 100%
    36141:  136:  for (int64_t i = 0; i < shapedType.getRank(); ++i) {
call    0 returned 100%
branch  1 taken 65% (fallthrough)
branch  2 taken 35%
    23621:  137:    if (shapedType.isDynamicDim(i)) {
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 99%
      197:  138:      if (shapedType.isa<MemRefType>()) {
call    0 returned 100%
branch  1 taken 96% (fallthrough)
branch  2 taken 4%
      189:  139:        dynamicDims.push_back(b.create<memref::DimOp>(loc, shapedValue, i));
call    0 returned 100%
call    1 returned 100%
        -:  140:      } else {
       8*:  141:        assert(shapedType.isa<RankedTensorType>() && "expected tensor");
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
        8:  142:        dynamicDims.push_back(b.create<tensor::DimOp>(loc, shapedValue, i));
call    0 returned 100%
call    1 returned 100%
        -:  143:      }
        -:  144:    }
        -:  145:  }
    12520:  146:}
        -:  147:
        -:  148://===----------------------------------------------------------------------===//
        -:  149:// AllocTensorOp
        -:  150://===----------------------------------------------------------------------===//
        -:  151:
function _ZN4mlir13bufferization13AllocTensorOp9bufferizeERNS_12RewriterBaseERKNS0_20BufferizationOptionsE called 15581 returned 100% blocks executed 81%
    15581:  152:LogicalResult AllocTensorOp::bufferize(RewriterBase &rewriter,
        -:  153:                                       const BufferizationOptions &options) {
    15581:  154:  OpBuilder::InsertionGuard g(rewriter);
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    15581:  155:  Location loc = getLoc();
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        -:  156:
        -:  157:  // Nothing to do for dead AllocTensorOps.
    31162:  158:  if (getOperation()->getUses().empty()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 1% (fallthrough)
branch  3 taken 100%
        1:  159:    rewriter.eraseOp(getOperation());
call    0 returned 100%
        1:  160:    return success();
        -:  161:  }
        -:  162:
        -:  163:  // Get "copy" buffer.
    15580:  164:  Value copyBuffer;
    15580:  165:  if (getCopy()) {
call    0 returned 100%
branch  1 taken 46% (fallthrough)
branch  2 taken 54%
     7118:  166:    FailureOr<Value> maybeCopyBuffer = getBuffer(rewriter, getCopy(), options);
call    0 returned 100%
call    1 returned 100%
     7118:  167:    if (failed(maybeCopyBuffer))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  168:      return failure();
     7118:  169:    copyBuffer = *maybeCopyBuffer;
        -:  170:  }
        -:  171:
        -:  172:  // Create memory allocation.
    15580:  173:  auto allocType = bufferization::getBufferType(getResult(), options);
call    0 returned 100%
call    1 returned 100%
    15580:  174:  if (failed(allocType))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  175:    return failure();
    31161:  176:  SmallVector<Value> dynamicDims = getDynamicSizes();
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
    15580:  177:  if (getCopy()) {
call    0 returned 100%
branch  1 taken 46% (fallthrough)
branch  2 taken 54%
    7118*:  178:    assert(dynamicDims.empty() && "expected either `copy` or `dynamicDims`");
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
     7118:  179:    populateDynamicDimSizes(rewriter, loc, copyBuffer, dynamicDims);
call    0 returned 100%
        -:  180:  }
    15580:  181:  FailureOr<Value> alloc = options.createAlloc(
call    0 returned 100%
    15580:  182:      rewriter, loc, allocType->cast<MemRefType>(), dynamicDims);
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
call    3 returned 100%
    15580:  183:  if (failed(alloc))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  184:    return failure();
        -:  185:
        -:  186:  // Create memory copy (if any).
    15580:  187:  if (getCopy()) {
call    0 returned 100%
branch  1 taken 46% (fallthrough)
branch  2 taken 54%
     7118:  188:    if (failed(options.createMemCpy(rewriter, loc, copyBuffer, *alloc)))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####:  189:      return failure();
        -:  190:  }
        -:  191:
        -:  192:  // Should the buffer be deallocated?
    15580:  193:  bool dealloc =
    15580:  194:      shouldDeallocateOpResult(getResult().cast<OpResult>(), options);
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -:  195:
        -:  196:  // Replace op.
    15580:  197:  replaceOpWithBufferizedValues(rewriter, getOperation(), *alloc);
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
call    3 returned 100%
        -:  198:
        -:  199:  // Create buffer deallocation (if requested).
    15580:  200:  if (!dealloc)
branch  0 taken 99% (fallthrough)
branch  1 taken 1%
    15451:  201:    return success();
        -:  202:
      129:  203:  rewriter.setInsertionPoint(rewriter.getInsertionBlock()->getTerminator());
call    0 returned 100%
call    1 returned 100%
      129:  204:  if (failed(options.createDealloc(rewriter, loc, *alloc)))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####:  205:    return failure();
    15580:  206:  return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  207:}
        -:  208:
function _ZN4mlir13bufferization13AllocTensorOp13isMemoryWriteENS_8OpResultERKNS0_13AnalysisStateE called 268 returned 100% blocks executed 100%
      268:  209:bool AllocTensorOp::isMemoryWrite(OpResult opResult,
        -:  210:                                  const AnalysisState &state) {
        -:  211:  // AllocTensorOps do not write unless they have a `copy` value.
      268:  212:  return static_cast<bool>(getCopy());
call    0 returned 100%
        -:  213:}
        -:  214:
function _ZN4mlir13bufferization13AllocTensorOp22bufferizesToMemoryReadERNS_9OpOperandERKNS0_13AnalysisStateE called 0 returned 0% blocks executed 0%
    #####:  215:bool AllocTensorOp::bufferizesToMemoryRead(OpOperand &opOperand,
        -:  216:                                           const AnalysisState &state) {
    #####:  217:  assert(opOperand.getOperandNumber() == getNumOperands() - 1 &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
        -:  218:         "expected copy operand");
    #####:  219:  return true;
        -:  220:}
        -:  221:
function _ZN4mlir13bufferization13AllocTensorOp23bufferizesToMemoryWriteERNS_9OpOperandERKNS0_13AnalysisStateE called 0 returned 0% blocks executed 0%
    #####:  222:bool AllocTensorOp::bufferizesToMemoryWrite(OpOperand &opOperand,
        -:  223:                                            const AnalysisState &state) {
    #####:  224:  assert(opOperand.getOperandNumber() == getNumOperands() - 1 &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
        -:  225:         "expected copy operand");
    #####:  226:  return false;
        -:  227:}
        -:  228:
        -:  229:SmallVector<OpResult>
function _ZN4mlir13bufferization13AllocTensorOp19getAliasingOpResultERNS_9OpOperandERKNS0_13AnalysisStateE called 0 returned 0% blocks executed 0%
    #####:  230:AllocTensorOp::getAliasingOpResult(OpOperand &opOperand,
        -:  231:                                   const AnalysisState &state) {
        -:  232:  // This is a new allocation. It does not alias with any other buffer.
    #####:  233:  return {};
        -:  234:}
        -:  235:
function _ZN4mlir13bufferization13AllocTensorOp13getBufferTypeENS_5ValueERKNS0_20BufferizationOptionsERKN4llvm8DenseMapIS2_NS_14BaseMemRefTypeENS6_12DenseMapInfoIS2_vEENS6_6detail12DenseMapPairIS2_S8_EEEE called 15600 returned 100% blocks executed 74%
    15600:  236:FailureOr<BaseMemRefType> AllocTensorOp::getBufferType(
        -:  237:    Value value, const BufferizationOptions &options,
        -:  238:    const DenseMap<Value, BaseMemRefType> &fixedTypes) {
   15600*:  239:  assert(value == getResult() && "invalid value");
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 never executed
        -:  240:
        -:  241:  // Compute memory space of this allocation.
    15600:  242:  unsigned memorySpace;
    15600:  243:  if (getMemorySpace().has_value()) {
call    0 returned 100%
branch  1 taken 53% (fallthrough)
branch  2 taken 47%
     8296:  244:    memorySpace = *getMemorySpace();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
     7304:  245:  } else if (getCopy()) {
call    0 returned 100%
branch  1 taken 98% (fallthrough)
branch  2 taken 2%
     7122:  246:    auto copyBufferType =
     7122:  247:        bufferization::getBufferType(getCopy(), options, fixedTypes);
call    0 returned 100%
call    1 returned 100%
     7122:  248:    if (failed(copyBufferType))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  249:      return failure();
     7122:  250:    memorySpace = copyBufferType->getMemorySpaceAsInt();
call    0 returned 100%
      182:  251:  } else if (options.defaultMemorySpace.has_value()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
      182:  252:    memorySpace = *options.defaultMemorySpace;
        -:  253:  } else {
    #####:  254:    return getOperation()->emitError("could not infer memory space");
call    0 never executed
call    1 never executed
call    2 never executed
        -:  255:  }
        -:  256:
    15600:  257:  return getMemRefTypeWithStaticIdentityLayout(getType(), memorySpace);
call    0 returned 100%
call    1 returned 100%
        -:  258:}
        -:  259:
function _ZN4mlir13bufferization13AllocTensorOp6verifyEv called 34833 returned 100% blocks executed 39%
    34833:  260:LogicalResult AllocTensorOp::verify() {
   34906*:  261:  if (getCopy() && !getDynamicSizes().empty())
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 100%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####:  262:    return emitError("dynamic sizes not needed when copying a tensor");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
   34833*:  263:  if (!getCopy() && getType().getNumDynamicDims() !=
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 1%
call    3 returned 100%
call    4 returned 100%
call    5 returned 100%
branch  6 taken 100% (fallthrough)
branch  7 taken 0%
    34760:  264:                        static_cast<int64_t>(getDynamicSizes().size()))
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    #####:  265:    return emitError("expected ")
call    0 never executed
call    1 never executed
    #####:  266:           << getType().getNumDynamicDims() << " dynamic sizes";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
   34833*:  267:  if (getCopy() && getCopy().getType() != getType())
call    0 returned 100%
branch  1 taken 1%
branch  2 taken 100%
call    3 returned 100%
call    4 returned 100%
call    5 returned 100%
branch  6 taken 100% (fallthrough)
branch  7 taken 0%
    #####:  268:    return emitError("expected that `copy` and return type match");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  269:
        -:  270:  // For sparse tensor allocation, we require that none of its
        -:  271:  // uses escapes the function boundary directly.
    34833:  272:  if (sparse_tensor::getSparseTensorEncoding(getType())) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0%
branch  3 taken 100%
    #####:  273:    for (auto &use : getOperation()->getUses())
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  274:      if (isa<func::ReturnOp, func::CallOp, func::CallIndirectOp>(
    #####:  275:              use.getOwner()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  276:        return emitError("sparse tensor allocation should not escape function");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  277:  }
        -:  278:
    34833:  279:  return success();
        -:  280:}
        -:  281:
function _ZN4mlir13bufferization13AllocTensorOp5buildERNS_9OpBuilderERNS_14OperationStateENS_16RankedTensorTypeENS_10ValueRangeE called 6773 returned 100% blocks executed 100%
     6773:  282:void AllocTensorOp::build(OpBuilder &builder, OperationState &result,
        -:  283:                          RankedTensorType type, ValueRange dynamicSizes) {
     6773:  284:  build(builder, result, type, dynamicSizes, /*copy=*/Value(),
call    0 returned 100%
        -:  285:        /*memory_space=*/IntegerAttr());
     6773:  286:}
        -:  287:
function _ZN4mlir13bufferization13AllocTensorOp5buildERNS_9OpBuilderERNS_14OperationStateENS_16RankedTensorTypeENS_10ValueRangeENS_5ValueE called 26779 returned 100% blocks executed 100%
   26779*:  288:void AllocTensorOp::build(OpBuilder &builder, OperationState &result,
        -:  289:                          RankedTensorType type, ValueRange dynamicSizes,
        -:  290:                          Value copy) {
   26779*:  291:  build(builder, result, type, dynamicSizes, copy,
call    0 never executed
call    1 never executed
call    2 returned 100%
        -:  292:        /*memory_space=*/IntegerAttr());
    26779:  293:}
        -:  294:
        -:  295:namespace {
        -:  296:/// Change the type of the result of a `bufferization.alloc_tensor` by making
        -:  297:/// the result type statically sized along dimension that in the original
        -:  298:/// operation where defined as dynamic, but the size was defined using a
        -:  299:/// `constant` op. For example:
        -:  300:///
        -:  301:///  %c5 = arith.constant 5: index
        -:  302:///  %0 = bufferization.alloc_tensor(%arg0, %c5) : tensor<?x?xf32>
        -:  303:///
        -:  304:///  to
        -:  305:///
        -:  306:///  %0 = bufferization.alloc_tensor(%arg0) : tensor<?x5xf32>
        -:  307:struct ReplaceStaticShapeDims : OpRewritePattern<AllocTensorOp> {
        -:  308:  using OpRewritePattern<AllocTensorOp>::OpRewritePattern;
        -:  309:
function _ZNK12_GLOBAL__N_122ReplaceStaticShapeDims15matchAndRewriteEN4mlir13bufferization13AllocTensorOpERNS1_15PatternRewriterE called 71 returned 100% blocks executed 57%
       71:  310:  LogicalResult matchAndRewrite(AllocTensorOp op,
        -:  311:                                PatternRewriter &rewriter) const override {
       71:  312:    if (op.getCopy())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  313:      return failure();
       71:  314:    SmallVector<int64_t> newShape = llvm::to_vector(op.getType().getShape());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
      142:  315:    SmallVector<Value> newDynamicSizes;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
       71:  316:    unsigned int dynValCounter = 0;
      221:  317:    for (int64_t i = 0; i < op.getType().getRank(); ++i) {
call    0 returned 100%
call    1 returned 100%
branch  2 taken 68% (fallthrough)
branch  3 taken 32%
      300:  318:      if (!op.isDynamicDim(i))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
      150:  319:        continue;
    #####:  320:      Value value = op.getDynamicSizes()[dynValCounter++];
call    0 never executed
call    1 never executed
    #####:  321:      APInt intVal;
call    0 never executed
    #####:  322:      if (matchPattern(value, m_ConstantInt(&intVal))) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  323:        newShape[i] = intVal.getSExtValue();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  324:      } else {
    #####:  325:        newDynamicSizes.push_back(value);
call    0 never executed
        -:  326:      }
        -:  327:    }
       71:  328:    RankedTensorType newType = RankedTensorType::get(
       71:  329:        newShape, op.getType().getElementType(), op.getType().getEncoding());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
call    4 returned 100%
       71:  330:    if (newType == op.getType())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
       71:  331:      return failure();
    #####:  332:    auto newOp = rewriter.create<AllocTensorOp>(
    #####:  333:        op.getLoc(), newType, newDynamicSizes, /*copy=*/Value());
call    0 never executed
    #####:  334:    rewriter.replaceOpWithNewOp<tensor::CastOp>(op, op.getType(), newOp);
call    0 never executed
call    1 never executed
      71*:  335:    return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  336:  }
        -:  337:};
        -:  338:
        -:  339:struct FoldDimOfAllocTensorOp : public OpRewritePattern<tensor::DimOp> {
        -:  340:  using OpRewritePattern<tensor::DimOp>::OpRewritePattern;
        -:  341:
function _ZNK12_GLOBAL__N_122FoldDimOfAllocTensorOp15matchAndRewriteEN4mlir6tensor5DimOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  342:  LogicalResult matchAndRewrite(tensor::DimOp dimOp,
        -:  343:                                PatternRewriter &rewriter) const override {
    #####:  344:    Optional<int64_t> maybeConstantIndex = dimOp.getConstantIndex();
call    0 never executed
    #####:  345:    auto allocTensorOp = dimOp.getSource().getDefiningOp<AllocTensorOp>();
call    0 never executed
call    1 never executed
    #####:  346:    if (!allocTensorOp || !maybeConstantIndex)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  347:      return failure();
    #####:  348:    if (!allocTensorOp.getType().isDynamicDim(*maybeConstantIndex))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  349:      return failure();
    #####:  350:    rewriter.replaceOp(
call    0 never executed
    #####:  351:        dimOp, allocTensorOp.getDynamicSize(rewriter, *maybeConstantIndex));
call    0 never executed
call    1 never executed
    #####:  352:    return success();
        -:  353:  }
        -:  354:};
        -:  355:} // namespace
        -:  356:
function _ZN4mlir13bufferization13AllocTensorOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235:  357:void AllocTensorOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  358:                                                MLIRContext *ctx) {
     1235:  359:  results.add<FoldDimOfAllocTensorOp, ReplaceStaticShapeDims>(ctx);
call    0 returned 100%
     1235:  360:}
        -:  361:
function _ZN4mlir13bufferization13AllocTensorOp17reifyResultShapesERNS_9OpBuilderERN4llvm11SmallVectorINS5_INS_5ValueELj6EEELj1EEE called 4 returned 100% blocks executed 88%
        4:  362:LogicalResult AllocTensorOp::reifyResultShapes(
        -:  363:    OpBuilder &builder, ReifiedRankedShapedTypeDims &reifiedReturnShapes) {
        8:  364:  auto shapes = llvm::to_vector<4>(llvm::map_range(
function _ZZN4mlir13bufferization13AllocTensorOp17reifyResultShapesERNS_9OpBuilderERN4llvm11SmallVectorINS5_INS_5ValueELj6EEELj1EEEENKUllE_clEl.isra.0 called 12 returned 100% blocks executed 89%
       16:  365:      llvm::seq<int64_t>(0, getType().getRank()), [&](int64_t dim) -> Value {
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
      24*:  366:        if (isDynamicDim(dim))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  367:          return getDynamicSize(builder, dim);
call    0 never executed
       12:  368:        return builder.create<arith::ConstantIndexOp>(getLoc(),
       12:  369:                                                      getStaticSize(dim));
call    0 returned 100%
call    1 returned 100%
        4:  370:      }));
call    0 returned 100%
        4:  371:  reifiedReturnShapes.emplace_back(std::move(shapes));
call    0 returned 100%
        4:  372:  return success();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
        -:  373:}
        -:  374:
function _ZN4mlir13bufferization13AllocTensorOp5parseERNS_11OpAsmParserERNS_14OperationStateE called 0 returned 0% blocks executed 0%
    #####:  375:ParseResult AllocTensorOp::parse(OpAsmParser &parser, OperationState &result) {
    #####:  376:  SmallVector<OpAsmParser::UnresolvedOperand> dynamicSizesOperands;
call    0 never executed
    #####:  377:  if (parser.parseLParen() || parser.parseOperandList(dynamicSizesOperands) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:  378:      parser.parseRParen())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  379:    return failure();
    #####:  380:  ParseResult copyKeyword = parser.parseOptionalKeyword("copy");
call    0 never executed
    #####:  381:  OpAsmParser::UnresolvedOperand copyOperand;
branch  0 never executed
branch  1 never executed
    #####:  382:  if (copyKeyword.succeeded())
branch  0 never executed
branch  1 never executed
    #####:  383:    if (parser.parseLParen() || parser.parseOperand(copyOperand) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:  384:        parser.parseRParen())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  385:      return failure();
    #####:  386:  if (parser.parseOptionalAttrDict(result.attributes) || parser.parseColon())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  387:    return failure();
        -:  388:
    #####:  389:  TensorType type;
    #####:  390:  if (parser.parseCustomTypeWithFallback(type))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  391:    return failure();
    #####:  392:  result.addTypes(type);
call    0 never executed
        -:  393:
    #####:  394:  Type indexType = parser.getBuilder().getIndexType();
call    0 never executed
call    1 never executed
    #####:  395:  if (parser.resolveOperands(dynamicSizesOperands, indexType, result.operands))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  396:    return failure();
    #####:  397:  if (copyKeyword.succeeded())
branch  0 never executed
branch  1 never executed
    #####:  398:    if (parser.resolveOperand(copyOperand, type, result.operands))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  399:      return failure();
    #####:  400:  result.addAttribute(AllocTensorOp::getOperandSegmentSizeAttr(),
call    0 never executed
    #####:  401:                      parser.getBuilder().getDenseI32ArrayAttr(
call    0 never executed
    #####:  402:                          {static_cast<int32_t>(dynamicSizesOperands.size()),
call    0 never executed
    #####:  403:                           static_cast<int32_t>(copyKeyword.succeeded())}));
call    0 never executed
    #####:  404:  return success();
        -:  405:}
        -:  406:
function _ZN4mlir13bufferization13AllocTensorOp5printERNS_12OpAsmPrinterE called 13653 returned 100% blocks executed 93%
    13653:  407:void AllocTensorOp::print(OpAsmPrinter &p) {
    27306:  408:  p << "(" << getDynamicSizes() << ")";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
    13653:  409:  if (getCopy())
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 100%
      144:  410:    p << " copy(" << getCopy() << ")";
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
    13653:  411:  p.printOptionalAttrDict((*this)->getAttrs(), /*elidedAttrs=*/{
call    0 returned 100%
    13653:  412:                              AllocTensorOp::getOperandSegmentSizeAttr()});
call    0 returned 100%
    13653:  413:  p << " : ";
call    0 returned 100%
    13653:  414:  auto type = getResult().getType();
call    0 returned 100%
call    1 returned 100%
    13653:  415:  if (auto validType = type.dyn_cast<::mlir::TensorType>())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
    13653:  416:    p.printStrippedAttrOrType(validType);
call    0 returned 100%
        -:  417:  else
    #####:  418:    p << type;
call    0 never executed
    13653:  419:}
        -:  420:
function _ZN4mlir13bufferization13AllocTensorOp14getDynamicSizeERNS_9OpBuilderEj called 0 returned 0% blocks executed 0%
    #####:  421:Value AllocTensorOp::getDynamicSize(OpBuilder &b, unsigned idx) {
    #####:  422:  assert(isDynamicDim(idx) && "expected dynamic dim");
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  423:  if (getCopy())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  424:    return b.create<tensor::DimOp>(getLoc(), getCopy(), idx);
call    0 never executed
call    1 never executed
    #####:  425:  return getOperand(getIndexOfDynamicSize(idx));
call    0 never executed
call    1 never executed
        -:  426:}
        -:  427:
        -:  428://===----------------------------------------------------------------------===//
        -:  429:// CloneOp
        -:  430://===----------------------------------------------------------------------===//
        -:  431:
function _ZN4mlir13bufferization7CloneOp10getEffectsERN4llvm15SmallVectorImplINS_11SideEffects14EffectInstanceINS_13MemoryEffects6EffectEEEEE called 549294 returned 100% blocks executed 100%
   549294:  432:void CloneOp::getEffects(
        -:  433:    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
        -:  434:        &effects) {
   549294:  435:  effects.emplace_back(MemoryEffects::Read::get(), getInput(),
call    0 returned 100%
  1098588:  436:                       SideEffects::DefaultResource::get());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
  2197176:  437:  effects.emplace_back(MemoryEffects::Write::get(), getOutput(),
  1098588:  438:                       SideEffects::DefaultResource::get());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
  2197176:  439:  effects.emplace_back(MemoryEffects::Allocate::get(), getOutput(),
  1098588:  440:                       SideEffects::DefaultResource::get());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
   549294:  441:}
        -:  442:
function _ZN4mlir13bufferization7CloneOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 506629 returned 100% blocks executed 80%
   506629:  443:OpFoldResult CloneOp::fold(ArrayRef<Attribute> operands) {
  506629*:  444:  return succeeded(memref::foldMemRefCast(*this)) ? getResult() : Value();
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
        -:  445:}
        -:  446:
        -:  447:namespace {
        -:  448:
        -:  449:/// Merge the clone and its source (by converting the clone to a cast) when
        -:  450:/// possible.
        -:  451:struct SimplifyClones : public OpRewritePattern<CloneOp> {
        -:  452:  using OpRewritePattern<CloneOp>::OpRewritePattern;
        -:  453:
function _ZNK12_GLOBAL__N_114SimplifyClones15matchAndRewriteEN4mlir13bufferization7CloneOpERNS1_15PatternRewriterE called 5363 returned 100% blocks executed 86%
     5363:  454:  LogicalResult matchAndRewrite(CloneOp cloneOp,
        -:  455:                                PatternRewriter &rewriter) const override {
    10726:  456:    if (cloneOp.use_empty()) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####:  457:      rewriter.eraseOp(cloneOp);
call    0 never executed
    #####:  458:      return success();
        -:  459:    }
        -:  460:
     5363:  461:    Value source = cloneOp.getInput();
call    0 returned 100%
        -:  462:    // Aims to find the dealloc op for the canonical source
        -:  463:    // which otherwise could prevent removal of unnecessary allocs.
     5363:  464:    Value canonicalSource = source;
    5363*:  465:    while (auto iface = dyn_cast_or_null<ViewLikeOpInterface>(
    10723:  466:               canonicalSource.getDefiningOp()))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 1%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
    #####:  467:      canonicalSource = iface.getViewSource();
call    0 never executed
        -:  468:
     5363:  469:    llvm::Optional<Operation *> maybeCloneDeallocOp =
     5363:  470:        memref::findDealloc(cloneOp.getOutput());
call    0 returned 100%
call    1 returned 100%
        -:  471:    // Skip if either of them has > 1 deallocate operations.
     5363:  472:    if (!maybeCloneDeallocOp.has_value())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  473:      return failure();
     5363:  474:    llvm::Optional<Operation *> maybeSourceDeallocOp =
     5363:  475:        memref::findDealloc(canonicalSource);
call    0 returned 100%
     5363:  476:    if (!maybeSourceDeallocOp.has_value())
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  477:      return failure();
     5363:  478:    Operation *cloneDeallocOp = *maybeCloneDeallocOp;
branch  0 taken 1% (fallthrough)
branch  1 taken 99%
     5363:  479:    Operation *sourceDeallocOp = *maybeSourceDeallocOp;
        -:  480:
        -:  481:    // If both are deallocated in the same block, their in-block lifetimes
        -:  482:    // might not fully overlap, so we cannot decide which one to drop.
     5363:  483:    if (cloneDeallocOp && sourceDeallocOp &&
branch  0 taken 1% (fallthrough)
branch  1 taken 99%
branch  2 taken 73% (fallthrough)
branch  3 taken 27%
       73:  484:        cloneDeallocOp->getBlock() == sourceDeallocOp->getBlock())
branch  0 taken 73% (fallthrough)
branch  1 taken 27%
       53:  485:      return failure();
        -:  486:
     5310:  487:    Block *currentBlock = cloneOp->getBlock();
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
     5310:  488:    Operation *redundantDealloc = nullptr;
     5310:  489:    if (cloneDeallocOp && cloneDeallocOp->getBlock() == currentBlock) {
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
        -:  490:      redundantDealloc = cloneDeallocOp;
     5285:  491:    } else if (sourceDeallocOp && sourceDeallocOp->getBlock() == currentBlock) {
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:  492:      redundantDealloc = sourceDeallocOp;
        -:  493:    }
        -:  494:
       36:  495:    if (!redundantDealloc)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
     5274:  496:      return failure();
        -:  497:
        -:  498:    // Safety check that there are no other deallocations inbetween
        -:  499:    // cloneOp and redundantDealloc, as otherwise we might deallocate an alias
        -:  500:    // of source before the uses of the clone. With alias information, we could
        -:  501:    // restrict this to only fail of the dealloc's operand is an alias
        -:  502:    // of the source.
       36:  503:    for (Operation *pos = cloneOp->getNextNode(); pos != redundantDealloc;
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
       60:  504:         pos = pos->getNextNode()) {
branch  0 taken 31% (fallthrough)
branch  1 taken 69%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
       15:  505:      auto effectInterface = dyn_cast<MemoryEffectOpInterface>(pos);
call    0 returned 100%
       15:  506:      if (!effectInterface)
branch  0 taken 7% (fallthrough)
branch  1 taken 93%
        1:  507:        continue;
       14:  508:      if (effectInterface.hasEffect<MemoryEffects::Free>())
call    0 returned 100%
branch  1 taken 21% (fallthrough)
branch  2 taken 79%
        3:  509:        return failure();
        -:  510:    }
        -:  511:
       66:  512:    rewriter.replaceOpWithNewOp<memref::CastOp>(cloneOp, cloneOp.getType(),
       33:  513:                                                source);
call    0 returned 100%
       33:  514:    rewriter.eraseOp(redundantDealloc);
call    0 returned 100%
       33:  515:    return success();
        -:  516:  }
        -:  517:};
        -:  518:
        -:  519:} // namespace
        -:  520:
function _ZN4mlir13bufferization7CloneOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235:  521:void CloneOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  522:                                          MLIRContext *context) {
     1235:  523:  results.add<SimplifyClones>(context);
call    0 returned 100%
     1235:  524:}
        -:  525:
        -:  526://===----------------------------------------------------------------------===//
        -:  527:// DeallocTensorOp
        -:  528://===----------------------------------------------------------------------===//
        -:  529:
function _ZN4mlir13bufferization15DeallocTensorOp9bufferizeERNS_12RewriterBaseERKNS0_20BufferizationOptionsE called 28671 returned 100% blocks executed 80%
    28671:  530:LogicalResult DeallocTensorOp::bufferize(RewriterBase &rewriter,
        -:  531:                                         const BufferizationOptions &options) {
    28671:  532:  FailureOr<Value> buffer = getBuffer(rewriter, getTensor(), options);
call    0 returned 100%
call    1 returned 100%
    28671:  533:  if (failed(buffer))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  534:    return failure();
    28671:  535:  if (failed(options.createDealloc(rewriter, getLoc(), *buffer)))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  536:    return failure();
    28671:  537:  rewriter.eraseOp(getOperation());
call    0 returned 100%
    28671:  538:  return success();
        -:  539:}
        -:  540:
        -:  541://===----------------------------------------------------------------------===//
        -:  542:// ToTensorOp
        -:  543://===----------------------------------------------------------------------===//
        -:  544:
function _ZN4mlir13bufferization10ToTensorOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 202723 returned 100% blocks executed 100%
   202723:  545:OpFoldResult ToTensorOp::fold(ArrayRef<Attribute>) {
   202723:  546:  if (auto toMemref = getMemref().getDefiningOp<ToMemrefOp>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 3% (fallthrough)
branch  3 taken 97%
        -:  547:    // Approximate alias analysis by conservatively folding only when no there
        -:  548:    // is no interleaved operation.
     8244:  549:    if (toMemref->getBlock() == this->getOperation()->getBlock() &&
branch  0 taken 22% (fallthrough)
branch  1 taken 78%
branch  2 taken 6% (fallthrough)
branch  3 taken 94%
     2932:  550:        toMemref->getNextNode() == this->getOperation())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 6% (fallthrough)
branch  3 taken 94%
       94:  551:      return toMemref.getTensor();
call    0 returned 100%
call    1 returned 100%
   202629:  552:  return {};
        -:  553:}
        -:  554:
        -:  555:namespace {
        -:  556:struct DimOfToTensorFolder : public OpRewritePattern<tensor::DimOp> {
        -:  557:  using OpRewritePattern<tensor::DimOp>::OpRewritePattern;
        -:  558:
function _ZNK12_GLOBAL__N_119DimOfToTensorFolder15matchAndRewriteEN4mlir6tensor5DimOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  559:  LogicalResult matchAndRewrite(tensor::DimOp dimOp,
        -:  560:                                PatternRewriter &rewriter) const override {
    #####:  561:    auto memrefToTensorOp = dimOp.getSource().getDefiningOp<ToTensorOp>();
call    0 never executed
call    1 never executed
    #####:  562:    if (!memrefToTensorOp)
branch  0 never executed
branch  1 never executed
    #####:  563:      return failure();
        -:  564:
    #####:  565:    rewriter.replaceOpWithNewOp<memref::DimOp>(
    #####:  566:        dimOp, memrefToTensorOp.getMemref(), dimOp.getIndex());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  567:    return success();
        -:  568:  }
        -:  569:};
        -:  570:} // namespace
        -:  571:
function _ZN4mlir13bufferization10ToTensorOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235:  572:void ToTensorOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  573:                                             MLIRContext *context) {
     1235:  574:  results.add<DimOfToTensorFolder>(context);
call    0 returned 100%
     1235:  575:}
        -:  576:
        -:  577://===----------------------------------------------------------------------===//
        -:  578:// ToMemrefOp
        -:  579://===----------------------------------------------------------------------===//
        -:  580:
function _ZN4mlir13bufferization10ToMemrefOp4foldEN4llvm8ArrayRefINS_9AttributeEEE called 227570 returned 100% blocks executed 100%
   227570:  581:OpFoldResult ToMemrefOp::fold(ArrayRef<Attribute>) {
   227570:  582:  if (auto memrefToTensor = getTensor().getDefiningOp<ToTensorOp>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 1% (fallthrough)
branch  3 taken 99%
     1645:  583:    if (memrefToTensor.getMemref().getType() == getType())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
     1645:  584:      return memrefToTensor.getMemref();
call    0 returned 100%
call    1 returned 100%
   225925:  585:  return {};
        -:  586:}
        -:  587:
        -:  588:namespace {
        -:  589:
        -:  590:/// Replace tensor.cast + to_memref by to_memref + memref.cast.
        -:  591:struct ToMemrefOfCast : public OpRewritePattern<ToMemrefOp> {
        -:  592:  using OpRewritePattern<ToMemrefOp>::OpRewritePattern;
        -:  593:
function _ZNK12_GLOBAL__N_114ToMemrefOfCast15matchAndRewriteEN4mlir13bufferization10ToMemrefOpERNS1_15PatternRewriterE called 3043 returned 100% blocks executed 94%
     3043:  594:  LogicalResult matchAndRewrite(ToMemrefOp toMemref,
        -:  595:                                PatternRewriter &rewriter) const final {
     3043:  596:    auto tensorCastOperand =
call    0 returned 100%
     3043:  597:        toMemref.getOperand().getDefiningOp<tensor::CastOp>();
call    0 returned 100%
     3043:  598:    if (!tensorCastOperand)
branch  0 taken 100% (fallthrough)
branch  1 taken 1%
     3036:  599:      return failure();
        7:  600:    auto srcTensorType =
call    0 returned 100%
        7:  601:        tensorCastOperand.getOperand().getType().dyn_cast<RankedTensorType>();
call    0 returned 100%
        7:  602:    if (!srcTensorType)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  603:      return failure();
        7:  604:    auto memrefType = MemRefType::get(srcTensorType.getShape(),
        7:  605:                                      srcTensorType.getElementType());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
call    3 returned 100%
       14:  606:    Value memref = rewriter.create<ToMemrefOp>(toMemref.getLoc(), memrefType,
        7:  607:                                               tensorCastOperand.getOperand());
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
       14:  608:    rewriter.replaceOpWithNewOp<memref::CastOp>(toMemref, toMemref.getType(),
        7:  609:                                                memref);
call    0 returned 100%
        7:  610:    return success();
        -:  611:  }
        -:  612:};
        -:  613:
        -:  614:/// Canonicalize bufferization.to_tensor + bufferization.to_memref. Insert a
        -:  615:/// cast if necessary.
        -:  616:struct ToMemrefToTensorFolding : public OpRewritePattern<ToMemrefOp> {
        -:  617:  using OpRewritePattern<ToMemrefOp>::OpRewritePattern;
        -:  618:
function _ZNK12_GLOBAL__N_123ToMemrefToTensorFolding15matchAndRewriteEN4mlir13bufferization10ToMemrefOpERNS1_15PatternRewriterE called 3036 returned 100% blocks executed 100%
     3036:  619:  LogicalResult matchAndRewrite(ToMemrefOp toMemref,
        -:  620:                                PatternRewriter &rewriter) const final {
     3036:  621:    return foldToMemrefToTensorPair(rewriter, toMemref);
call    0 returned 100%
        -:  622:  }
        -:  623:};
        -:  624:
        -:  625:/// Fold a load on a to_memref operation into an tensor.extract on the
        -:  626:/// corresponding tensor.
        -:  627:struct LoadOfToMemref : public OpRewritePattern<memref::LoadOp> {
        -:  628:  using OpRewritePattern<memref::LoadOp>::OpRewritePattern;
        -:  629:
function _ZNK12_GLOBAL__N_114LoadOfToMemref15matchAndRewriteEN4mlir6memref6LoadOpERNS1_15PatternRewriterE called 8 returned 100% blocks executed 100%
        8:  630:  LogicalResult matchAndRewrite(memref::LoadOp load,
        -:  631:                                PatternRewriter &rewriter) const override {
        8:  632:    auto toMemref = load.getMemref().getDefiningOp<ToMemrefOp>();
call    0 returned 100%
call    1 returned 100%
        8:  633:    if (!toMemref)
branch  0 taken 50% (fallthrough)
branch  1 taken 50%
        4:  634:      return failure();
        -:  635:
        4:  636:    rewriter.replaceOpWithNewOp<tensor::ExtractOp>(load, toMemref.getTensor(),
call    0 returned 100%
        8:  637:                                                   load.getIndices());
call    0 returned 100%
call    1 returned 100%
        4:  638:    return success();
        -:  639:  }
        -:  640:};
        -:  641:
        -:  642:/// Fold dim of a to_memref into the dim of the tensor.
        -:  643:struct DimOfCastOp : public OpRewritePattern<memref::DimOp> {
        -:  644:  using OpRewritePattern<memref::DimOp>::OpRewritePattern;
        -:  645:
function _ZNK12_GLOBAL__N_111DimOfCastOp15matchAndRewriteEN4mlir6memref5DimOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  646:  LogicalResult matchAndRewrite(memref::DimOp dimOp,
        -:  647:                                PatternRewriter &rewriter) const override {
    #####:  648:    auto castOp = dimOp.getSource().getDefiningOp<ToMemrefOp>();
call    0 never executed
call    1 never executed
    #####:  649:    if (!castOp)
branch  0 never executed
branch  1 never executed
    #####:  650:      return failure();
    #####:  651:    Value newSource = castOp.getOperand();
call    0 never executed
    #####:  652:    rewriter.replaceOpWithNewOp<tensor::DimOp>(dimOp, newSource,
    #####:  653:                                               dimOp.getIndex());
call    0 never executed
call    1 never executed
    #####:  654:    return success();
        -:  655:  }
        -:  656:};
        -:  657:
        -:  658:} // namespace
        -:  659:
function _ZN4mlir13bufferization10ToMemrefOp27getCanonicalizationPatternsERNS_17RewritePatternSetEPNS_11MLIRContextE called 1235 returned 100% blocks executed 100%
     1235:  660:void ToMemrefOp::getCanonicalizationPatterns(RewritePatternSet &results,
        -:  661:                                             MLIRContext *context) {
     1235:  662:  results.add<DimOfCastOp, LoadOfToMemref, ToMemrefOfCast,
     1235:  663:              ToMemrefToTensorFolding>(context);
call    0 returned 100%
     1235:  664:}
        -:  665:
function _ZN4mlir13bufferization10ToMemrefOp9bufferizeERNS_12RewriterBaseERKNS0_20BufferizationOptionsE called 19312 returned 100% blocks executed 100%
    19312:  666:LogicalResult ToMemrefOp::bufferize(RewriterBase &rewriter,
        -:  667:                                    const BufferizationOptions &options) {
        -:  668:  // Fold to_memref(to_tensor(x)) to x. Insert a cast if necessary.
    19312:  669:  (void)foldToMemrefToTensorPair(rewriter, *this);
call    0 returned 100%
        -:  670:  // Note: The return value of `bufferize` indicates whether there was an error
        -:  671:  // or not. (And not whether the pattern matched or not.)
    19312:  672:  return success();
        -:  673:}
        -:  674:
function _ZN4mlir13bufferization7CloneOp12buildDeallocERNS_9OpBuilderENS_5ValueE called 9539 returned 100% blocks executed 100%
     9539:  675:Optional<Operation *> CloneOp::buildDealloc(OpBuilder &builder, Value alloc) {
     9539:  676:  return builder.create<memref::DeallocOp>(alloc.getLoc(), alloc)
call    0 returned 100%
call    1 returned 100%
     9539:  677:      .getOperation();
        -:  678:}
        -:  679:
function _ZN4mlir13bufferization7CloneOp10buildCloneERNS_9OpBuilderENS_5ValueE called 8 returned 100% blocks executed 100%
        8:  680:Optional<Value> CloneOp::buildClone(OpBuilder &builder, Value alloc) {
        8:  681:  return builder.create<CloneOp>(alloc.getLoc(), alloc).getResult();
call    0 returned 100%
call    1 returned 100%
        -:  682:}
        -:  683:
        -:  684://===----------------------------------------------------------------------===//
        -:  685:// TableGen'd op method definitions
        -:  686://===----------------------------------------------------------------------===//
        -:  687:
        -:  688:#define GET_OP_CLASSES
        -:  689:#include "mlir/Dialect/Bufferization/IR/BufferizationOps.cpp.inc"
