        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Linalg/Transforms/ElementwiseOpFusion.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Linalg/Transforms/CMakeFiles/obj.MLIRLinalgTransforms.dir/ElementwiseOpFusion.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Linalg/Transforms/CMakeFiles/obj.MLIRLinalgTransforms.dir/ElementwiseOpFusion.cpp.gcda
        -:    0:Runs:116159
        -:    1://===- ElementwiseOpFusion.cpp - Implementation of linalg Fusion ---------===///
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements the linalg dialect Fusion on tensors operations pass.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#include "mlir/Dialect/Linalg/Passes.h"
        -:   14:
        -:   15:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   16:#include "mlir/Dialect/Arith/Utils/Utils.h"
        -:   17:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   18:#include "mlir/Dialect/Linalg/Transforms/Transforms.h"
        -:   19:#include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
        -:   20:#include "mlir/IR/AffineExpr.h"
        -:   21:#include "mlir/IR/AffineMap.h"
        -:   22:#include "mlir/IR/Matchers.h"
        -:   23:#include "mlir/IR/PatternMatch.h"
        -:   24:#include "mlir/Support/LLVM.h"
        -:   25:#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
        -:   26:#include <utility>
        -:   27:
        -:   28:namespace mlir {
        -:   29:#define GEN_PASS_DEF_LINALGFOLDUNITEXTENTDIMS
        -:   30:#define GEN_PASS_DEF_LINALGELEMENTWISEOPFUSION
        -:   31:#include "mlir/Dialect/Linalg/Passes.h.inc"
        -:   32:} // namespace mlir
        -:   33:
        -:   34:using namespace mlir;
        -:   35:using namespace mlir::linalg;
        -:   36:
        -:   37://===---------------------------------------------------------------------===//
        -:   38:// Methods and patterns that fuse elementwise `linalg.generic` operations.
        -:   39://===---------------------------------------------------------------------===//
        -:   40:
        -:   41:/// Append to `fusedOpIndexingMapAttrs` the indexing maps for the operands of
        -:   42:/// the `producer` to use in the fused operation given the indexing map of the
        -:   43:/// result of the producer in the consumer.
function _ZL54getIndexingMapOfProducerOperandsInCoordinatesOfFusedOpPN4mlir9OpOperandENS_9AffineMapES2_ called 0 returned 0% blocks executed 0%
    #####:   44:static AffineMap getIndexingMapOfProducerOperandsInCoordinatesOfFusedOp(
        -:   45:    OpOperand *producerOpOperand, AffineMap producerResultIndexMap,
        -:   46:    AffineMap fusedConsumerArgIndexMap) {
        -:   47:  // The indexing map in the consumer op (fusedConsumerArgIndexMap) is a map
        -:   48:  // from consumer loop -> consumer arg tensor index/producer result tensor
        -:   49:  // index. The fused loop is same as the consumer loop. For each producer arg
        -:   50:  // the indexing map to be computed is a map from consumer loop -> producer
        -:   51:  // arg tensor index.
        -:   52:  // producerResultIndexMap is a map from producer loop -> tensor index.
        -:   53:  // Compute the inverse to get map from tensor index -> producer loop.
        -:   54:  // The inverse is a map from producer result tensor index -> producer loop.
    #####:   55:  AffineMap invProducerResultIndexMap =
    #####:   56:      inversePermutation(producerResultIndexMap);
call    0 never executed
    #####:   57:  assert(invProducerResultIndexMap &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:   58:         "expected producer result indexing map to be invertible");
        -:   59:
    #####:   60:  LinalgOp producer = cast<LinalgOp>(producerOpOperand->getOwner());
call    0 never executed
        -:   61:  // argMap is a map from producer loop -> producer arg tensor index.
    #####:   62:  AffineMap argMap = producer.getMatchingIndexingMap(producerOpOperand);
call    0 never executed
        -:   63:
        -:   64:  // Compose argMap with invProducerResultIndexMap to get a map from
        -:   65:  // producer result tensor index -> producer arg tensor index.
    #####:   66:  AffineMap t1 = argMap.compose(invProducerResultIndexMap);
call    0 never executed
        -:   67:
        -:   68:  // Compose t1 with fusedConsumerArgIndexMap gives an indexing map from
        -:   69:  // consumer loop/ fused loop -> producer arg tensor index.
    #####:   70:  return t1.compose(fusedConsumerArgIndexMap);
call    0 never executed
        -:   71:}
        -:   72:
        -:   73:/// Conditions for elementwise fusion of generic operations.
function _ZN4mlir6linalg24areElementwiseOpsFusableEPNS_9OpOperandE called 0 returned 0% blocks executed 0%
    #####:   74:bool mlir::linalg::areElementwiseOpsFusable(OpOperand *fusedOperand) {
    #####:   75:  auto producer = fusedOperand->get().getDefiningOp<GenericOp>();
call    0 never executed
    #####:   76:  auto consumer = dyn_cast<GenericOp>(fusedOperand->getOwner());
call    0 never executed
        -:   77:
        -:   78:  // Check producer and consumer are generic ops.
    #####:   79:  if (!producer || !consumer)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:   80:    return false;
        -:   81:
        -:   82:  // Producer and consumer must have tensor semantics.
    #####:   83:  if (!producer.hasTensorSemantics() || !consumer.hasTensorSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:   84:    return false;
        -:   85:
        -:   86:  // Verify that
        -:   87:  // - the producer has all "parallel" iterator type.
    #####:   88:  if (producer.getNumParallelLoops() != producer.getNumLoops())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:   89:    return false;
        -:   90:
        -:   91:  // Only allow fusing the producer of an input operand for now.
        -:   92:  // TODO: allow fusing the producer of an output operand.
    #####:   93:  if (!consumer.isDpsInput(fusedOperand))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:   94:    return false;
        -:   95:
        -:   96:  // Get the consumer index map. The number of results of the consumer index
        -:   97:  // map must match the number of loops of the producer.
    #####:   98:  AffineMap consumerIndexMap = consumer.getMatchingIndexingMap(fusedOperand);
call    0 never executed
    #####:   99:  if (consumerIndexMap.getNumResults() != producer.getNumLoops())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  100:    return false;
        -:  101:
        -:  102:  // Finally the index_map for the result must be invertible. For now just
        -:  103:  // verify it is a permutation.
    #####:  104:  AffineMap producerResultIndexMap =
    #####:  105:      producer.getMatchingIndexingMap(producer.getDpsInitOperand(0));
call    0 never executed
call    1 never executed
    #####:  106:  if (!producerResultIndexMap.isPermutation())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  107:    return false;
        -:  108:
        -:  109:  // Ensure that the fusion does not remove size information required to
        -:  110:  // get the loop bounds. For non-reduction generics, this is trivially the
        -:  111:  // case due to the output operand. For reductions, we need to check that after
        -:  112:  // the fusion, each loop dimension has at least one input that defines it.
    #####:  113:  if ((consumer.getNumReductionLoops())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  114:    BitVector coveredDims(consumer.getNumLoops(), false);
call    0 never executed
call    1 never executed
        -:  115:
function _ZZN4mlir6linalg24areElementwiseOpsFusableEPNS_9OpOperandEENKUlNS_9AffineMapEE_clES3_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  116:    auto addToCoveredDims = [&](AffineMap map) {
    #####:  117:      for (auto result : map.getResults())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  118:        if (auto dimExpr = result.dyn_cast<AffineDimExpr>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  119:          coveredDims[dimExpr.getPosition()] = true;
call    0 never executed
call    1 never executed
    #####:  120:    };
        -:  121:
    #####:  122:    for (auto pair :
    #####:  123:         llvm::zip(consumer->getOperands(), consumer.getIndexingMapsArray())) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:  124:      Value operand = std::get<0>(pair);
branch  0 never executed
branch  1 never executed
    #####:  125:      if (operand == fusedOperand->get())
branch  0 never executed
branch  1 never executed
    #####:  126:        continue;
    #####:  127:      AffineMap operandMap = std::get<1>(pair);
call    0 never executed
    #####:  128:      addToCoveredDims(operandMap);
call    0 never executed
        -:  129:    }
        -:  130:
    #####:  131:    for (OpOperand *operand : producer.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  132:      AffineMap newIndexingMap =
        -:  133:          getIndexingMapOfProducerOperandsInCoordinatesOfFusedOp(
    #####:  134:              operand, producerResultIndexMap, consumerIndexMap);
call    0 never executed
    #####:  135:      addToCoveredDims(newIndexingMap);
call    0 never executed
        -:  136:    }
    #####:  137:    if (!coveredDims.all())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  138:      return false;
branch  0 never executed
branch  1 never executed
        -:  139:  }
        -:  140:
        -:  141:  return true;
        -:  142:}
        -:  143:
        -:  144:/// Generate the region of the fused tensor operation. The region of the fused
        -:  145:/// op must be empty.
        -:  146:static void
        -:  147:generateFusedElementwiseOpRegion(RewriterBase &rewriter, GenericOp fusedOp,
        -:  148:                                 AffineMap consumerToProducerLoopsMap,
        -:  149:                                 OpOperand *fusedOperand, unsigned nloops) {
        -:  150:  auto producer = cast<GenericOp>(fusedOperand->get().getDefiningOp());
        -:  151:  auto consumer = cast<GenericOp>(fusedOperand->getOwner());
        -:  152:  // Build the region of the fused op.
        -:  153:  Block &producerBlock = producer->getRegion(0).front();
        -:  154:  Block &consumerBlock = consumer->getRegion(0).front();
        -:  155:  Block *fusedBlock = new Block();
        -:  156:  fusedOp.getRegion().push_back(fusedBlock);
        -:  157:  BlockAndValueMapping mapper;
        -:  158:  OpBuilder::InsertionGuard guard(rewriter);
        -:  159:  rewriter.setInsertionPointToStart(fusedBlock);
        -:  160:
        -:  161:  // 2. Add an index operation for every fused loop dimension and use the
        -:  162:  // `consumerToProducerLoopsMap` to map the producer indices.
        -:  163:  if (producer.hasIndexSemantics()) {
        -:  164:    // Add an index operation for every fused loop dimension.
        -:  165:    unsigned numFusedOpLoops =
        -:  166:        std::max(producer.getNumLoops(), consumer.getNumLoops());
        -:  167:    SmallVector<Value> fusedIndices;
        -:  168:    fusedIndices.reserve(numFusedOpLoops);
        -:  169:    llvm::transform(llvm::seq<uint64_t>(0, numFusedOpLoops),
    #####:  170:                    std::back_inserter(fusedIndices), [&](uint64_t dim) {
    #####:  171:                      return rewriter.create<IndexOp>(producer.getLoc(), dim);
call    0 never executed
        -:  172:                    });
        -:  173:    for (IndexOp indexOp :
        -:  174:         llvm::make_early_inc_range(producerBlock.getOps<IndexOp>())) {
        -:  175:      Value newIndex = rewriter.create<mlir::AffineApplyOp>(
        -:  176:          producer.getLoc(),
        -:  177:          consumerToProducerLoopsMap.getSubMap(indexOp.getDim()), fusedIndices);
        -:  178:      mapper.map(indexOp.getResult(), newIndex);
        -:  179:    }
        -:  180:  }
        -:  181:  // TODO: allow fusing the producer of an output operand.
        -:  182:  assert(consumer.isDpsInput(fusedOperand) &&
        -:  183:         "expected producer of input operand");
        -:  184:  // 3. Consumer input operands up to consumerIdx (exclusive).
        -:  185:  for (BlockArgument bbArg : consumerBlock.getArguments().take_front(
        -:  186:           fusedOperand->getOperandNumber())) // input assumption.
        -:  187:    mapper.map(bbArg, fusedBlock->addArgument(bbArg.getType(), bbArg.getLoc()));
        -:  188:
        -:  189:  // Replacing consumerIdx requires getting the cloned, yielded, value from
        -:  190:  // the (cloned) producer block. This happens in step 9.
        -:  191:
        -:  192:  // 4. Splice in producer's input operands.
        -:  193:  for (BlockArgument bbArg :
        -:  194:       producerBlock.getArguments().take_front(producer.getNumDpsInputs()))
        -:  195:    mapper.map(bbArg, fusedBlock->addArgument(bbArg.getType(), bbArg.getLoc()));
        -:  196:
        -:  197:  // 5. Remaining consumer's input operands (drop past index `consumerIdx`).
        -:  198:  for (BlockArgument bbArg :
        -:  199:       consumerBlock.getArguments()
        -:  200:           .take_front(consumer.getNumDpsInputs())
        -:  201:           .drop_front(fusedOperand->getOperandNumber() + 1))
        -:  202:    mapper.map(bbArg, fusedBlock->addArgument(bbArg.getType(), bbArg.getLoc()));
        -:  203:
        -:  204:  // 6. All of the producer's output operands
        -:  205:  for (BlockArgument bbArg :
        -:  206:       producerBlock.getArguments().take_back(producer.getNumDpsInits()))
        -:  207:    mapper.map(bbArg, fusedBlock->addArgument(bbArg.getType(), bbArg.getLoc()));
        -:  208:
        -:  209:  // 7. All of consumer's output operands.
        -:  210:  for (BlockArgument bbArg :
        -:  211:       consumerBlock.getArguments().take_back(consumer.getNumDpsInits()))
        -:  212:    mapper.map(bbArg, fusedBlock->addArgument(bbArg.getType(), bbArg.getLoc()));
        -:  213:
        -:  214:  // 8. Clone all producer operations except for the yield and index operations
        -:  215:  // to the fused operation.
        -:  216:  for (auto &op : producerBlock.without_terminator()) {
        -:  217:    if (!isa<IndexOp>(op))
        -:  218:      rewriter.clone(op, mapper);
        -:  219:  }
        -:  220:  // 9. Now we can map the consumerBlock's `consumerIdx` block argument. Just
        -:  221:  // forward the yield operand.
        -:  222:  auto producerYieldOp = cast<linalg::YieldOp>(producerBlock.getTerminator());
        -:  223:  unsigned producerResultNumber =
        -:  224:      fusedOperand->get().cast<OpResult>().getResultNumber();
        -:  225:  Value replacement =
        -:  226:      mapper.lookupOrDefault(producerYieldOp.getOperand(producerResultNumber));
        -:  227:
        -:  228:  // Sanity checks, if replacement is not already in the mapper then it must be
        -:  229:  // produced outside.
        -:  230:  if (replacement == producerYieldOp.getOperand(producerResultNumber)) {
        -:  231:    if (auto bb = replacement.dyn_cast<BlockArgument>())
        -:  232:      assert(bb.getOwner() != &producerBlock &&
        -:  233:             "yielded block argument must have been mapped");
        -:  234:    else
        -:  235:      assert(!producer->isAncestor(replacement.getDefiningOp()) &&
        -:  236:             "yielded value must have been mapped");
        -:  237:  }
        -:  238:  mapper.map(consumerBlock.getArgument(fusedOperand->getOperandNumber()),
        -:  239:             replacement);
        -:  240:  // 10. Clone operations from the consumer to the fused op.
        -:  241:  for (auto &op : consumerBlock.without_terminator())
        -:  242:    rewriter.clone(op, mapper);
        -:  243:
        -:  244:  // 11. Include the final yield (which is the remapped values for all the
        -:  245:  // yield)
        -:  246:  auto consumerYieldOp = cast<linalg::YieldOp>(consumerBlock.getTerminator());
        -:  247:  SmallVector<Value> fusedYieldValues;
        -:  248:  fusedYieldValues.reserve(producerYieldOp.getNumOperands() +
        -:  249:                           consumerYieldOp.getNumOperands());
        -:  250:  for (auto producerYieldVal : producerYieldOp.getOperands())
        -:  251:    fusedYieldValues.push_back(mapper.lookupOrDefault(producerYieldVal));
        -:  252:  for (auto consumerYieldVal : consumerYieldOp.getOperands())
        -:  253:    fusedYieldValues.push_back(mapper.lookupOrDefault(consumerYieldVal));
        -:  254:  rewriter.create<YieldOp>(fusedOp.getLoc(), fusedYieldValues);
        -:  255:
        -:  256:  // Sanity checks.
        -:  257:  assert(fusedBlock->getNumArguments() == fusedOp.getNumOperands() &&
        -:  258:         "Ill-formed GenericOp region");
        -:  259:}
        -:  260:
        -:  261:FailureOr<Operation *>
function _ZN4mlir6linalg18fuseElementwiseOpsERNS_12RewriterBaseEPNS_9OpOperandE called 0 returned 0% blocks executed 0%
    #####:  262:mlir::linalg::fuseElementwiseOps(RewriterBase &rewriter,
        -:  263:                                 OpOperand *fusedOperand) {
    #####:  264:  assert(areElementwiseOpsFusable(fusedOperand) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  265:         "expected elementwise operation pre-conditions to pass");
    #####:  266:  auto producerResult = fusedOperand->get().cast<OpResult>();
call    0 never executed
    #####:  267:  auto producer = cast<GenericOp>(producerResult.getOwner());
call    0 never executed
call    1 never executed
    #####:  268:  auto consumer = cast<GenericOp>(fusedOperand->getOwner());
call    0 never executed
        -:  269:  // TODO: allow fusing the producer of an output operand.
    #####:  270:  assert(consumer.isDpsInput(fusedOperand) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  271:         "expected producer of input operand");
        -:  272:
        -:  273:  // Compute the fused operands list and indexing maps.
    #####:  274:  SmallVector<Value> fusedInputOperands, fusedOutputOperands;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  275:  SmallVector<Type> fusedResultTypes;
branch  0 never executed
branch  1 never executed
    #####:  276:  SmallVector<AffineMap> fusedIndexMaps;
branch  0 never executed
branch  1 never executed
    #####:  277:  fusedInputOperands.reserve(producer.getNumDpsInputs() +
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  278:                             consumer.getNumDpsInputs());
call    0 never executed
    #####:  279:  fusedOutputOperands.reserve(producer.getNumDpsInits() +
call    0 never executed
call    1 never executed
    #####:  280:                              consumer.getNumDpsInits());
branch  0 never executed
branch  1 never executed
    #####:  281:  fusedResultTypes.reserve(producer.getNumDpsInits() +
call    0 never executed
call    1 never executed
    #####:  282:                           consumer.getNumDpsInits());
branch  0 never executed
branch  1 never executed
    #####:  283:  fusedIndexMaps.reserve(producer->getNumOperands() +
branch  0 never executed
branch  1 never executed
    #####:  284:                         consumer->getNumOperands());
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  285:  // In the following, numbering matches that of `generateFusedTensorOpRegion`.
        -:  286:  // 3. Consumer input operands/maps up to consumerIdx (exclusive).
    #####:  287:  auto consumerInputs = consumer.getDpsInputOperands();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  288:  auto *it = llvm::find_if(consumerInputs, [&](OpOperand *operand) {
branch  0 never executed
branch  1 never executed
        -:  289:    return operand == fusedOperand;
        -:  290:  });
    #####:  291:  assert(it != consumerInputs.end() && "expected to find the consumer operand");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  292:  for (OpOperand *opOperand : llvm::make_range(consumerInputs.begin(), it)) {
branch  0 never executed
branch  1 never executed
    #####:  293:    fusedInputOperands.push_back(opOperand->get());
call    0 never executed
    #####:  294:    fusedIndexMaps.push_back(consumer.getMatchingIndexingMap(opOperand));
call    0 never executed
call    1 never executed
        -:  295:  }
        -:  296:  // 4. Splice in producer's input operands/maps.
    #####:  297:  AffineMap producerResultIndexMap =
    #####:  298:      producer.getIndexingMapMatchingResult(producerResult);
call    0 never executed
    #####:  299:  for (OpOperand *opOperand : producer.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  300:    fusedInputOperands.push_back(opOperand->get());
call    0 never executed
        -:  301:    // Compute indexing maps for the producer args in the fused operation.
    #####:  302:    AffineMap map = getIndexingMapOfProducerOperandsInCoordinatesOfFusedOp(
        -:  303:        opOperand, producerResultIndexMap,
    #####:  304:        consumer.getMatchingIndexingMap(fusedOperand));
call    0 never executed
call    1 never executed
    #####:  305:    fusedIndexMaps.push_back(map);
call    0 never executed
        -:  306:  }
        -:  307:  // 5. Remaining consumer's input operands/maps (drop past index
        -:  308:  // `consumerIdx`).
    #####:  309:  for (OpOperand *opOperand :
    #####:  310:       llvm::make_range(std::next(it), consumerInputs.end())) {
branch  0 never executed
branch  1 never executed
    #####:  311:    fusedInputOperands.push_back(opOperand->get());
call    0 never executed
    #####:  312:    fusedIndexMaps.push_back(consumer.getMatchingIndexingMap(opOperand));
call    0 never executed
call    1 never executed
        -:  313:  }
        -:  314:
        -:  315:  // 6. Collect all of the producer outputs.
    #####:  316:  for (OpOperand *opOperand : producer.getDpsInitOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  317:    fusedOutputOperands.push_back(opOperand->get());
call    0 never executed
    #####:  318:    AffineMap map = getIndexingMapOfProducerOperandsInCoordinatesOfFusedOp(
        -:  319:        opOperand, producerResultIndexMap,
    #####:  320:        consumer.getMatchingIndexingMap(fusedOperand));
call    0 never executed
call    1 never executed
    #####:  321:    fusedIndexMaps.push_back(map);
call    0 never executed
    #####:  322:    fusedResultTypes.push_back(opOperand->get().getType());
call    0 never executed
        -:  323:  }
        -:  324:
        -:  325:  // 7. All of consumer's output operands (skip operands: added by the builder).
    #####:  326:  for (OpOperand *opOperand : consumer.getDpsInitOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  327:    fusedOutputOperands.push_back(opOperand->get());
call    0 never executed
    #####:  328:    fusedIndexMaps.push_back(consumer.getMatchingIndexingMap(opOperand));
call    0 never executed
call    1 never executed
    #####:  329:    fusedResultTypes.push_back(opOperand->get().getType());
call    0 never executed
        -:  330:  }
        -:  331:
        -:  332:  // Generate the fused op.
    #####:  333:  auto fusedOp = rewriter.create<GenericOp>(
        -:  334:      consumer.getLoc(), fusedResultTypes, fusedInputOperands,
    #####:  335:      fusedOutputOperands, rewriter.getAffineMapArrayAttr(fusedIndexMaps),
call    0 never executed
    #####:  336:      consumer.getIteratorTypes(),
    #####:  337:      /*doc=*/nullptr,
    #####:  338:      /*library_call=*/nullptr);
call    0 never executed
call    1 never executed
    #####:  339:  if (!fusedOp.getShapesToLoopsMap()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  340:    // Fused op has invalid indexing maps. Typically this means something is off
        -:  341:    // in the input, but going ahead here would result in verification errors.
        -:  342:    // So cleanup and abort.
    #####:  343:    rewriter.eraseOp(fusedOp);
call    0 never executed
    #####:  344:    return rewriter.notifyMatchFailure(
    #####:  345:        fusedOp, "fused op failed loop bound computation check");
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  346:  }
        -:  347:
        -:  348:  // Construct an AffineMap from consumer loops to producer loops.
        -:  349:  // consumer loop -> tensor index
    #####:  350:  AffineMap consumerResultIndexMap =
    #####:  351:      consumer.getMatchingIndexingMap(fusedOperand);
call    0 never executed
        -:  352:  // tensor index -> producer loop
    #####:  353:  AffineMap invProducerResultIndexMap =
    #####:  354:      inversePermutation(producerResultIndexMap);
call    0 never executed
    #####:  355:  assert(invProducerResultIndexMap &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  356:         "expected producer result indexig map to be invertible");
        -:  357:  // consumer loop -> producer loop
    #####:  358:  AffineMap consumerToProducerLoopsMap =
    #####:  359:      invProducerResultIndexMap.compose(consumerResultIndexMap);
call    0 never executed
        -:  360:
    #####:  361:  generateFusedElementwiseOpRegion(rewriter, fusedOp,
call    0 never executed
call    1 never executed
        -:  362:                                   consumerToProducerLoopsMap, fusedOperand,
        -:  363:                                   consumer.getNumLoops());
    #####:  364:  return fusedOp.getOperation();
        -:  365:}
        -:  366:
        -:  367:namespace {
        -:  368:/// Patterns to fuse a generic op, with the producer of its operands.
        -:  369:class FuseElementwiseOps : public OpRewritePattern<GenericOp> {
        -:  370:public:
function _ZN12_GLOBAL__N_118FuseElementwiseOpsC2EPN4mlir11MLIRContextESt8functionIFbPNS1_9OpOperandEEENS1_14PatternBenefitE called 412 returned 100% blocks executed 100%
      412:  371:  FuseElementwiseOps(MLIRContext *context, ControlFusionFn fun,
        -:  372:                     PatternBenefit benefit = 1)
      412:  373:      : OpRewritePattern<GenericOp>(context, benefit),
      412:  374:        controlFn(std::move(fun)) {}
call    0 returned 100%
        -:  375:
function _ZNK12_GLOBAL__N_118FuseElementwiseOps15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  376:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -:  377:                                PatternRewriter &rewriter) const override {
        -:  378:    // Find the first operand that is defined by another generic op on tensors.
    #####:  379:    for (OpOperand &opOperand : genericOp->getOpOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  380:      if (!areElementwiseOpsFusable(&opOperand))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  381:        continue;
    #####:  382:      if (!controlFn(&opOperand))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  383:        continue;
        -:  384:
    #####:  385:      FailureOr<Operation *> fusedOp = fuseElementwiseOps(rewriter, &opOperand);
call    0 never executed
    #####:  386:      if (succeeded(fusedOp)) {
branch  0 never executed
branch  1 never executed
    #####:  387:        auto replacements =
branch  0 never executed
branch  1 never executed
    #####:  388:            fusedOp.value()->getResults().take_back(genericOp.getNumResults());
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  389:        rewriter.replaceOp(genericOp, replacements);
call    0 never executed
call    1 never executed
    #####:  390:        return success();
        -:  391:      }
        -:  392:    }
    #####:  393:    return failure();
        -:  394:  }
        -:  395:
        -:  396:private:
        -:  397:  ControlFusionFn controlFn;
        -:  398:};
        -:  399:} // namespace
        -:  400:
        -:  401://===---------------------------------------------------------------------===//
        -:  402:// Methods and patterns that fuse reshape ops with elementwise operations by
        -:  403:// expanding the dimensionality of the elementwise operations.
        -:  404://===---------------------------------------------------------------------===//
        -:  405:
        -:  406:/// Conditions for folding a generic operation with a reshape op by expanding
        -:  407:/// the iteration space dimensionality for tensor operations. These are
        -:  408:/// preconditions assumed by `foldReshapeByDimExpansion` which implements the
        -:  409:/// following fusion pattern.
        -:  410:///
        -:  411:///  Consider
        -:  412:///
        -:  413:///  %c = linalg.generic ins(%a, %b : memref<?x?x?xf32>, memref<?x?xf32>)
        -:  414:///         indexing_maps = [affine_map<(d0, d1, d2) -> (d1, d0, d2)>,
        -:  415:///                          affine_map<(d0, d1, d2) -> (d1, d2)>,
        -:  416:///                          affine_map<(d0, d1, d2) -> (d0, d2, d1)>]
        -:  417:///  %d = tensor.expand_shape %c [[0, 1], [2], [3, 4, 5]]
        -:  418:///       : tensor<?x?x?xf32> into tensor<?x?x?x?x?x?xf32>
        -:  419:///
        -:  420:///  The reshape can be folded into the `genericOp` if its loop dimensionality
        -:  421:///  is increased to match the result (operand) of the tensor.expand_shape.
        -:  422:///  The indexing_map of the fused tensor in the `genericOp` and the
        -:  423:///  reassociation map helps compute the indexing maps of the modified op.
        -:  424:///  For the above example, based on the reassociation map it
        -:  425:///  can be concluded that
        -:  426:///
        -:  427:///  - The loop used to access the first dimension of the fused tensor is split
        -:  428:///    into two.
        -:  429:///  - The loop used to access the second dimension of the fused tensor is kept
        -:  430:///    as is.
        -:  431:///  - The loop used to access the third dimension of the fused tensor is split
        -:  432:///    into three.
        -:  433:///
        -:  434:///  i.e. (e0, e1, e2, e3, e4) is the domain of the indexing map of the modified
        -:  435:///  op, then
        -:  436:///
        -:  437:///   d0 -> e0, e1
        -:  438:///   d1 -> e2, e3, e4
        -:  439:///   d2 -> e5
        -:  440:///
        -:  441:///  substituting this, the generic op can be rewritten as
        -:  442:///
        -:  443:///  %d = linalg.generic ins(%0, %1 : )
        -:  444:///        indexing_maps =
        -:  445:///         [affine_map<(e0, e1, e2, e3, e4, e5) -> (e2, e3, e4, e0, e1, e5)>,
        -:  446:///          affine_map<(e0, e1, e2, e3, e4, e5) -> (e2, e3, e4, e5)>,
        -:  447:///          affine_map<(e0, e1, e2, e3, e4, e5) -> (e0, e1, e5, e2, e3, e4)>]
        -:  448:///
        -:  449:///  Since operands to the linalg generic are now 5D, reshapes can be introduced
        -:  450:///  to make it consistent
        -:  451:///
        -:  452:///  %0 = tensor.expand_shape %a [[0, 1, 2], [3, 4], [5]]
        -:  453:///       : tensor<?x?x?xf32> into tensor<?x?x?x?x?x?xf32>
        -:  454:///  %1 = tensor.expand_shape %b [[0, 1, 2], [3]]
        -:  455:///       : tensor<?x?x?xf32> into tensor<?x?x?x?xf32>
        -:  456:///
        -:  457:///  The added reshapes are again expanding patterns, so they will get fused
        -:  458:///  with its producers if possible.
function _ZL34isFusableWithReshapeByDimExpansionN4mlir6linalg9GenericOpEPNS_9OpOperandE called 0 returned 0% blocks executed 0%
    #####:  459:static bool isFusableWithReshapeByDimExpansion(GenericOp genericOp,
        -:  460:                                               OpOperand *fusableOpOperand) {
        -:  461:  // Is fusable only if:
        -:  462:  // - All the indexing maps for operands and results are projected
        -:  463:  //   permutations.
        -:  464:  // - The fused tensor is not a scalar.
        -:  465:  // - All the loops are parallel loops.
    #####:  466:  return genericOp.hasTensorSemantics() &&
call    0 never executed
    #####:  467:         llvm::all_of(genericOp.getIndexingMaps().getValue(),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
function _ZZL34isFusableWithReshapeByDimExpansionN4mlir6linalg9GenericOpEPNS_9OpOperandEENKUlNS_9AttributeEE_clES4_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  468:                      [](Attribute attr) {
    #####:  469:                        return attr.cast<AffineMapAttr>()
call    0 never executed
    #####:  470:                            .getValue()
call    0 never executed
    #####:  471:                            .isProjectedPermutation();
call    0 never executed
    #####:  472:                      }) &&
branch  0 never executed
branch  1 never executed
    #####:  473:         genericOp.getMatchingIndexingMap(fusableOpOperand).getNumResults() > 0 &&
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  474:         llvm::all_of(genericOp.getIteratorTypesArray(), [](StringRef it) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -:  475:           return it == getParallelIteratorTypeName();
    #####:  476:         });
        -:  477:}
        -:  478:
        -:  479:namespace {
        -:  480:/// Information needed to expand a generic operation to fold the reshape with
        -:  481:/// it.
    #####:  482:class ExpansionInfo {
        -:  483:public:
        -:  484:  // Computes the mapping from original dimensions of the op to the dimensions
        -:  485:  // of the expanded op given the `indexingMap` of the fused operand/result of
        -:  486:  // the generic op, the `reassocationMaps` of the reshape op and the shape of
        -:  487:  // the expanded op.
        -:  488:  LogicalResult compute(LinalgOp linalgOp, OpOperand *fusableOpOperand,
        -:  489:                        ArrayRef<AffineMap> reassociationMaps,
        -:  490:                        ArrayRef<int64_t> expandedShape,
        -:  491:                        ArrayRef<int64_t> collapsedShape,
        -:  492:                        PatternRewriter &rewriter);
    #####:  493:  unsigned getOrigOpNumDims() const { return reassociation.size(); }
    #####:  494:  unsigned getExpandedOpNumDims() const { return expandedOpNumDims; }
function _ZNK12_GLOBAL__N_113ExpansionInfo15getExpandedDimsEj called 0 returned 0% blocks executed 0%
    #####:  495:  ReassociationIndicesRef getExpandedDims(unsigned i) const {
    #####:  496:    return reassociation[i];
branch  0 never executed
branch  1 never executed
        -:  497:  }
function _ZNK12_GLOBAL__N_113ExpansionInfo21getExpandedShapeOfDimEj called 0 returned 0% blocks executed 0%
    #####:  498:  ArrayRef<int64_t> getExpandedShapeOfDim(unsigned i) const {
    #####:  499:    return expandedShapeMap[i];
branch  0 never executed
branch  1 never executed
        -:  500:  }
        -:  501:  ArrayRef<int64_t> getOriginalShape() const { return originalLoopExtent; }
        -:  502:
        -:  503:private:
        -:  504:  /// Reassociation from the dimensions in the original operation to the
        -:  505:  /// dimension of the expanded operation.
        -:  506:  SmallVector<ReassociationIndices> reassociation;
        -:  507:  /// Mapping from extent of loops in the original operation, to the extent of
        -:  508:  /// loops in the expanded operation.
        -:  509:  SmallVector<SmallVector<int64_t>> expandedShapeMap;
        -:  510:  /// Extent of the loop in the original operation.
        -:  511:  SmallVector<int64_t> originalLoopExtent;
        -:  512:  unsigned expandedOpNumDims;
        -:  513:};
        -:  514:} // namespace
        -:  515:
        -:  516:LogicalResult ExpansionInfo::compute(LinalgOp linalgOp,
        -:  517:                                     OpOperand *fusableOpOperand,
        -:  518:                                     ArrayRef<AffineMap> reassociationMaps,
        -:  519:                                     ArrayRef<int64_t> expandedShape,
        -:  520:                                     ArrayRef<int64_t> collapsedShape,
        -:  521:                                     PatternRewriter &rewriter) {
        -:  522:  if (reassociationMaps.empty())
        -:  523:    return failure();
        -:  524:  AffineMap fusedIndexMap = linalgOp.getMatchingIndexingMap(fusableOpOperand);
        -:  525:
        -:  526:  SmallVector<int64_t, 4> originalLoopRange = linalgOp.getStaticLoopRanges();
        -:  527:  originalLoopExtent.assign(originalLoopRange.begin(), originalLoopRange.end());
        -:  528:
        -:  529:  reassociation.clear();
        -:  530:  expandedShapeMap.clear();
        -:  531:  // Compute the number of dimension in the expanded op that correspond to each
        -:  532:  // dimension of the original op.
        -:  533:  SmallVector<unsigned> numExpandedDims(fusedIndexMap.getNumDims(), 1);
        -:  534:  expandedShapeMap.resize(fusedIndexMap.getNumDims());
        -:  535:  for (const auto &resultExpr : llvm::enumerate(fusedIndexMap.getResults())) {
        -:  536:    unsigned pos = resultExpr.value().cast<AffineDimExpr>().getPosition();
        -:  537:    AffineMap foldedDims = reassociationMaps[resultExpr.index()];
        -:  538:    numExpandedDims[pos] = foldedDims.getNumResults();
        -:  539:    ArrayRef<int64_t> shape =
        -:  540:        expandedShape.slice(foldedDims.getDimPosition(0), numExpandedDims[pos]);
        -:  541:    expandedShapeMap[pos].assign(shape.begin(), shape.end());
        -:  542:  }
        -:  543:  // The remaining dimensions remain the same.
        -:  544:  for (unsigned i : llvm::seq<unsigned>(0, fusedIndexMap.getNumDims()))
        -:  545:    if (expandedShapeMap[i].empty())
        -:  546:      expandedShapeMap[i] = {originalLoopExtent[i]};
        -:  547:
        -:  548:  // Compute reassociation map from the original op to the expanded op.
        -:  549:  unsigned sum = 0;
        -:  550:  reassociation.reserve(fusedIndexMap.getNumDims());
        -:  551:  for (const auto &numFoldedDim : llvm::enumerate(numExpandedDims)) {
        -:  552:    auto seq = llvm::seq<int64_t>(sum, sum + numFoldedDim.value());
        -:  553:    reassociation.emplace_back(seq.begin(), seq.end());
        -:  554:    sum += numFoldedDim.value();
        -:  555:  }
        -:  556:  expandedOpNumDims = sum;
        -:  557:  return success();
        -:  558:}
        -:  559:
        -:  560:/// Epanding the body of a linalg operation requires adaptations of the accessed
        -:  561:/// loop indices. Specifically, access of indices in the original operation need
        -:  562:/// to be replaced with linearizations of indices in the expanded op. That
        -:  563:/// requires the shape of the expanded dimensions to be static (at least all but
        -:  564:/// the most significant). For now check that these are all statically sized.
        -:  565:/// Note that this could be extended to handle dynamic case, but the
        -:  566:/// implementation below uses `affine.apply` which seems to have issues when the
        -:  567:/// shapes are not static.
function _ZL21isGenericOpExpandableN4mlir6linalg9GenericOpERKN12_GLOBAL__N_113ExpansionInfoERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  568:static LogicalResult isGenericOpExpandable(GenericOp genericOp,
        -:  569:                                           const ExpansionInfo &expansionInfo,
        -:  570:                                           PatternRewriter &rewriter) {
    #####:  571:  if (!genericOp.hasIndexSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  572:    return success();
    #####:  573:  for (unsigned i : llvm::seq<unsigned>(0, expansionInfo.getOrigOpNumDims())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  574:    ArrayRef<int64_t> expandedShape = expansionInfo.getExpandedShapeOfDim(i);
call    0 never executed
    #####:  575:    if (expandedShape.size() == 1)
branch  0 never executed
branch  1 never executed
    #####:  576:      continue;
    #####:  577:    for (int64_t shape : expandedShape.drop_front()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  578:      if (ShapedType::isDynamic(shape)) {
branch  0 never executed
branch  1 never executed
    #####:  579:        return rewriter.notifyMatchFailure(
    #####:  580:            genericOp, "cannot expand due to index semantics and dynamic dims");
call    0 never executed
        -:  581:      }
        -:  582:    }
        -:  583:  }
    #####:  584:  return success();
        -:  585:}
        -:  586:
        -:  587:/// Return the indexing map to use in the expanded op for a given the
        -:  588:/// `indexingMap` of the original operation.
        -:  589:static AffineMap
function _ZL26getIndexingMapInExpandedOpRN4mlir9OpBuilderENS_9AffineMapERKN12_GLOBAL__N_113ExpansionInfoE called 0 returned 0% blocks executed 0%
    #####:  590:getIndexingMapInExpandedOp(OpBuilder &builder, AffineMap indexingMap,
        -:  591:                           const ExpansionInfo &expansionInfo) {
    #####:  592:  SmallVector<AffineExpr> newExprs;
call    0 never executed
    #####:  593:  for (AffineExpr expr : indexingMap.getResults()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  594:    unsigned pos = expr.cast<AffineDimExpr>().getPosition();
call    0 never executed
call    1 never executed
    #####:  595:    SmallVector<AffineExpr, 4> expandedExprs = llvm::to_vector<4>(
    #####:  596:        llvm::map_range(expansionInfo.getExpandedDims(pos), [&](int64_t v) {
call    0 never executed
call    1 never executed
        -:  597:          return builder.getAffineDimExpr(static_cast<unsigned>(v));
    #####:  598:        }));
call    0 never executed
    #####:  599:    newExprs.append(expandedExprs.begin(), expandedExprs.end());
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  600:  }
    #####:  601:  return AffineMap::get(expansionInfo.getExpandedOpNumDims(),
        -:  602:                        indexingMap.getNumSymbols(), newExprs,
    #####:  603:                        builder.getContext());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  604:}
        -:  605:
        -:  606:/// Return the type of the operand/result to use in the expanded op given the
        -:  607:/// type in the original op.
function _ZL15getExpandedTypeN4mlir16RankedTensorTypeENS_9AffineMapERKN12_GLOBAL__N_113ExpansionInfoE called 0 returned 0% blocks executed 0%
    #####:  608:static RankedTensorType getExpandedType(RankedTensorType originalType,
        -:  609:                                        AffineMap indexingMap,
        -:  610:                                        const ExpansionInfo &expansionInfo) {
    #####:  611:  SmallVector<int64_t> expandedShape;
call    0 never executed
    #####:  612:  for (AffineExpr expr : indexingMap.getResults()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  613:    unsigned dim = expr.cast<AffineDimExpr>().getPosition();
call    0 never executed
call    1 never executed
    #####:  614:    auto dimExpansion = expansionInfo.getExpandedShapeOfDim(dim);
call    0 never executed
    #####:  615:    expandedShape.append(dimExpansion.begin(), dimExpansion.end());
call    0 never executed
        -:  616:  }
    #####:  617:  return RankedTensorType::get(expandedShape, originalType.getElementType());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  618:}
        -:  619:
        -:  620:/// Returns the reassociation maps to use in the `tensor.expand_shape`
        -:  621:/// operation to convert the operands of the original operation to operands of
        -:  622:/// the expanded operation. The same method is used to compute the
        -:  623:/// `tensor.collapse_shape` used to collapse the result of the expanded
        -:  624:/// op to get the value that can replace all uses of the results of the original
        -:  625:/// op.
        -:  626:static SmallVector<ReassociationIndices>
function _ZL28getReassociationForExpansionN4mlir9AffineMapERKN12_GLOBAL__N_113ExpansionInfoE called 0 returned 0% blocks executed 0%
    #####:  627:getReassociationForExpansion(AffineMap indexingMap,
        -:  628:                             const ExpansionInfo &expansionInfo) {
    #####:  629:  SmallVector<ReassociationIndices> reassociation;
call    0 never executed
    #####:  630:  unsigned numReshapeDims = 0;
    #####:  631:  for (AffineExpr expr : indexingMap.getResults()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  632:    unsigned dim = expr.cast<AffineDimExpr>().getPosition();
call    0 never executed
call    1 never executed
    #####:  633:    auto numExpandedDims = expansionInfo.getExpandedDims(dim).size();
call    0 never executed
call    1 never executed
    #####:  634:    SmallVector<int64_t, 2> indices = llvm::to_vector<2>(
    #####:  635:        llvm::seq<int64_t>(numReshapeDims, numReshapeDims + numExpandedDims));
call    0 never executed
call    1 never executed
    #####:  636:    reassociation.emplace_back(std::move(indices));
call    0 never executed
    #####:  637:    numReshapeDims += numExpandedDims;
branch  0 never executed
branch  1 never executed
        -:  638:  }
    #####:  639:  return reassociation;
        -:  640:}
        -:  641:
        -:  642:/// Update the body of an expanded linalg operation having index semantics. The
        -:  643:/// indices of the original operation need to be recovered by linearizing the
        -:  644:/// indices of the correspoding dimensions of the expanded operation. For now it
        -:  645:/// is assumed that the shapes of the expanded operation needed for
        -:  646:/// linearization are static.
function _ZL29updateExpandedGenericOpRegionRN4mlir15PatternRewriterENS_8LocationERNS_6RegionERKN12_GLOBAL__N_113ExpansionInfoE called 0 returned 0% blocks executed 0%
    #####:  647:static void updateExpandedGenericOpRegion(PatternRewriter &rewriter,
        -:  648:                                          Location loc, Region &fusedRegion,
        -:  649:                                          const ExpansionInfo &expansionInfo) {
        -:  650:  // Replace the original indices by the linearization of the expanded indices.
    #####:  651:  for (IndexOp indexOp :
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  652:       llvm::make_early_inc_range(fusedRegion.front().getOps<IndexOp>())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
    #####:  653:    ArrayRef<int64_t> expandedDims =
    #####:  654:        expansionInfo.getExpandedDims(indexOp.getDim());
call    0 never executed
call    1 never executed
    #####:  655:    assert(!expandedDims.empty() && "expected valid expansion info");
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  656:
        -:  657:    // Skip index operations that are not affected by the expansion.
    #####:  658:    if (expandedDims.size() == 1 &&
branch  0 never executed
branch  1 never executed
    #####:  659:        expandedDims.front() == (int64_t)indexOp.getDim())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  660:      continue;
        -:  661:
        -:  662:    // Linearize the expanded indices of the original index dimension.
    #####:  663:    OpBuilder::InsertionGuard guard(rewriter);
call    0 never executed
    #####:  664:    rewriter.setInsertionPointAfter(indexOp);
call    0 never executed
    #####:  665:    ArrayRef<int64_t> expandedDimsShape =
    #####:  666:        expansionInfo.getExpandedShapeOfDim(indexOp.getDim()).drop_front();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  667:    SmallVector<Value> expandedIndices;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  668:    expandedIndices.reserve(expandedDims.size() - 1);
branch  0 never executed
branch  1 never executed
    #####:  669:    llvm::transform(
    #####:  670:        expandedDims.drop_front(), std::back_inserter(expandedIndices),
call    0 never executed
call    1 never executed
    #####:  671:        [&](int64_t dim) { return rewriter.create<IndexOp>(loc, dim); });
call    0 never executed
call    1 never executed
    #####:  672:    Value newIndex = rewriter.create<IndexOp>(loc, expandedDims.front());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  673:    for (auto it : llvm::zip(expandedDimsShape, expandedIndices)) {
branch  0 never executed
branch  1 never executed
    #####:  674:      assert(!ShapedType::isDynamic(std::get<0>(it)));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  675:      AffineExpr idx, acc;
    #####:  676:      bindDims(rewriter.getContext(), idx, acc);
call    0 never executed
    #####:  677:      newIndex = rewriter.create<AffineApplyOp>(
    #####:  678:          indexOp.getLoc(), idx + acc * std::get<0>(it),
call    0 never executed
call    1 never executed
    #####:  679:          ValueRange{std::get<1>(it), newIndex});
call    0 never executed
call    1 never executed
call    2 never executed
        -:  680:    }
    #####:  681:    rewriter.replaceOp(indexOp, newIndex);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  682:  }
    #####:  683:}
        -:  684:
        -:  685:/// Implements the fusion of a tensor.collapse_shape or a tensor.expand_shape op
        -:  686:/// and a generic op as explained in `isFusableWithReshapeByExpansion`. Assumes
        -:  687:/// that those conditions have been satisfied.
        -:  688:static Optional<SmallVector<Value>>
function _ZL26fuseWithReshapeByExpansionN4mlir6linalg9GenericOpEPNS_9OperationEPNS_9OpOperandERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  689:fuseWithReshapeByExpansion(GenericOp genericOp, Operation *reshapeOp,
        -:  690:                           OpOperand *fusableOpOperand,
        -:  691:                           PatternRewriter &rewriter) {
    #####:  692:  assert(isFusableWithReshapeByDimExpansion(genericOp, fusableOpOperand) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  693:         "preconditions for fuse operation failed");
        -:  694:  // Check if reshape is expanding or collapsing.
    #####:  695:  auto expandingReshapeOp = dyn_cast<tensor::ExpandShapeOp>(*reshapeOp);
call    0 never executed
    #####:  696:  auto collapsingReshapeOp = dyn_cast<tensor::CollapseShapeOp>(*reshapeOp);
call    0 never executed
    #####:  697:  bool isExpanding = (expandingReshapeOp != nullptr);
branch  0 never executed
branch  1 never executed
    #####:  698:  RankedTensorType expandedType = isExpanding
    #####:  699:                                      ? expandingReshapeOp.getResultType()
call    0 never executed
    #####:  700:                                      : collapsingReshapeOp.getSrcType();
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  701:  RankedTensorType collapsedType = isExpanding
    #####:  702:                                       ? expandingReshapeOp.getSrcType()
call    0 never executed
    #####:  703:                                       : collapsingReshapeOp.getResultType();
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  704:
    #####:  705:  ExpansionInfo expansionInfo;
call    0 never executed
    #####:  706:  if (failed(expansionInfo.compute(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
        -:  707:          genericOp, fusableOpOperand,
    #####:  708:          isExpanding ? expandingReshapeOp.getReassociationMaps()
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
        -:  709:                      : collapsingReshapeOp.getReassociationMaps(),
        -:  710:          expandedType.getShape(), collapsedType.getShape(), rewriter)))
    #####:  711:    return llvm::None;
        -:  712:
    #####:  713:  if (failed(isGenericOpExpandable(genericOp, expansionInfo, rewriter)))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  714:    return llvm::None;
        -:  715:
    #####:  716:  SmallVector<AffineMap, 4> expandedOpIndexingMaps = llvm::to_vector<4>(
    #####:  717:      llvm::map_range(genericOp.getIndexingMapsArray(), [&](AffineMap m) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  718:        return getIndexingMapInExpandedOp(rewriter, m, expansionInfo);
call    0 never executed
    #####:  719:      }));
call    0 never executed
call    1 never executed
        -:  720:
        -:  721:  // Set insertion point to the generic op.
    #####:  722:  OpBuilder::InsertionGuard g(rewriter);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  723:  rewriter.setInsertionPoint(genericOp);
call    0 never executed
        -:  724:
    #####:  725:  SmallVector<Value> expandedOpOperands;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  726:  expandedOpOperands.reserve(genericOp.getNumDpsInputs());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  727:  for (OpOperand *opOperand : genericOp.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  728:    if (opOperand == fusableOpOperand) {
branch  0 never executed
branch  1 never executed
    #####:  729:      expandedOpOperands.push_back(isExpanding ? expandingReshapeOp.getSrc()
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  730:                                               : collapsingReshapeOp.getSrc());
call    0 never executed
    #####:  731:      continue;
        -:  732:    }
    #####:  733:    if (auto opOperandType =
branch  0 never executed
branch  1 never executed
    #####:  734:            opOperand->get().getType().dyn_cast<RankedTensorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  735:      AffineMap indexingMap = genericOp.getMatchingIndexingMap(opOperand);
call    0 never executed
    #####:  736:      RankedTensorType expandedOperandType =
    #####:  737:          getExpandedType(opOperandType, indexingMap, expansionInfo);
call    0 never executed
    #####:  738:      if (expandedOperandType != opOperand->get().getType()) {
branch  0 never executed
branch  1 never executed
        -:  739:        // Reshape the operand to get the right type.
    #####:  740:        SmallVector<ReassociationIndices> reassociation =
    #####:  741:            getReassociationForExpansion(indexingMap, expansionInfo);
call    0 never executed
    #####:  742:        if (failed(reshapeLikeShapesAreCompatible(
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  743:                [&](const Twine &msg) {
    #####:  744:                  return rewriter.notifyMatchFailure(genericOp, msg);
call    0 never executed
        -:  745:                },
        -:  746:                opOperandType.getShape(), expandedOperandType.getShape(),
        -:  747:                reassociation,
        -:  748:                /*isExpandingReshape=*/true)))
    #####:  749:          return llvm::None;
call    0 never executed
    #####:  750:        expandedOpOperands.push_back(rewriter.create<tensor::ExpandShapeOp>(
call    0 never executed
    #####:  751:            genericOp.getLoc(), expandedOperandType, opOperand->get(),
call    0 never executed
    #####:  752:            reassociation));
call    0 never executed
call    1 never executed
    #####:  753:        continue;
call    0 never executed
        -:  754:      }
        -:  755:    }
    #####:  756:    expandedOpOperands.push_back(opOperand->get());
call    0 never executed
        -:  757:  }
        -:  758:
    #####:  759:  Location loc = genericOp.getLoc();
call    0 never executed
    #####:  760:  SmallVector<Value> outputs;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  761:  for (OpOperand *opOperand : genericOp.getDpsInitOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  762:    AffineMap indexingMap = genericOp.getMatchingIndexingMap(opOperand);
call    0 never executed
    #####:  763:    auto opOperandType = opOperand->get().getType().cast<RankedTensorType>();
call    0 never executed
    #####:  764:    RankedTensorType expandedOutputType =
    #####:  765:        getExpandedType(opOperandType, indexingMap, expansionInfo);
call    0 never executed
    #####:  766:    if (expandedOutputType != opOperand->get().getType()) {
branch  0 never executed
branch  1 never executed
    #####:  767:      SmallVector<ReassociationIndices> reassociation =
    #####:  768:          getReassociationForExpansion(indexingMap, expansionInfo);
call    0 never executed
    #####:  769:      if (failed(reshapeLikeShapesAreCompatible(
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  770:              [&](const Twine &msg) {
    #####:  771:                return rewriter.notifyMatchFailure(genericOp, msg);
call    0 never executed
        -:  772:              },
        -:  773:              opOperandType.getShape(), expandedOutputType.getShape(),
        -:  774:              reassociation,
        -:  775:              /*isExpandingReshape=*/true)))
    #####:  776:        return llvm::None;
call    0 never executed
    #####:  777:      outputs.push_back(rewriter.create<tensor::ExpandShapeOp>(
call    0 never executed
    #####:  778:          genericOp.getLoc(), expandedOutputType, opOperand->get(),
call    0 never executed
call    1 never executed
    #####:  779:          reassociation));
call    0 never executed
call    1 never executed
        -:  780:    } else {
    #####:  781:      outputs.push_back(opOperand->get());
call    0 never executed
        -:  782:    }
        -:  783:  }
        -:  784:
        -:  785:  // The iterator types of the expanded op are all parallel.
    #####:  786:  SmallVector<StringRef> iteratorTypes(expansionInfo.getExpandedOpNumDims(),
call    0 never executed
    #####:  787:                                       getParallelIteratorTypeName());
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  788:
    #####:  789:  TypeRange resultTypes = ValueRange(outputs).getTypes();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  790:  auto fusedOp =
        -:  791:      rewriter.create<GenericOp>(genericOp.getLoc(), resultTypes,
        -:  792:                                 /*inputs=*/expandedOpOperands, outputs,
    #####:  793:                                 expandedOpIndexingMaps, iteratorTypes);
call    0 never executed
    #####:  794:  Region &fusedRegion = fusedOp->getRegion(0);
call    0 never executed
    #####:  795:  Region &originalRegion = genericOp->getRegion(0);
call    0 never executed
    #####:  796:  rewriter.cloneRegionBefore(originalRegion, fusedRegion, fusedRegion.begin());
call    0 never executed
        -:  797:
        -:  798:  // Update the index accesses after the expansion.
    #####:  799:  updateExpandedGenericOpRegion(rewriter, loc, fusedRegion, expansionInfo);
call    0 never executed
        -:  800:
        -:  801:  // Reshape the result values to their original shape if this is a collapsing
        -:  802:  // reshape folded into its consumer.
    #####:  803:  SmallVector<Value> resultVals;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  804:  for (OpResult opResult : genericOp->getOpResults()) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  805:    int64_t resultNumber = opResult.getResultNumber();
call    0 never executed
    #####:  806:    if (resultTypes[resultNumber] != opResult.getType()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  807:      SmallVector<ReassociationIndices> reassociation =
        -:  808:          getReassociationForExpansion(
        -:  809:              genericOp.getMatchingIndexingMap(
        -:  810:                  genericOp.getDpsInitOperand(resultNumber)),
    #####:  811:              expansionInfo);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  812:      resultVals.push_back(rewriter.create<tensor::CollapseShapeOp>(
call    0 never executed
    #####:  813:          genericOp.getLoc(), opResult.getType(),
    #####:  814:          fusedOp->getResult(resultNumber), reassociation));
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
        -:  815:    } else {
    #####:  816:      resultVals.push_back(fusedOp->getResult(resultNumber));
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  817:    }
        -:  818:  }
        -:  819:  // Assuming a single result.
    #####:  820:  return resultVals;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  821:}
        -:  822:
        -:  823:namespace {
        -:  824:
        -:  825:/// Pattern to fuse a tensor.collapse_shape op with its consumer generic op,
        -:  826:/// when the reshape op is collapsing dimensions. The dimensionality of the loop
        -:  827:/// in the consumer is expanded.
        -:  828:class FoldWithProducerReshapeOpByExpansion
        -:  829:    : public OpRewritePattern<GenericOp> {
        -:  830:public:
function _ZN12_GLOBAL__N_136FoldWithProducerReshapeOpByExpansionC2EPN4mlir11MLIRContextESt8functionIFbPNS1_9OpOperandEEENS1_14PatternBenefitE called 412 returned 100% blocks executed 100%
      412:  831:  FoldWithProducerReshapeOpByExpansion(MLIRContext *context,
        -:  832:                                       ControlFusionFn foldReshapes,
        -:  833:                                       PatternBenefit benefit = 1)
      412:  834:      : OpRewritePattern<GenericOp>(context, benefit),
      412:  835:        controlFoldingReshapes(std::move(foldReshapes)) {}
call    0 returned 100%
        -:  836:
function _ZNK12_GLOBAL__N_136FoldWithProducerReshapeOpByExpansion15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  837:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -:  838:                                PatternRewriter &rewriter) const override {
    #####:  839:    for (OpOperand *opOperand : genericOp.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  840:      tensor::CollapseShapeOp reshapeOp =
    #####:  841:          opOperand->get().getDefiningOp<tensor::CollapseShapeOp>();
call    0 never executed
    #####:  842:      if (!reshapeOp)
branch  0 never executed
branch  1 never executed
    #####:  843:        continue;
        -:  844:      // Fold only if
        -:  845:      // - The tensor reshape op is folding.
        -:  846:      // - All constraints of fusing with reshape by expansion are met.
    #####:  847:      if (!isFusableWithReshapeByDimExpansion(genericOp, opOperand) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  848:          (!controlFoldingReshapes(opOperand)))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  849:        continue;
        -:  850:
    #####:  851:      Optional<SmallVector<Value>> replacementValues =
    #####:  852:          fuseWithReshapeByExpansion(genericOp, reshapeOp, opOperand, rewriter);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  853:      if (!replacementValues)
branch  0 never executed
branch  1 never executed
    #####:  854:        return failure();
    #####:  855:      rewriter.replaceOp(genericOp, *replacementValues);
call    0 never executed
call    1 never executed
    #####:  856:      return success();
branch  0 never executed
branch  1 never executed
        -:  857:    }
    #####:  858:    return failure();
        -:  859:  }
        -:  860:
        -:  861:private:
        -:  862:  ControlFusionFn controlFoldingReshapes;
        -:  863:};
        -:  864:
        -:  865:/// Pattern to fold a tensor.expand_shape op with its producer generic op
        -:  866:/// by expanding the dimensionality of the loop in the producer op.
        -:  867:struct FoldReshapeWithGenericOpByExpansion
        -:  868:    : public OpRewritePattern<tensor::ExpandShapeOp> {
        -:  869:
function _ZN12_GLOBAL__N_135FoldReshapeWithGenericOpByExpansionC2EPN4mlir11MLIRContextESt8functionIFbPNS1_9OpOperandEEENS1_14PatternBenefitE called 412 returned 100% blocks executed 100%
      412:  870:  FoldReshapeWithGenericOpByExpansion(MLIRContext *context,
        -:  871:                                      ControlFusionFn foldReshapes,
        -:  872:                                      PatternBenefit benefit = 1)
      412:  873:      : OpRewritePattern<tensor::ExpandShapeOp>(context, benefit),
      412:  874:        controlFoldingReshapes(std::move(foldReshapes)) {}
call    0 returned 100%
        -:  875:
function _ZNK12_GLOBAL__N_135FoldReshapeWithGenericOpByExpansion15matchAndRewriteEN4mlir6tensor13ExpandShapeOpERNS1_15PatternRewriterE called 822 returned 100% blocks executed 18%
      822:  876:  LogicalResult matchAndRewrite(tensor::ExpandShapeOp reshapeOp,
        -:  877:                                PatternRewriter &rewriter) const override {
        -:  878:    // Fold only if all constraints of fusing with reshape by expansion are met.
      822:  879:    auto producerResult = reshapeOp.getSrc().dyn_cast<OpResult>();
call    0 returned 100%
call    1 returned 100%
      822:  880:    if (!producerResult) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  881:      return rewriter.notifyMatchFailure(reshapeOp,
    #####:  882:                                         "source not produced by an operation");
call    0 never executed
        -:  883:    }
        -:  884:
      822:  885:    auto producer = dyn_cast<GenericOp>(producerResult.getOwner());
call    0 returned 100%
call    1 returned 100%
      822:  886:    if (!producer) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
      822:  887:      return rewriter.notifyMatchFailure(reshapeOp,
      822:  888:                                         "producer not a generic op");
call    0 returned 100%
        -:  889:    }
        -:  890:
    #####:  891:    if (!isFusableWithReshapeByDimExpansion(
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  892:            producer,
    #####:  893:            producer.getDpsInitOperand(producerResult.getResultNumber()))) {
call    0 never executed
    #####:  894:      return rewriter.notifyMatchFailure(
    #####:  895:          reshapeOp, "failed preconditions of fusion with producer generic op");
call    0 never executed
        -:  896:    }
        -:  897:
    #####:  898:    if (!controlFoldingReshapes(&reshapeOp->getOpOperand(0))) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  899:      return rewriter.notifyMatchFailure(reshapeOp,
    #####:  900:                                         "fusion blocked by control function");
call    0 never executed
        -:  901:    }
        -:  902:
    #####:  903:    Optional<SmallVector<Value>> replacementValues = fuseWithReshapeByExpansion(
call    0 never executed
        -:  904:        producer, reshapeOp,
    #####:  905:        producer.getDpsInitOperand(producerResult.getResultNumber()), rewriter);
call    0 never executed
call    1 never executed
    #####:  906:    if (!replacementValues) {
branch  0 never executed
branch  1 never executed
    #####:  907:      return rewriter.notifyMatchFailure(reshapeOp,
    #####:  908:                                         "fusion by expansion failed");
call    0 never executed
        -:  909:    }
        -:  910:
        -:  911:    // Find the replacement for the reshape op. Since the replacements have the
        -:  912:    // same type as the returns of the original generic op, the consumer reshape
        -:  913:    // op can be replaced by the source of the collapse_shape op that defines
        -:  914:    // the replacement.
    #####:  915:    Value reshapeReplacement = (*replacementValues)
call    0 never executed
    #####:  916:        [reshapeOp.getSrc().cast<OpResult>().getResultNumber()];
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  917:    if (auto collapseOp =
branch  0 never executed
branch  1 never executed
    #####:  918:            reshapeReplacement.getDefiningOp<tensor::CollapseShapeOp>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  919:      reshapeReplacement = collapseOp.getSrc();
call    0 never executed
        -:  920:    }
    #####:  921:    rewriter.replaceOp(reshapeOp, reshapeReplacement);
call    0 never executed
call    1 never executed
    #####:  922:    rewriter.replaceOp(producer, *replacementValues);
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  923:    return success();
branch  0 never executed
branch  1 never executed
        -:  924:  }
        -:  925:
        -:  926:private:
        -:  927:  ControlFusionFn controlFoldingReshapes;
        -:  928:};
        -:  929:} // namespace
        -:  930:
        -:  931://===---------------------------------------------------------------------===//
        -:  932:// Methods and patterns to fuse reshape with linalg.generic operations by
        -:  933:// contraction of dimensions.
        -:  934://===---------------------------------------------------------------------===//
        -:  935:
        -:  936:/// For a given list of indices in the range of the `indexingMap` that are
        -:  937:/// folded, return the indices of the corresponding domain. Return `llvm::None`
        -:  938:/// on failure. Ensures that all the elements of the returned reassociation are
        -:  939:/// distinct.
        -:  940:static ReassociationIndices
function _ZL22getDomainReassociationN4mlir9AffineMapEN4llvm8ArrayRefIlEE called 0 returned 0% blocks executed 0%
    #####:  941:getDomainReassociation(AffineMap indexingMap,
        -:  942:                       ReassociationIndicesRef rangeReassociation) {
    #####:  943:  assert(indexingMap.isProjectedPermutation() &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  944:         "expected projected permutation");
        -:  945:
    #####:  946:  ReassociationIndices domainReassociation = llvm::to_vector<4>(
branch  0 never executed
branch  1 never executed
function _ZZL22getDomainReassociationN4mlir9AffineMapEN4llvm8ArrayRefIlEEENKUllE_clEl.isra.0 called 0 returned 0% blocks executed 0%
    #####:  947:      llvm::map_range(rangeReassociation, [&](int64_t pos) -> int64_t {
    #####:  948:        return indexingMap.getResults()[pos]
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  949:            .cast<AffineDimExpr>()
call    0 never executed
    #####:  950:            .getPosition();
call    0 never executed
    #####:  951:      }));
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  952:  // The projected permutation semantics ensures that there is no repetition of
        -:  953:  // the domain indices.
    #####:  954:  return domainReassociation;
        -:  955:}
        -:  956:
        -:  957:/// For a given `dimSequence`, check if the sequence is conserved in the
        -:  958:/// `indexingMap`. `indexingMap` is expected to be a projected permutation.
        -:  959:/// Non-existence of the sequence returns true as well.
function _ZL22isDimSequencePreservedN4mlir9AffineMapEN4llvm8ArrayRefIlEE called 0 returned 0% blocks executed 0%
    #####:  960:static bool isDimSequencePreserved(AffineMap indexingMap,
        -:  961:                                   ReassociationIndicesRef dimSequence) {
    #####:  962:  assert(!dimSequence.empty() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  963:         "expected non-empty list for dimension sequence");
    #####:  964:  assert(indexingMap.isProjectedPermutation() &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  965:         "expected indexing map to be projected permutation");
        -:  966:
    #####:  967:  llvm::SmallDenseSet<unsigned, 4> sequenceElements;
call    0 never executed
    #####:  968:  sequenceElements.insert(dimSequence.begin(), dimSequence.end());
call    0 never executed
        -:  969:
    #####:  970:  unsigned dimSequenceStart = dimSequence[0];
call    0 never executed
    #####:  971:  for (const auto &expr : enumerate(indexingMap.getResults())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  972:    unsigned dimInMapStart = expr.value().cast<AffineDimExpr>().getPosition();
call    0 never executed
call    1 never executed
        -:  973:    // 1.  Check if this start of the sequence.
    #####:  974:    if (dimInMapStart == dimSequenceStart) {
branch  0 never executed
branch  1 never executed
    #####:  975:      if (expr.index() + dimSequence.size() > indexingMap.getNumResults())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  976:        return false;
        -:  977:      // 1a. Check if sequence is preserved.
    #####:  978:      for (const auto &dimInSequence : enumerate(dimSequence)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  979:        unsigned dimInMap =
    #####:  980:            indexingMap.getResult(expr.index() + dimInSequence.index())
call    0 never executed
    #####:  981:                .cast<AffineDimExpr>()
call    0 never executed
    #####:  982:                .getPosition();
call    0 never executed
    #####:  983:        if (dimInMap != dimInSequence.value())
branch  0 never executed
branch  1 never executed
    #####:  984:          return false;
        -:  985:      }
        -:  986:      // Found the sequence. Projected permutation
        -:  987:      // enforces that all AffineDimExprs in the result are unique, so no
        -:  988:      // further checks are needed.
    #####:  989:      return true;
        -:  990:    }
        -:  991:    // 2. If position in the expr (which is of type AffineDimExpr) is part
        -:  992:    // of sequence, return false here. This implies the entire sequence does not
        -:  993:    // exist in the indexing map.
    #####:  994:    if (sequenceElements.count(dimInMapStart))
call    0 never executed
call    1 never executed
    #####:  995:      return false;
        -:  996:  }
        -:  997:  // 3. No element of sequence found. Return true.
    #####:  998:  return true;
        -:  999:}
        -: 1000:
        -: 1001:// Return the list of dimensions of the iteration domain that can be
        -: 1002:// collapsed to allow for fusion with the a producer that is an expand_shape
        -: 1003:// operation. If all dimensions created by expansion can be collapsed in the
        -: 1004:// iteration space then the reshape is defunct.
        -: 1005://
        -: 1006:// Example:
        -: 1007://
        -: 1008:// ```mlir
        -: 1009:// #map = affine_map<(d0, d1) -> (d0, d1)>
        -: 1010:// %1 = tensor.expand_shape %0 [[0, 1]] : tensor<?xf32> into tensor<?x4xf32>
        -: 1011:// %2 = tensor.empty [..] : tensor<?x4xf32>
        -: 1012:// %3 = linalg.generic {
        -: 1013://     indexing_maps = [#map, #map],
        -: 1014://     iterator_types = ["parallel" ,"parallel"]}
        -: 1015://     ins(%1 : tensor<?x4xf32>) outs(%2 : tensor<?x4xf32>) {.. }
        -: 1016:// ```
        -: 1017://
        -: 1018:// can be fused by collapsing the dimensions of the iteration space.
        -: 1019://
        -: 1020:// ```mlir
        -: 1021:// #map = affine_map<(d0) -> (d0)>
        -: 1022:// %2 = tensor.empty [..] : tensor<?xf32>
        -: 1023:// %3 = linalg.generic {
        -: 1024://     indexing_maps = [#map, #map],
        -: 1025://     iterator_types = ["parallel"]}
        -: 1026://     ins(%1 : tensor<?xf32>) outs(%2 : tensor<?xf32>) {.. }
        -: 1027:// %4 = tensor.expand_shape %3 [[0, 1]] : tensor<?xf32> into tensor<?x4xf32>
        -: 1028:// ```
        -: 1029://
        -: 1030:// In the following example,
        -: 1031://
        -: 1032:// ```mlir
        -: 1033:// #map0 = affine_map<(d0, d1) -> (d0, d1)>
        -: 1034:// #map1 = affine_map<(d0, d1) -> (d1, d0)>
        -: 1035:// %1 = tensor.expand_shape %0 [[0, 1]] : tensor<?xf32> into tensor<?x4xf32>
        -: 1036:// %2 = tensor.empty [..] : tensor<4x?xf32>
        -: 1037:// %2 = linalg.generic {
        -: 1038://     indexing_maps = [#map0, #map1],
        -: 1039://     iterator_types = ["parallel" ,"parallel"]}
        -: 1040://     ins(%1 : tensor<?x4xf32>) outs(%2 : tensor<4x?xf32>) {.. }
        -: 1041:// ```
        -: 1042://
        -: 1043:// the reshape cannot be fused with the generic op by collapsing the op
        -: 1044:// dimensions since the indexing maps will have to contain mods and divs
        -: 1045:// to preserve the accesses pattern. When no dimensions of the iteration
        -: 1046:// space are collapsable and empty vector is returned.
        -: 1047:static SmallVector<ReassociationIndices>
function _ZL32getCollapsableIterationSpaceDimsN4mlir6linalg9GenericOpEPNS_9OpOperandEN4llvm8ArrayRefINS4_11SmallVectorIlLj2EEEEE called 0 returned 0% blocks executed 0%
    #####: 1048:getCollapsableIterationSpaceDims(GenericOp genericOp, OpOperand *fusableOperand,
        -: 1049:                                 ArrayRef<ReassociationIndices> reassociation) {
        -: 1050:  // Some basic checks for this fusion to be valid.
    #####: 1051:  if (!genericOp.hasTensorSemantics() || genericOp.getNumDpsInits() != 1)
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1052:    return {};
        -: 1053:
    #####: 1054:  if (!llvm::all_of(genericOp.getIndexingMapsArray(), [](AffineMap map) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
        -: 1055:        return map.isProjectedPermutation();
        -: 1056:      })) {
    #####: 1057:    return {};
        -: 1058:  }
        -: 1059:
        -: 1060:  // Compute all the loops with the reduction iterator types.
    #####: 1061:  SmallVector<unsigned> reductionDims;
call    0 never executed
    #####: 1062:  genericOp.getReductionDims(reductionDims);
call    0 never executed
        -: 1063:
    #####: 1064:  llvm::SmallDenseSet<unsigned, 4> processedIterationDims;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1065:  AffineMap indexingMap = genericOp.getMatchingIndexingMap(fusableOperand);
call    0 never executed
    #####: 1066:  auto iteratorTypes = genericOp.getIteratorTypesArray();
call    0 never executed
call    1 never executed
    #####: 1067:  SmallVector<ReassociationIndices> iterationSpaceReassociation;
branch  0 never executed
branch  1 never executed
    #####: 1068:  for (ReassociationIndicesRef foldedRangeDims : reassociation) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1069:    assert(!foldedRangeDims.empty() && "unexpected empty reassociation");
branch  0 never executed
branch  1 never executed
call    2 never executed
        -: 1070:
        -: 1071:    // Ignore dims that are not folded.
    #####: 1072:    if (foldedRangeDims.size() == 1)
branch  0 never executed
branch  1 never executed
    #####: 1073:      continue;
        -: 1074:
    #####: 1075:    ReassociationIndices foldedIterationSpaceDims =
    #####: 1076:        getDomainReassociation(indexingMap, foldedRangeDims);
call    0 never executed
        -: 1077:
        -: 1078:    // Check that the folded iteration dims do not contain already processed
        -: 1079:    // dims.
    #####: 1080:    if (llvm::any_of(foldedIterationSpaceDims, [&](int64_t dim) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1081:          return processedIterationDims.count(dim);
        -: 1082:        }))
    #####: 1083:      continue;
        -: 1084:
        -: 1085:    // Check that all folded iterator types are all parallel or all reductions.
    #####: 1086:    StringRef startIteratorType = iteratorTypes[foldedIterationSpaceDims[0]];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1087:    if (!isParallelIterator(startIteratorType) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1088:        !isReductionIterator(startIteratorType))
call    0 never executed
    #####: 1089:      continue;
function _ZZL32getCollapsableIterationSpaceDimsN4mlir6linalg9GenericOpEPNS_9OpOperandEN4llvm8ArrayRefINS4_11SmallVectorIlLj2EEEEEENKUllE1_clEl.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1090:    if (llvm::any_of(foldedIterationSpaceDims, [&](int64_t dim) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1091:          return iteratorTypes[dim] != startIteratorType;
branch  0 never executed
branch  1 never executed
        -: 1092:        }))
    #####: 1093:      continue;
        -: 1094:
        -: 1095:    // If the folded dimensions correspond to a "reduction" iterator type,
        -: 1096:    // the folded dimensions need to be "in-order". Strictly speaking this is
        -: 1097:    // not necessary, for reductions that are associative and commutative,  but
        -: 1098:    // using a more strict definition of reduction for now.
    #####: 1099:    if (isReductionIterator(startIteratorType)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1100:      bool isContiguous = false;
    #####: 1101:      for (const auto &startDim : llvm::enumerate(reductionDims)) {
branch  0 never executed
branch  1 never executed
        -: 1102:        // Move window in `reductionDims` to start of the folded iteration dims.
    #####: 1103:        if (startDim.value() != foldedIterationSpaceDims[0])
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1104:          continue;
call    0 never executed
        -: 1105:        // If sizes doesnt match, trivial not contiguous. This condition should
        -: 1106:        // not be hit.
    #####: 1107:        if (startDim.index() + foldedIterationSpaceDims.size() >
branch  0 never executed
branch  1 never executed
    #####: 1108:            reductionDims.size())
branch  0 never executed
branch  1 never executed
        -: 1109:          break;
        -: 1110:        // Check that the contiguity is maintained.
    #####: 1111:        isContiguous = true;
    #####: 1112:        for (const auto &foldedDim :
    #####: 1113:             llvm::enumerate(foldedIterationSpaceDims)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1114:          if (reductionDims[foldedDim.index() + startDim.index()] !=
branch  0 never executed
branch  1 never executed
    #####: 1115:              foldedDim.value()) {
branch  0 never executed
branch  1 never executed
        -: 1116:            isContiguous = false;
        -: 1117:            break;
        -: 1118:          }
        -: 1119:        }
    #####: 1120:        break;
        -: 1121:      }
    #####: 1122:      if (!isContiguous)
branch  0 never executed
branch  1 never executed
    #####: 1123:        continue;
        -: 1124:    }
        -: 1125:
        -: 1126:    // Check that the sequence is preserved in all indexing maps.
    #####: 1127:    if (llvm::any_of(genericOp.getIndexingMapsArray(),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
        -: 1128:                     [&](AffineMap indexingMap) {
        -: 1129:                       return !isDimSequencePreserved(indexingMap,
        -: 1130:                                                      foldedIterationSpaceDims);
        -: 1131:                     }))
    #####: 1132:      continue;
        -: 1133:
    #####: 1134:    processedIterationDims.insert(foldedIterationSpaceDims.begin(),
call    0 never executed
        -: 1135:                                  foldedIterationSpaceDims.end());
    #####: 1136:    iterationSpaceReassociation.emplace_back(
call    0 never executed
    #####: 1137:        std::move(foldedIterationSpaceDims));
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1138:  }
        -: 1139:
    #####: 1140:  return iterationSpaceReassociation;
branch  0 never executed
branch  1 never executed
call    2 never executed
        -: 1141:}
        -: 1142:
        -: 1143:/// Helper class to carry state while collapsing the `linalg.generic` op.
        -: 1144:namespace {
    #####: 1145:class CollapsingInfo {
        -: 1146:public:
function _ZN12_GLOBAL__N_114CollapsingInfo10initializeEjN4llvm8ArrayRefINS1_11SmallVectorIlLj2EEEEE called 0 returned 0% blocks executed 0%
    #####: 1147:  LogicalResult initialize(unsigned origNumLoops,
        -: 1148:                           ArrayRef<ReassociationIndices> foldedIterationDims) {
    #####: 1149:    llvm::SmallDenseSet<int64_t, 4> processedDims;
call    0 never executed
        -: 1150:    // Find all the dims that are folded.
    #####: 1151:    for (ReassociationIndicesRef foldedIterationDim : foldedIterationDims) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1152:      if (foldedIterationDim.empty())
branch  0 never executed
branch  1 never executed
    #####: 1153:        continue;
        -: 1154:      // If the folded dims contain dims already folded, that's illegal
        -: 1155:      // specification. Repetition within a list is also illegal.
    #####: 1156:      for (auto dim : foldedIterationDim) {
branch  0 never executed
branch  1 never executed
    #####: 1157:        if (dim >= origNumLoops)
branch  0 never executed
branch  1 never executed
    #####: 1158:          return failure();
    #####: 1159:        if (processedDims.count(dim))
call    0 never executed
    #####: 1160:          return failure();
    #####: 1161:        processedDims.insert(dim);
call    0 never executed
        -: 1162:      }
    #####: 1163:      collapsedOpToOrigOpIterationDim.emplace_back(foldedIterationDim.begin(),
    #####: 1164:                                                   foldedIterationDim.end());
call    0 never executed
        -: 1165:    }
    #####: 1166:    if (processedDims.size() > origNumLoops)
branch  0 never executed
branch  1 never executed
    #####: 1167:      return failure();
        -: 1168:
        -: 1169:    // Add all the preserved dims of the original op as single
        -: 1170:    // elements to `collapsedOpToOrigOpIterationDim`.
    #####: 1171:    for (auto dim : llvm::seq<int64_t>(0, origNumLoops)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 1172:      if (processedDims.count(dim))
call    0 never executed
    #####: 1173:        continue;
    #####: 1174:      collapsedOpToOrigOpIterationDim.emplace_back(ReassociationIndices{dim});
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1175:    }
        -: 1176:
    #####: 1177:    llvm::sort(collapsedOpToOrigOpIterationDim,
call    0 never executed
function _ZZN12_GLOBAL__N_114CollapsingInfo10initializeEjN4llvm8ArrayRefINS1_11SmallVectorIlLj2EEEEEENKUlNS2_IlEES6_E_clES6_S6_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1178:               [&](ReassociationIndicesRef lhs, ReassociationIndicesRef rhs) {
    #####: 1179:                 return lhs[0] < rhs[0];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -: 1180:               });
    #####: 1181:    origOpToCollapsedOpIterationDim.resize(origNumLoops);
call    0 never executed
    #####: 1182:    for (const auto &foldedDims :
    #####: 1183:         llvm::enumerate(collapsedOpToOrigOpIterationDim)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1184:      for (const auto &dim : enumerate(foldedDims.value()))
branch  0 never executed
branch  1 never executed
    #####: 1185:        origOpToCollapsedOpIterationDim[dim.value()] =
call    0 never executed
    #####: 1186:            std::make_pair<int64_t, unsigned>(foldedDims.index(), dim.index());
branch  0 never executed
branch  1 never executed
call    2 never executed
        -: 1187:    }
    #####: 1188:    return success();
call    0 never executed
        -: 1189:  }
        -: 1190:
        -: 1191:  /// Return mapping from collapsed loop domain to original loop domain.
    #####: 1192:  ArrayRef<ReassociationIndices> getCollapsedOpToOrigOpMapping() const {
    #####: 1193:    return collapsedOpToOrigOpIterationDim;
        -: 1194:  }
        -: 1195:
        -: 1196:  /// Return mapping from original loop domain to collapsed loop domain. The
        -: 1197:  /// mapping is a pair. First value is the dimension in the collapsed loop that
        -: 1198:  /// the original loop is mapped to. Second is the relative position in folded
        -: 1199:  /// list of this domain. For example if the original loop domain is 3D, and
        -: 1200:  /// the collapsed loop domain is folding all of it, i.e.
        -: 1201:  ///
        -: 1202:  /// ```
        -: 1203:  /// collapsedOpToOrigOpMapping = [[0, 1, 2] [3, 4]]`
        -: 1204:  /// ```
        -: 1205:  ///
        -: 1206:  /// then
        -: 1207:  ///
        -: 1208:  /// ```
        -: 1209:  ///  origOpToCollapsedOpMapping[0] = {0, 0};
        -: 1210:  ///  origOpToCollapsedOpMapping[1] = {0, 1};
        -: 1211:  ///  origOpToCollapsedOpMapping[2] = {0, 2};
        -: 1212:  ///  origOpToCollapsedOpMapping[3] = {1, 0};
        -: 1213:  ///  origOpToCollapsedOpMapping[4] = {1, 1};
        -: 1214:  /// ```
        -: 1215:  ///
    #####: 1216:  ArrayRef<std::pair<int64_t, unsigned>> getOrigOpToCollapsedOpMapping() const {
    #####: 1217:    return origOpToCollapsedOpIterationDim;
        -: 1218:  }
        -: 1219:
        -: 1220:  /// Return the collapsed op iteration domain rank.
    #####: 1221:  unsigned getCollapsedOpIterationRank() const {
    #####: 1222:    return collapsedOpToOrigOpIterationDim.size();
        -: 1223:  }
        -: 1224:
        -: 1225:private:
        -: 1226:  /// Map from the iteration domain index in collapsed op to the iteration
        -: 1227:  /// domain indices in the original op.
        -: 1228:  SmallVector<ReassociationIndices> collapsedOpToOrigOpIterationDim;
        -: 1229:
        -: 1230:  /// Map from iteration domain index in the original op to the iteration domain
        -: 1231:  /// index in the collapsed op.
        -: 1232:  SmallVector<std::pair<int64_t, unsigned>> origOpToCollapsedOpIterationDim;
        -: 1233:};
        -: 1234:} // namespace
        -: 1235:
        -: 1236:/// Get the iterator types for the collapsed operation given the original
        -: 1237:/// iterator types and collapsed dimensions.
        -: 1238:static SmallVector<StringRef>
function _ZL27getCollapsedOpIteratorTypesN4llvm8ArrayRefIN4mlir9AttributeEEERKN12_GLOBAL__N_114CollapsingInfoE called 0 returned 0% blocks executed 0%
    #####: 1239:getCollapsedOpIteratorTypes(ArrayRef<Attribute> iteratorTypes,
        -: 1240:                            const CollapsingInfo &collapsingInfo) {
    #####: 1241:  SmallVector<StringRef> collapsedIteratorTypes;
    #####: 1242:  for (ReassociationIndicesRef foldedIterDims :
    #####: 1243:       collapsingInfo.getCollapsedOpToOrigOpMapping()) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1244:    assert(!foldedIterDims.empty() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -: 1245:           "reassociation indices expected to have non-empty sets");
        -: 1246:    // Just pick the iterator type of the first folded dim. Pre-condition checks
        -: 1247:    // expected to have checked that iterator types of all folded dimensions are
        -: 1248:    // the same.
    #####: 1249:    collapsedIteratorTypes.push_back(
call    0 never executed
    #####: 1250:        iteratorTypes[foldedIterDims[0]].cast<StringAttr>().getValue());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -: 1251:  }
    #####: 1252:  return collapsedIteratorTypes;
        -: 1253:}
        -: 1254:
        -: 1255:/// Compute the indexing map in the collapsed op that corresponds to the given
        -: 1256:/// `indexingMap` of the original operation.
        -: 1257:static AffineMap
function _ZL25getCollapsedOpIndexingMapN4mlir9AffineMapERKN12_GLOBAL__N_114CollapsingInfoE called 0 returned 0% blocks executed 0%
    #####: 1258:getCollapsedOpIndexingMap(AffineMap indexingMap,
        -: 1259:                          const CollapsingInfo &collapsingInfo) {
    #####: 1260:  MLIRContext *context = indexingMap.getContext();
call    0 never executed
    #####: 1261:  assert(indexingMap.isProjectedPermutation() &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -: 1262:         "expected indexing map to be projected permutation");
    #####: 1263:  SmallVector<AffineExpr> resultExprs;
call    0 never executed
    #####: 1264:  auto origOpToCollapsedOpMapping =
call    0 never executed
    #####: 1265:      collapsingInfo.getOrigOpToCollapsedOpMapping();
    #####: 1266:  for (auto expr : indexingMap.getResults()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1267:    unsigned dim = expr.cast<AffineDimExpr>().getPosition();
call    0 never executed
call    1 never executed
        -: 1268:    // If the dim is not the first of the collapsed dim, do nothing.
    #####: 1269:    if (origOpToCollapsedOpMapping[dim].second != 0)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1270:      continue;
        -: 1271:    // The next n-dims are guaranteed to be collapsed. So just use the
        -: 1272:    // iteration dimension of the collapsed op.
    #####: 1273:    resultExprs.push_back(
call    0 never executed
    #####: 1274:        getAffineDimExpr(origOpToCollapsedOpMapping[dim].first, context));
call    0 never executed
        -: 1275:  }
    #####: 1276:  return AffineMap::get(collapsingInfo.getCollapsedOpIterationRank(), 0,
call    0 never executed
    #####: 1277:                        resultExprs, context);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1278:}
        -: 1279:
        -: 1280:/// Return the `reassociation` indices to use to collapse the operand when the
        -: 1281:/// iteration space of a generic op is collapsed.
        -: 1282:static SmallVector<ReassociationIndices>
function _ZL23getOperandReassociationN4mlir9AffineMapERKN12_GLOBAL__N_114CollapsingInfoE called 0 returned 0% blocks executed 0%
    #####: 1283:getOperandReassociation(AffineMap indexingMap,
        -: 1284:                        const CollapsingInfo &collapsingInfo) {
    #####: 1285:  unsigned counter = 0;
    #####: 1286:  SmallVector<ReassociationIndices> operandReassociation;
    #####: 1287:  auto origOpToCollapsedOpMapping =
    #####: 1288:      collapsingInfo.getOrigOpToCollapsedOpMapping();
    #####: 1289:  auto collapsedOpToOrigOpMapping =
    #####: 1290:      collapsingInfo.getCollapsedOpToOrigOpMapping();
    #####: 1291:  while (counter < indexingMap.getNumResults()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1292:    unsigned dim =
    #####: 1293:        indexingMap.getResult(counter).cast<AffineDimExpr>().getPosition();
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1294:    if (origOpToCollapsedOpMapping[dim].second == 0) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -: 1295:      // This is the start of a collapsed dimensions of the iteration that
        -: 1296:      // is gauranteed to be preserved in the indexing map. The number of folded
        -: 1297:      // dims is obtained from the collapsed op to original op mapping.
    #####: 1298:      unsigned numFoldedDims =
    #####: 1299:          collapsedOpToOrigOpMapping[origOpToCollapsedOpMapping[dim].first]
branch  0 never executed
branch  1 never executed
    #####: 1300:              .size();
call    0 never executed
    #####: 1301:      auto range = llvm::seq<unsigned>(counter, counter + numFoldedDims);
call    0 never executed
    #####: 1302:      operandReassociation.emplace_back(range.begin(), range.end());
call    0 never executed
    #####: 1303:      counter += numFoldedDims;
        -: 1304:    }
        -: 1305:  }
    #####: 1306:  return operandReassociation;
        -: 1307:}
        -: 1308:
        -: 1309:/// Get the new value to use for a given `OpOperand` in the collapsed operation.
function _ZL21getCollapsedOpOperandN4mlir8LocationENS_6linalg9GenericOpEPNS_9OpOperandERKN12_GLOBAL__N_114CollapsingInfoERNS_9OpBuilderE called 0 returned 0% blocks executed 0%
    #####: 1310:static Value getCollapsedOpOperand(Location loc, GenericOp genericOp,
        -: 1311:                                   OpOperand *opOperand,
        -: 1312:                                   const CollapsingInfo &collapsingInfo,
        -: 1313:                                   OpBuilder &builder) {
    #####: 1314:  AffineMap indexingMap = genericOp.getMatchingIndexingMap(opOperand);
call    0 never executed
    #####: 1315:  SmallVector<ReassociationIndices> operandReassociation =
    #####: 1316:      getOperandReassociation(indexingMap, collapsingInfo);
call    0 never executed
call    1 never executed
        -: 1317:
        -: 1318:  // If the number of entries in the reassocation for the operand is same as the
        -: 1319:  // number of results of the indexing map, then nothing to do for this operand.
    #####: 1320:  Value operand = opOperand->get();
call    0 never executed
    #####: 1321:  if (operandReassociation.size() == indexingMap.getNumResults())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1322:    return operand;
        -: 1323:
        -: 1324:  // Insert a reshape to collapse the dimensions.
    #####: 1325:  auto reshapeOp = builder.create<tensor::CollapseShapeOp>(
    #####: 1326:      loc, operand, operandReassociation);
call    0 never executed
    #####: 1327:  return reshapeOp.getResult();
call    0 never executed
        -: 1328:}
        -: 1329:
        -: 1330:/// Modify the `linalg.index` operations in the original generic op, to its
        -: 1331:/// value in the collapsed operation.
function _Z31generateCollapsedIndexingRegionN4mlir8LocationEPNS_5BlockERKN12_GLOBAL__N_114CollapsingInfoENS_10ValueRangeERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1332:void generateCollapsedIndexingRegion(Location loc, Block *block,
        -: 1333:                                     const CollapsingInfo &collapsingInfo,
        -: 1334:                                     ValueRange loopRange,
        -: 1335:                                     PatternRewriter &rewriter) {
    #####: 1336:  OpBuilder::InsertionGuard g(rewriter);
call    0 never executed
    #####: 1337:  rewriter.setInsertionPointToStart(block);
call    0 never executed
        -: 1338:
        -: 1339:  // Collect all the original index ops.
    #####: 1340:  auto indexOps = llvm::to_vector(block->getOps<linalg::IndexOp>());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1341:
        -: 1342:  // For each folded dimension list resolve the original induction variable
        -: 1343:  // values in terms of the folded dimension induction variable.
        -: 1344:  //   i_{folded} = (i_0 * d1 + i1) * d2 + i2.
        -: 1345:  // can be inverted to
        -: 1346:  //   i2 = i_{folded} % d2
        -: 1347:  //   i1 = (i_{folded} / d2) % d1
        -: 1348:  //   i0 = i_{folded} / (d1 * d2)
    #####: 1349:  llvm::DenseMap<unsigned, Value> indexReplacementVals;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1350:  for (auto &foldedDims :
    #####: 1351:       enumerate(collapsingInfo.getCollapsedOpToOrigOpMapping())) {
branch  0 never executed
branch  1 never executed
    #####: 1352:    ReassociationIndicesRef foldedDimsRef(foldedDims.value());
call    0 never executed
    #####: 1353:    Value newIndexVal =
    #####: 1354:        rewriter.create<linalg::IndexOp>(loc, foldedDims.index());
call    0 never executed
call    1 never executed
    #####: 1355:    for (auto dim : llvm::reverse(foldedDimsRef.drop_front())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 1356:      indexReplacementVals[dim] =
call    0 never executed
    #####: 1357:          rewriter.create<arith::RemUIOp>(loc, newIndexVal, loopRange[dim]);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1358:      newIndexVal =
    #####: 1359:          rewriter.create<arith::DivUIOp>(loc, newIndexVal, loopRange[dim]);
call    0 never executed
call    1 never executed
        -: 1360:    }
    #####: 1361:    indexReplacementVals[foldedDims.value().front()] = newIndexVal;
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -: 1362:  }
        -: 1363:
    #####: 1364:  for (auto indexOp : indexOps) {
branch  0 never executed
branch  1 never executed
    #####: 1365:    auto dim = indexOp.getDim();
call    0 never executed
    #####: 1366:    rewriter.replaceOp(indexOp, indexReplacementVals[dim]);
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1367:  }
    #####: 1368:}
        -: 1369:
        -: 1370:/// Implementation of fusion with reshape operation by collapsing dimensions.
function _ZL30collapseGenericOpIterationDimsN4mlir6linalg9GenericOpEN4llvm8ArrayRefINS2_11SmallVectorIlLj2EEEEERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1371:static FailureOr<SmallVector<Value>> collapseGenericOpIterationDims(
        -: 1372:    GenericOp genericOp, ArrayRef<ReassociationIndices> foldedIterationDims,
        -: 1373:    PatternRewriter &rewriter) {
        -: 1374:  // Bail on trivial no-op cases.
    #####: 1375:  if (genericOp.getNumLoops() <= 1 || foldedIterationDims.empty() ||
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####: 1376:      llvm::all_of(foldedIterationDims, [](ReassociationIndicesRef foldedDims) {
branch  0 never executed
branch  1 never executed
        -: 1377:        return foldedDims.size() <= 1;
        -: 1378:      }))
    #####: 1379:    return failure();
        -: 1380:
    #####: 1381:  CollapsingInfo collapsingInfo;
call    0 never executed
    #####: 1382:  if (failed(collapsingInfo.initialize(genericOp.getNumLoops(),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1383:                                       foldedIterationDims))) {
    #####: 1384:    return rewriter.notifyMatchFailure(
    #####: 1385:        genericOp, "illegal to collapse specified dimensions");
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1386:  }
        -: 1387:
        -: 1388:  // Bail on non-canonical ranges.
    #####: 1389:  SmallVector<Range> loopRanges =
    #####: 1390:      cast<LinalgOp>(genericOp.getOperation())
call    0 never executed
    #####: 1391:          .createLoopRanges(rewriter, genericOp.getLoc());
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
function _ZZL30collapseGenericOpIterationDimsN4mlir6linalg9GenericOpEN4llvm8ArrayRefINS2_11SmallVectorIlLj2EEEEERNS_15PatternRewriterEENKUlNS_12OpFoldResultElE0_clES9_l.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1392:  auto opFoldIsConstantValue = [](OpFoldResult ofr, int64_t value) {
    #####: 1393:    if (auto attr = ofr.dyn_cast<Attribute>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1394:      return attr.cast<IntegerAttr>().getInt() == value;
call    0 never executed
call    1 never executed
    #####: 1395:    llvm::APInt actual;
call    0 never executed
    #####: 1396:    return matchPattern(ofr.get<Value>(), m_ConstantInt(&actual)) &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####: 1397:           actual.getSExtValue() == value;
call    0 never executed
        -: 1398:  };
function _ZZL30collapseGenericOpIterationDimsN4mlir6linalg9GenericOpEN4llvm8ArrayRefINS2_11SmallVectorIlLj2EEEEERNS_15PatternRewriterEENKUlNS_5RangeEE1_clES9_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1399:  if (!llvm::all_of(loopRanges, [&](Range range) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1400:        return opFoldIsConstantValue(range.offset, 0) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1401:               opFoldIsConstantValue(range.stride, 1);
call    0 never executed
        -: 1402:      })) {
    #####: 1403:    return rewriter.notifyMatchFailure(
        -: 1404:        genericOp,
    #####: 1405:        "expected all loop ranges to have zero start and unit stride");
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1406:  }
        -: 1407:
        -: 1408:  // Get the iterator types for the operand.
    #####: 1409:  SmallVector<StringRef> iteratorTypes = getCollapsedOpIteratorTypes(
    #####: 1410:      genericOp.getIteratorTypes().getValue(), collapsingInfo);
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -: 1411:
        -: 1412:  // Get the indexing maps.
    #####: 1413:  auto indexingMaps = llvm::to_vector(
    #####: 1414:      llvm::map_range(genericOp.getIndexingMapsArray(), [&](AffineMap map) {
call    0 never executed
call    1 never executed
        -: 1415:        return getCollapsedOpIndexingMap(map, collapsingInfo);
    #####: 1416:      }));
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -: 1417:
    #####: 1418:  Location loc = genericOp->getLoc();
call    0 never executed
        -: 1419:
        -: 1420:  // Get the input operands.
    #####: 1421:  auto inputOperands = llvm::to_vector(llvm::map_range(
branch  0 never executed
branch  1 never executed
    #####: 1422:      genericOp.getDpsInputOperands(), [&](OpOperand *opOperand) {
call    0 never executed
call    1 never executed
    #####: 1423:        return getCollapsedOpOperand(loc, genericOp, opOperand, collapsingInfo,
    #####: 1424:                                     rewriter);
call    0 never executed
    #####: 1425:      }));
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1426:
        -: 1427:  // Get the output operands and result types.
    #####: 1428:  SmallVector<Type> resultTypes;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1429:  SmallVector<Value> outputOperands;
branch  0 never executed
branch  1 never executed
    #####: 1430:  resultTypes.reserve(genericOp.getNumDpsInits());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1431:  outputOperands.reserve(genericOp.getNumDpsInits());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1432:  for (OpOperand *output : genericOp.getDpsInitOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1433:    Value newOutput =
    #####: 1434:        getCollapsedOpOperand(loc, genericOp, output, collapsingInfo, rewriter);
call    0 never executed
    #####: 1435:    outputOperands.push_back(newOutput);
call    0 never executed
    #####: 1436:    resultTypes.push_back(newOutput.getType());
call    0 never executed
        -: 1437:  }
        -: 1438:
        -: 1439:  // Create the generic op.
    #####: 1440:  auto collapsedGenericOp = rewriter.create<linalg::GenericOp>(
        -: 1441:      loc, resultTypes, inputOperands, outputOperands, indexingMaps,
    #####: 1442:      iteratorTypes, [](OpBuilder &builder, Location loc, ValueRange args) {});
call    0 never executed
    #####: 1443:  Block *origOpBlock = &genericOp->getRegion(0).front();
call    0 never executed
call    1 never executed
    #####: 1444:  Block *collapsedOpBlock = &collapsedGenericOp->getRegion(0).front();
call    0 never executed
call    1 never executed
    #####: 1445:  rewriter.mergeBlocks(origOpBlock, collapsedOpBlock,
call    0 never executed
    #####: 1446:                       collapsedOpBlock->getArguments());
call    0 never executed
        -: 1447:
    #####: 1448:  if (collapsedGenericOp.hasIndexSemantics()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -: 1449:    // Collect the loop range of the generic op.
    #####: 1450:    OpBuilder::InsertionGuard g(rewriter);
call    0 never executed
    #####: 1451:    rewriter.setInsertionPoint(collapsedGenericOp);
call    0 never executed
    #####: 1452:    SmallVector<Value> loopBound =
    #####: 1453:        llvm::to_vector(llvm::map_range(loopRanges, [&](Range range) {
    #####: 1454:          return getValueOrCreateConstantIndexOp(rewriter, loc, range.size);
call    0 never executed
    #####: 1455:        }));
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1456:    generateCollapsedIndexingRegion(loc,
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1457:                                    &collapsedGenericOp->getRegion(0).front(),
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1458:                                    collapsingInfo, loopBound, rewriter);
        -: 1459:  }
        -: 1460:
        -: 1461:  // Insert expanding reshape for the result to get back the original result
        -: 1462:  // type.
    #####: 1463:  SmallVector<Value> results;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1464:  for (const auto &originalResult : llvm::enumerate(genericOp->getResults())) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
    #####: 1465:    Value collapsedOpResult =
    #####: 1466:        collapsedGenericOp->getResult(originalResult.index());
branch  0 never executed
branch  1 never executed
    #####: 1467:    auto originalResultType =
call    0 never executed
    #####: 1468:        originalResult.value().getType().cast<ShapedType>();
call    0 never executed
    #####: 1469:    auto collapsedOpResultType = collapsedOpResult.getType().cast<ShapedType>();
call    0 never executed
    #####: 1470:    if (collapsedOpResultType.getRank() != originalResultType.getRank()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1471:      AffineMap indexingMap =
call    0 never executed
    #####: 1472:          genericOp.getIndexingMapMatchingResult(originalResult.value());
call    0 never executed
    #####: 1473:      SmallVector<ReassociationIndices> reassociation =
    #####: 1474:          getOperandReassociation(indexingMap, collapsingInfo);
call    0 never executed
call    1 never executed
    #####: 1475:      Value result = rewriter.create<tensor::ExpandShapeOp>(
    #####: 1476:          loc, originalResultType, collapsedOpResult, reassociation);
call    0 never executed
call    1 never executed
    #####: 1477:      results.push_back(result);
call    0 never executed
        -: 1478:    } else {
    #####: 1479:      results.push_back(collapsedOpResult);
call    0 never executed
        -: 1480:    }
        -: 1481:  }
    #####: 1482:  return results;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -: 1483:}
        -: 1484:
        -: 1485:namespace {
        -: 1486:
        -: 1487:/// Pattern to fuse a tensor.expand_shape op with its consumer generic op by
        -: 1488:/// contracting dimensions of the loop.
        -: 1489:class FoldWithProducerReshapeOpByCollapsing
        -: 1490:    : public OpRewritePattern<GenericOp> {
        -: 1491:public:
function _ZN12_GLOBAL__N_137FoldWithProducerReshapeOpByCollapsingC2EPN4mlir11MLIRContextESt8functionIFbPNS1_9OpOperandEEENS1_14PatternBenefitE called 0 returned 0% blocks executed 0%
    #####: 1492:  FoldWithProducerReshapeOpByCollapsing(MLIRContext *context,
        -: 1493:                                        ControlFusionFn foldReshapes,
        -: 1494:                                        PatternBenefit benefit = 1)
    #####: 1495:      : OpRewritePattern<GenericOp>(context, benefit),
    #####: 1496:        controlFoldingReshapes(std::move(foldReshapes)) {}
call    0 never executed
        -: 1497:
function _ZNK12_GLOBAL__N_137FoldWithProducerReshapeOpByCollapsing15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1498:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -: 1499:                                PatternRewriter &rewriter) const override {
    #####: 1500:    for (OpOperand &opOperand : genericOp->getOpOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1501:      tensor::ExpandShapeOp reshapeOp =
    #####: 1502:          opOperand.get().getDefiningOp<tensor::ExpandShapeOp>();
call    0 never executed
    #####: 1503:      if (!reshapeOp)
branch  0 never executed
branch  1 never executed
    #####: 1504:        continue;
        -: 1505:
    #####: 1506:      SmallVector<ReassociationIndices> collapsableIterationDims =
        -: 1507:          getCollapsableIterationSpaceDims(genericOp, &opOperand,
    #####: 1508:                                           reshapeOp.getReassociationIndices());
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1509:      if (collapsableIterationDims.empty() ||
branch  0 never executed
branch  1 never executed
    #####: 1510:          !controlFoldingReshapes(&opOperand)) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1511:        continue;
call    0 never executed
        -: 1512:      }
        -: 1513:
    #####: 1514:      Optional<SmallVector<Value>> replacements =
call    0 never executed
    #####: 1515:          collapseGenericOpIterationDims(genericOp, collapsableIterationDims,
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1516:                                         rewriter);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1517:      if (!replacements) {
branch  0 never executed
branch  1 never executed
    #####: 1518:        return rewriter.notifyMatchFailure(
    #####: 1519:            genericOp, "failed to do the fusion by collapsing transformation");
call    0 never executed
        -: 1520:      }
        -: 1521:
    #####: 1522:      rewriter.replaceOp(genericOp, *replacements);
call    0 never executed
call    1 never executed
    #####: 1523:      return success();
branch  0 never executed
branch  1 never executed
        -: 1524:    }
    #####: 1525:    return failure();
        -: 1526:  }
        -: 1527:
        -: 1528:private:
        -: 1529:  ControlFusionFn controlFoldingReshapes;
        -: 1530:};
        -: 1531:
        -: 1532:/// Pattern to collapse dimensions.
        -: 1533:class CollapseLinalgDimensions : public OpRewritePattern<GenericOp> {
        -: 1534:public:
function _ZN12_GLOBAL__N_124CollapseLinalgDimensionsC2EPN4mlir11MLIRContextESt8functionIFN4llvm11SmallVectorINS6_IlLj2EEELj1EEENS1_6linalg9GenericOpEEENS1_14PatternBenefitE called 0 returned 0% blocks executed 0%
    #####: 1535:  CollapseLinalgDimensions(MLIRContext *context,
        -: 1536:                           GetCollapsableDimensionsFn collapseDimensions,
        -: 1537:                           PatternBenefit benefit = 1)
    #####: 1538:      : OpRewritePattern<GenericOp>(context, benefit),
    #####: 1539:        controlCollapseDimension(std::move(collapseDimensions)) {}
call    0 never executed
        -: 1540:
function _ZNK12_GLOBAL__N_124CollapseLinalgDimensions15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1541:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -: 1542:                                PatternRewriter &rewriter) const override {
    #####: 1543:    SmallVector<ReassociationIndices> collapsableIterationDims =
    #####: 1544:        controlCollapseDimension(genericOp);
branch  0 never executed
branch  1 never executed
    #####: 1545:    if (collapsableIterationDims.empty())
branch  0 never executed
branch  1 never executed
    #####: 1546:      return failure();
        -: 1547:
    #####: 1548:    Optional<SmallVector<Value>> replacements = collapseGenericOpIterationDims(
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1549:        genericOp, collapsableIterationDims, rewriter);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####: 1550:    if (!replacements) {
branch  0 never executed
branch  1 never executed
    #####: 1551:      return rewriter.notifyMatchFailure(genericOp,
    #####: 1552:                                         "failed to collpase dimensions");
call    0 never executed
        -: 1553:    }
    #####: 1554:    rewriter.replaceOp(genericOp, *replacements);
call    0 never executed
call    1 never executed
    #####: 1555:    return success();
branch  0 never executed
branch  1 never executed
        -: 1556:  }
        -: 1557:
        -: 1558:private:
        -: 1559:  GetCollapsableDimensionsFn controlCollapseDimension;
        -: 1560:};
        -: 1561:
        -: 1562:} // namespace
        -: 1563:
        -: 1564://===---------------------------------------------------------------------===//
        -: 1565:// Methods and patterns that fuse constants with linalg.generic operations.
        -: 1566://===---------------------------------------------------------------------===//
        -: 1567:
        -: 1568:namespace {
        -: 1569:/// Pattern to fold a generic op with a splat constant/scalar constant. Does not
        -: 1570:/// handle cases where the constant is not single-valued.
        -: 1571:class FoldScalarOrSplatConstant : public OpRewritePattern<GenericOp> {
        -: 1572:public:
function _ZN12_GLOBAL__N_125FoldScalarOrSplatConstantC2EPN4mlir11MLIRContextENS1_14PatternBenefitE called 412 returned 100% blocks executed 100%
      412: 1573:  FoldScalarOrSplatConstant(MLIRContext *context, PatternBenefit benefit = 1)
      412: 1574:      : OpRewritePattern<GenericOp>(context, benefit) {}
call    0 returned 100%
        -: 1575:
function _ZNK12_GLOBAL__N_125FoldScalarOrSplatConstant15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1576:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -: 1577:                                PatternRewriter &rewriter) const override {
    #####: 1578:    if (!genericOp.hasTensorSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1579:      return failure();
    #####: 1580:    for (OpOperand *opOperand : genericOp.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1581:      Operation *def = opOperand->get().getDefiningOp();
call    0 never executed
    #####: 1582:      TypedAttr constantAttr;
call    0 never executed
function _ZZNK12_GLOBAL__N_125FoldScalarOrSplatConstant15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterEENKUlPNS1_9OperationEE_clES7_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1583:      auto isScalarOrSplatConstantOp = [&constantAttr](Operation *def) -> bool {
    #####: 1584:        {
    #####: 1585:          DenseElementsAttr splatAttr;
    #####: 1586:          if (matchPattern(def, m_Constant<DenseElementsAttr>(&splatAttr)) &&
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1587:              splatAttr.isSplat() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1588:              splatAttr.getType().getElementType().isIntOrFloat()) {
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1589:            constantAttr = splatAttr.getSplatValue<TypedAttr>();
call    0 never executed
    #####: 1590:            return true;
        -: 1591:          }
        -: 1592:        }
    #####: 1593:        {
    #####: 1594:          IntegerAttr intAttr;
    #####: 1595:          if (matchPattern(def, m_Constant<IntegerAttr>(&intAttr))) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1596:            constantAttr = intAttr;
call    0 never executed
    #####: 1597:            return true;
        -: 1598:          }
        -: 1599:        }
    #####: 1600:        {
    #####: 1601:          FloatAttr floatAttr;
    #####: 1602:          if (matchPattern(def, m_Constant<FloatAttr>(&floatAttr))) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1603:            constantAttr = floatAttr;
call    0 never executed
    #####: 1604:            return true;
        -: 1605:          }
        -: 1606:        }
    #####: 1607:        return false;
    #####: 1608:      };
        -: 1609:
    #####: 1610:      auto resultValue = opOperand->get().dyn_cast<OpResult>();
call    0 never executed
    #####: 1611:      if (!def || !resultValue || !isScalarOrSplatConstantOp(def))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####: 1612:        continue;
        -: 1613:
        -: 1614:      // The operands and the indexing_maps of the fused operation the same as
        -: 1615:      // the operands and indexing_maps of the generic operations with the
        -: 1616:      // values at the constant index dropped.
    #####: 1617:      SmallVector<AffineMap> fusedIndexMaps;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1618:      SmallVector<Value> fusedOperands;
branch  0 never executed
branch  1 never executed
    #####: 1619:      SmallVector<Location> fusedLocs{genericOp.getLoc()};
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1620:      fusedIndexMaps.reserve(genericOp->getNumOperands());
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1621:      fusedOperands.reserve(genericOp.getNumDpsInputs());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1622:      fusedLocs.reserve(fusedLocs.size() + genericOp.getNumDpsInputs());
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1623:      for (OpOperand *inputOperand : genericOp.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1624:        if (inputOperand == opOperand)
branch  0 never executed
branch  1 never executed
    #####: 1625:          continue;
    #####: 1626:        Value inputValue = inputOperand->get();
call    0 never executed
    #####: 1627:        fusedIndexMaps.push_back(
call    0 never executed
call    1 never executed
        -: 1628:            genericOp.getMatchingIndexingMap(inputOperand));
    #####: 1629:        fusedOperands.push_back(inputValue);
call    0 never executed
    #####: 1630:        fusedLocs.push_back(inputValue.getLoc());
call    0 never executed
call    1 never executed
        -: 1631:      }
    #####: 1632:      for (OpOperand *outputOperand : genericOp.getDpsInitOperands())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1633:        fusedIndexMaps.push_back(
call    0 never executed
call    1 never executed
        -: 1634:            genericOp.getMatchingIndexingMap(outputOperand));
        -: 1635:
        -: 1636:      // Check if the operation shapes to loops map is computable.
    #####: 1637:      if (!inversePermutation(concatAffineMaps(fusedIndexMaps))) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1638:        return rewriter.notifyMatchFailure(
    #####: 1639:            genericOp, "fused op loop bound computation failed");
call    0 never executed
        -: 1640:      }
        -: 1641:
        -: 1642:      // Create a constant scalar value from the splat constant.
    #####: 1643:      Value scalarConstant = rewriter.create<arith::ConstantOp>(
    #####: 1644:          def->getLoc(), constantAttr, constantAttr.getType());
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1645:
    #####: 1646:      SmallVector<Value> outputOperands = genericOp.getOutputs();
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1647:      auto fusedOp = rewriter.create<GenericOp>(
    #####: 1648:          rewriter.getFusedLoc(fusedLocs), genericOp->getResultTypes(),
branch  0 never executed
branch  1 never executed
        -: 1649:          /*inputs=*/fusedOperands,
        -: 1650:          /*outputs=*/outputOperands,
    #####: 1651:          rewriter.getAffineMapArrayAttr(fusedIndexMaps),
call    0 never executed
    #####: 1652:          genericOp.getIteratorTypes(),
    #####: 1653:          /*doc=*/nullptr,
    #####: 1654:          /*library_call=*/nullptr);
call    0 never executed
call    1 never executed
call    2 never executed
        -: 1655:
        -: 1656:      // Map the block argument corresponding to the replaced argument with the
        -: 1657:      // scalar constant.
    #####: 1658:      Region &region = genericOp->getRegion(0);
call    0 never executed
    #####: 1659:      Block &entryBlock = *region.begin();
branch  0 never executed
branch  1 never executed
    #####: 1660:      BlockAndValueMapping mapping;
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####: 1661:      mapping.map(entryBlock.getArgument(opOperand->getOperandNumber()),
call    0 never executed
call    1 never executed
        -: 1662:                  scalarConstant);
    #####: 1663:      Region &fusedRegion = fusedOp->getRegion(0);
call    0 never executed
    #####: 1664:      rewriter.cloneRegionBefore(region, fusedRegion, fusedRegion.begin(),
    #####: 1665:                                 mapping);
call    0 never executed
    #####: 1666:      rewriter.replaceOp(genericOp, fusedOp->getResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####: 1667:      return success();
call    0 never executed
        -: 1668:    }
    #####: 1669:    return failure();
        -: 1670:  }
        -: 1671:};
        -: 1672:
        -: 1673:} // namespace
        -: 1674:
        -: 1675://===---------------------------------------------------------------------===//
        -: 1676:// Miscellaneous patterns that help fusion.
        -: 1677://===---------------------------------------------------------------------===//
        -: 1678:
        -: 1679:namespace {
        -: 1680:/// Forces `outs` operands of linalg operations to use `tensor.empty` if the
        -: 1681:/// value of the `outs` operand is not used within the op.  This is only
        -: 1682:/// implemented for `linalg.generic` operations for now, but should hold for all
        -: 1683:/// linalg structured ops.
        -: 1684:struct RemoveOutsDependency : public OpRewritePattern<GenericOp> {
        -: 1685:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -: 1686:
function _ZNK12_GLOBAL__N_120RemoveOutsDependency15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1687:  LogicalResult matchAndRewrite(GenericOp op,
        -: 1688:                                PatternRewriter &rewriter) const override {
    #####: 1689:    rewriter.startRootUpdate(op);
call    0 never executed
    #####: 1690:    bool modifiedOutput = false;
    #####: 1691:    Location loc = op.getLoc();
call    0 never executed
    #####: 1692:    for (OpOperand *opOperand : op.getDpsInitOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1693:      if (!op.payloadUsesValueFromOperand(opOperand)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1694:        Value operandVal = opOperand->get();
call    0 never executed
    #####: 1695:        auto operandType = operandVal.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####: 1696:        if (!operandType)
branch  0 never executed
branch  1 never executed
    #####: 1697:          continue;
        -: 1698:
        -: 1699:        // If outs is sparse, leave it to the sparse compiler.
    #####: 1700:        if (sparse_tensor::getSparseTensorEncoding(operandVal.getType()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1701:          continue;
        -: 1702:
        -: 1703:        // If outs is already an `empty` operation, nothing to do.
    #####: 1704:        auto definingOp = operandVal.getDefiningOp<tensor::EmptyOp>();
call    0 never executed
    #####: 1705:        if (definingOp)
branch  0 never executed
branch  1 never executed
    #####: 1706:          continue;
    #####: 1707:        modifiedOutput = true;
    #####: 1708:        SmallVector<Value> dynamicDims;
call    0 never executed
    #####: 1709:        for (const auto &dim : llvm::enumerate(operandType.getShape())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####: 1710:          if (dim.value() != ShapedType::kDynamicSize)
branch  0 never executed
branch  1 never executed
    #####: 1711:            continue;
    #####: 1712:          dynamicDims.push_back(rewriter.createOrFold<tensor::DimOp>(
call    0 never executed
    #####: 1713:              loc, operandVal, dim.index()));
call    0 never executed
        -: 1714:        }
    #####: 1715:        Value emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####: 1716:            loc, operandType.getShape(), operandType.getElementType(),
call    0 never executed
    #####: 1717:            dynamicDims);
call    0 never executed
call    1 never executed
call    2 never executed
    #####: 1718:        op->setOperand(opOperand->getOperandNumber(), emptyTensor);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -: 1719:      }
        -: 1720:    }
    #####: 1721:    if (!modifiedOutput) {
branch  0 never executed
branch  1 never executed
    #####: 1722:      rewriter.cancelRootUpdate(op);
call    0 never executed
    #####: 1723:      return failure();
        -: 1724:    }
    #####: 1725:    rewriter.finalizeRootUpdate(op);
call    0 never executed
    #####: 1726:    return success();
        -: 1727:  }
        -: 1728:};
        -: 1729:
        -: 1730:/// Fold linalg.fill into linalg.generic
        -: 1731:struct FoldFillWithGenericOp : public OpRewritePattern<GenericOp> {
        -: 1732:  using OpRewritePattern<GenericOp>::OpRewritePattern;
        -: 1733:
function _ZNK12_GLOBAL__N_121FoldFillWithGenericOp15matchAndRewriteEN4mlir6linalg9GenericOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####: 1734:  LogicalResult matchAndRewrite(GenericOp genericOp,
        -: 1735:                                PatternRewriter &rewriter) const override {
    #####: 1736:    if (!genericOp.hasTensorSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1737:      return failure();
    #####: 1738:    bool fillFound = false;
    #####: 1739:    Block &payload = genericOp.getRegion().front();
call    0 never executed
call    1 never executed
    #####: 1740:    for (OpOperand *opOperand : genericOp.getDpsInputOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1741:      if (!genericOp.payloadUsesValueFromOperand(opOperand))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####: 1742:        continue;
    #####: 1743:      FillOp fillOp = opOperand->get().getDefiningOp<FillOp>();
call    0 never executed
    #####: 1744:      if (!fillOp)
branch  0 never executed
branch  1 never executed
    #####: 1745:        continue;
    #####: 1746:      fillFound = true;
    #####: 1747:      payload.getArgument(opOperand->getOperandNumber())
call    0 never executed
call    1 never executed
    #####: 1748:          .replaceAllUsesWith(fillOp.value());
call    0 never executed
        -: 1749:    }
    #####: 1750:    return success(fillFound);
        -: 1751:  }
        -: 1752:};
        -: 1753:} // namespace
        -: 1754:
function _ZN4mlir6linalg41populateFoldReshapeOpsByExpansionPatternsERNS_17RewritePatternSetERKSt8functionIFbPNS_9OpOperandEEE called 412 returned 100% blocks executed 100%
      412: 1755:void mlir::linalg::populateFoldReshapeOpsByExpansionPatterns(
        -: 1756:    RewritePatternSet &patterns,
        -: 1757:    const ControlFusionFn &controlFoldingReshapes) {
      412: 1758:  patterns.add<FoldReshapeWithGenericOpByExpansion>(patterns.getContext(),
call    0 returned 100%
      412: 1759:                                                    controlFoldingReshapes);
call    0 returned 100%
      412: 1760:  patterns.add<FoldWithProducerReshapeOpByExpansion>(patterns.getContext(),
call    0 returned 100%
      412: 1761:                                                     controlFoldingReshapes);
call    0 returned 100%
      412: 1762:}
        -: 1763:
function _ZN4mlir6linalg42populateFoldReshapeOpsByCollapsingPatternsERNS_17RewritePatternSetERKSt8functionIFbPNS_9OpOperandEEE called 0 returned 0% blocks executed 0%
    #####: 1764:void mlir::linalg::populateFoldReshapeOpsByCollapsingPatterns(
        -: 1765:    RewritePatternSet &patterns,
        -: 1766:    const ControlFusionFn &controlFoldingReshapes) {
    #####: 1767:  patterns.add<FoldWithProducerReshapeOpByCollapsing>(patterns.getContext(),
call    0 never executed
    #####: 1768:                                                      controlFoldingReshapes);
call    0 never executed
    #####: 1769:}
        -: 1770:
function _ZN4mlir6linalg36populateElementwiseOpsFusionPatternsERNS_17RewritePatternSetERKSt8functionIFbPNS_9OpOperandEEE called 412 returned 100% blocks executed 100%
      412: 1771:void mlir::linalg::populateElementwiseOpsFusionPatterns(
        -: 1772:    RewritePatternSet &patterns,
        -: 1773:    const ControlFusionFn &controlElementwiseOpsFusion) {
      412: 1774:  auto *context = patterns.getContext();
call    0 returned 100%
      412: 1775:  patterns.add<FuseElementwiseOps>(context, controlElementwiseOpsFusion);
call    0 returned 100%
      412: 1776:  patterns.add<FoldFillWithGenericOp, FoldScalarOrSplatConstant,
      412: 1777:               RemoveOutsDependency>(context);
call    0 returned 100%
      412: 1778:}
        -: 1779:
function _ZN4mlir6linalg26populateCollapseDimensionsERNS_17RewritePatternSetERKSt8functionIFN4llvm11SmallVectorINS5_IlLj2EEELj1EEENS0_9GenericOpEEE called 0 returned 0% blocks executed 0%
    #####: 1780:void mlir::linalg::populateCollapseDimensions(
        -: 1781:    RewritePatternSet &patterns,
        -: 1782:    const GetCollapsableDimensionsFn &controlCollapseDimensions) {
    #####: 1783:  patterns.add<CollapseLinalgDimensions>(patterns.getContext(),
call    0 never executed
    #####: 1784:                                         controlCollapseDimensions);
call    0 never executed
    #####: 1785:}
        -: 1786:
        -: 1787://===---------------------------------------------------------------------===//
        -: 1788:// Passes
        -: 1789://===---------------------------------------------------------------------===//
        -: 1790:
        -: 1791:namespace {
        -: 1792:
        -: 1793:/// Pass that fuses generic ops on tensors. Used only for testing.
        -: 1794:// TODO(ravishankarm): This pass is to be deprecated. The efficacy of the
        -: 1795:// patterns added here heavily depends on the cost function used. Having an
        -: 1796:// opinionated pass of this form is not recommended. Deprecate this pass in
        -: 1797:// favor of test passes that check the functionality of each of the patterns
        -: 1798:// added here individually.
  116719*: 1799:struct LinalgElementwiseOpFusionPass
call    0 never executed
call    1 returned 100%
        -: 1800:    : public impl::LinalgElementwiseOpFusionBase<
        -: 1801:          LinalgElementwiseOpFusionPass> {
function _ZN12_GLOBAL__N_129LinalgElementwiseOpFusionPass14runOnOperationEv called 412 returned 100% blocks executed 95%
      412: 1802:  void runOnOperation() override {
      412: 1803:    Operation *op = getOperation();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
      412: 1804:    MLIRContext *context = op->getContext();
call    0 returned 100%
      412: 1805:    RewritePatternSet patterns(context);
call    0 returned 100%
        -: 1806:
        -: 1807:    // Add folding with reshape by expansion patterns.
function _ZZN12_GLOBAL__N_129LinalgElementwiseOpFusionPass14runOnOperationEvENKUlPN4mlir9OpOperandEE_clES3_.isra.0 called 0 returned 0% blocks executed 0%
    #####: 1808:    ControlFusionFn defaultControlFn = [](OpOperand *fusedOperand) {
    #####: 1809:      Operation *producer = fusedOperand->get().getDefiningOp();
call    0 never executed
    #####: 1810:      return producer && producer->hasOneUse();
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
      824: 1811:    };
call    0 returned 100%
call    1 returned 100%
        -: 1812:
        -: 1813:    // Add elementwise op fusion patterns.
      412: 1814:    populateElementwiseOpsFusionPatterns(patterns, defaultControlFn);
call    0 returned 100%
      412: 1815:    populateFoldReshapeOpsByExpansionPatterns(patterns, defaultControlFn);
call    0 returned 100%
        -: 1816:
        -: 1817:    // General canonicalization patterns.
      412: 1818:    AffineApplyOp::getCanonicalizationPatterns(patterns, context);
call    0 returned 100%
      412: 1819:    GenericOp::getCanonicalizationPatterns(patterns, context);
call    0 returned 100%
      412: 1820:    tensor::ExpandShapeOp::getCanonicalizationPatterns(patterns, context);
call    0 returned 100%
      412: 1821:    tensor::CollapseShapeOp::getCanonicalizationPatterns(patterns, context);
call    0 returned 100%
      412: 1822:    context->getLoadedDialect<LinalgDialect>()->getCanonicalizationPatterns(
call    0 returned 100%
      412: 1823:        patterns);
call    0 returned 100%
        -: 1824:
        -: 1825:    // Add constant folding patterns.
      412: 1826:    populateConstantFoldLinalgOperations(patterns, defaultControlFn);
call    0 returned 100%
        -: 1827:
        -: 1828:    // Use TopDownTraversal for compile time reasons
      412: 1829:    GreedyRewriteConfig grc;
      412: 1830:    grc.useTopDownTraversal = true;
      412: 1831:    (void)applyPatternsAndFoldGreedily(op->getRegions(), std::move(patterns),
call    0 returned 100%
      824: 1832:                                       grc);
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
call    4 returned 100%
branch  5 taken 100% (fallthrough)
branch  6 taken 0%
      412: 1833:  }
        -: 1834:};
        -: 1835:
        -: 1836:} // namespace
        -: 1837:
function _ZN4mlir35createLinalgElementwiseOpFusionPassEv called 116719 returned 100% blocks executed 100%
   116719: 1838:std::unique_ptr<Pass> mlir::createLinalgElementwiseOpFusionPass() {
   116719: 1839:  return std::make_unique<LinalgElementwiseOpFusionPass>();
call    0 returned 100%
        -: 1840:}
