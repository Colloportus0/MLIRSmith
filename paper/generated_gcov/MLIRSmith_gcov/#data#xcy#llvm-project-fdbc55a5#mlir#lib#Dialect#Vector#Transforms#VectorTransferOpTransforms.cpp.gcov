        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Dialect/Vector/Transforms/VectorTransferOpTransforms.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Vector/Transforms/CMakeFiles/obj.MLIRVectorTransforms.dir/VectorTransferOpTransforms.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Vector/Transforms/CMakeFiles/obj.MLIRVectorTransforms.dir/VectorTransferOpTransforms.cpp.gcda
        -:    0:Runs:116175
        -:    1://===- VectorTransferOpTransforms.cpp - transfer op transforms ------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements functions concerned with optimizing transfer_read and
        -:   10:// transfer_write ops.
        -:   11://
        -:   12://===----------------------------------------------------------------------===//
        -:   13:
        -:   14:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   15:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   16:#include "mlir/Dialect/Vector/IR/VectorOps.h"
        -:   17:#include "mlir/Dialect/Vector/Transforms/VectorTransforms.h"
        -:   18:#include "mlir/Dialect/Vector/Utils/VectorUtils.h"
        -:   19:#include "mlir/IR/BuiltinOps.h"
        -:   20:#include "mlir/IR/Dominance.h"
        -:   21:#include "mlir/Transforms/SideEffectUtils.h"
        -:   22:#include "llvm/ADT/STLExtras.h"
        -:   23:#include "llvm/ADT/StringRef.h"
        -:   24:#include "llvm/Support/Debug.h"
        -:   25:
        -:   26:#define DEBUG_TYPE "vector-transfer-opt"
        -:   27:
        -:   28:#define DBGS() (llvm::dbgs() << '[' << DEBUG_TYPE << "] ")
        -:   29:
        -:   30:using namespace mlir;
        -:   31:
        -:   32:/// Return the ancestor op in the region or nullptr if the region is not
        -:   33:/// an ancestor of the op.
function _ZL22findAncestorOpInRegionPN4mlir6RegionEPNS_9OperationE called 0 returned 0% blocks executed 0%
    #####:   34:static Operation *findAncestorOpInRegion(Region *region, Operation *op) {
    #####:   35:  for (; op != nullptr && op->getParentRegion() != region;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:   36:       op = op->getParentOp())
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:   37:    ;
    #####:   38:  return op;
        -:   39:}
        -:   40:
        -:   41:namespace {
        -:   42:
        -:   43:class TransferOptimization {
        -:   44:public:
        -:   45:  TransferOptimization(Operation *op) : dominators(op), postDominators(op) {}
        -:   46:  void deadStoreOp(vector::TransferWriteOp);
        -:   47:  void storeToLoadForwarding(vector::TransferReadOp);
function _ZN12_GLOBAL__N_120TransferOptimization12removeDeadOpEv called 784 returned 100% blocks executed 57%
      784:   48:  void removeDeadOp() {
     784*:   49:    for (Operation *op : opToErase)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
    #####:   50:      op->erase();
call    0 never executed
      784:   51:    opToErase.clear();
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
      784:   52:  }
        -:   53:
        -:   54:private:
        -:   55:  bool isReachable(Operation *start, Operation *dest);
        -:   56:  DominanceInfo dominators;
        -:   57:  PostDominanceInfo postDominators;
        -:   58:  std::vector<Operation *> opToErase;
        -:   59:};
        -:   60:
        -:   61:/// Return true if there is a path from start operation to dest operation,
        -:   62:/// otherwise return false. The operations have to be in the same region.
function _ZN12_GLOBAL__N_120TransferOptimization11isReachableEPN4mlir9OperationES3_ called 0 returned 0% blocks executed 0%
    #####:   63:bool TransferOptimization::isReachable(Operation *start, Operation *dest) {
    #####:   64:  assert(start->getParentRegion() == dest->getParentRegion() &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
call    6 never executed
        -:   65:         "This function only works for ops i the same region");
        -:   66:  // Simple case where the start op dominate the destination.
    #####:   67:  if (dominators.dominates(start, dest))
branch  0 never executed
branch  1 never executed
    #####:   68:    return true;
    #####:   69:  Block *startBlock = start->getBlock();
call    0 never executed
    #####:   70:  Block *destBlock = dest->getBlock();
    #####:   71:  SmallVector<Block *, 32> worklist(startBlock->succ_begin(),
call    0 never executed
    #####:   72:                                    startBlock->succ_end());
call    0 never executed
call    1 never executed
    #####:   73:  SmallPtrSet<Block *, 32> visited;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   74:  while (!worklist.empty()) {
branch  0 never executed
branch  1 never executed
    #####:   75:    Block *bb = worklist.pop_back_val();
call    0 never executed
    #####:   76:    if (!visited.insert(bb).second)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   77:      continue;
    #####:   78:    if (dominators.dominates(bb, destBlock))
branch  0 never executed
branch  1 never executed
    #####:   79:      return true;
    #####:   80:    worklist.append(bb->succ_begin(), bb->succ_end());
call    0 never executed
call    1 never executed
call    2 never executed
        -:   81:  }
        -:   82:  return false;
        -:   83:}
        -:   84:
        -:   85:/// For transfer_write to overwrite fully another transfer_write must:
        -:   86:/// 1. Access the same memref with the same indices and vector type.
        -:   87:/// 2. Post-dominate the other transfer_write operation.
        -:   88:/// If several candidates are available, one must be post-dominated by all the
        -:   89:/// others since they are all post-dominating the same transfer_write. We only
        -:   90:/// consider the transfer_write post-dominated by all the other candidates as
        -:   91:/// this will be the first transfer_write executed after the potentially dead
        -:   92:/// transfer_write.
        -:   93:/// If we found such an overwriting transfer_write we know that the original
        -:   94:/// transfer_write is dead if all reads that can be reached from the potentially
        -:   95:/// dead transfer_write are dominated by the overwriting transfer_write.
function _ZN12_GLOBAL__N_120TransferOptimization11deadStoreOpEN4mlir6vector15TransferWriteOpE called 1489 returned 100% blocks executed 38%
     1489:   96:void TransferOptimization::deadStoreOp(vector::TransferWriteOp write) {
    1489*:   97:  LLVM_DEBUG(DBGS() << "Candidate for dead store: " << *write.getOperation()
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
call   13 never executed
        -:   98:                    << "\n");
    1489*:   99:  llvm::SmallVector<Operation *, 8> blockingAccesses;
call    0 returned 100%
     1489:  100:  Operation *firstOverwriteCandidate = nullptr;
     1489:  101:  Value source = write.getSource();
call    0 returned 100%
        -:  102:  // Skip subview ops.
    1489*:  103:  while (auto subView = source.getDefiningOp<memref::SubViewOp>())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  104:    source = subView.getSource();
call    0 never executed
     1489:  105:  llvm::SmallVector<Operation *, 32> users(source.getUsers().begin(),
call    0 returned 100%
    1489*:  106:                                           source.getUsers().end());
call    0 returned 100%
branch  1 never executed
branch  2 never executed
    1489*:  107:  llvm::SmallDenseSet<Operation *, 32> processed;
call    0 returned 100%
branch  1 never executed
branch  2 never executed
    18917:  108:  while (!users.empty()) {
branch  0 taken 92% (fallthrough)
branch  1 taken 8%
    17428:  109:    Operation *user = users.pop_back_val();
call    0 returned 100%
        -:  110:    // If the user has already been processed skip.
    17428:  111:    if (!processed.insert(user).second)
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 100%
     2561:  112:      continue;
   17357*:  113:    if (auto subView = dyn_cast<memref::SubViewOp>(user)) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  114:      users.append(subView->getUsers().begin(), subView->getUsers().end());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  115:      continue;
        -:  116:    }
    17357:  117:    if (isMemoryEffectFree(user))
call    0 returned 100%
branch  1 taken 5% (fallthrough)
branch  2 taken 95%
      870:  118:      continue;
    16487:  119:    if (user == write.getOperation())
branch  0 taken 9% (fallthrough)
branch  1 taken 91%
     1489:  120:      continue;
    14998:  121:    if (auto nextWrite = dyn_cast<vector::TransferWriteOp>(user)) {
call    0 returned 100%
branch  1 taken 3% (fallthrough)
branch  2 taken 97%
        -:  122:      // Check candidate that can override the store.
     832*:  123:      if (write.getSource() == nextWrite.getSource() &&
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
branch  4 taken 0% (fallthrough)
branch  5 taken 100%
      416:  124:          checkSameValueWAW(nextWrite, write) &&
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
     416*:  125:          postDominators.postDominates(nextWrite, write)) {
branch  0 never executed
branch  1 never executed
    #####:  126:        if (firstOverwriteCandidate == nullptr ||
branch  0 never executed
branch  1 never executed
    #####:  127:            postDominators.postDominates(firstOverwriteCandidate, nextWrite))
branch  0 never executed
branch  1 never executed
    #####:  128:          firstOverwriteCandidate = nextWrite;
        -:  129:        else
    #####:  130:          assert(
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  131:              postDominators.postDominates(nextWrite, firstOverwriteCandidate));
    #####:  132:        continue;
        -:  133:      }
        -:  134:    }
    14998:  135:    if (auto transferOp = dyn_cast<VectorTransferOpInterface>(user)) {
call    0 returned 100%
branch  1 taken 5% (fallthrough)
branch  2 taken 95%
        -:  136:      // Don't need to consider disjoint accesses.
      752:  137:      if (vector::isDisjointTransferSet(
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 17% (fallthrough)
branch  4 taken 83%
        -:  138:              cast<VectorTransferOpInterface>(write.getOperation()),
        -:  139:              cast<VectorTransferOpInterface>(transferOp.getOperation())))
      131:  140:        continue;
        -:  141:    }
    14867:  142:    blockingAccesses.push_back(user);
call    0 returned 100%
        -:  143:  }
     1489:  144:  if (firstOverwriteCandidate == nullptr)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
     1489:  145:    return;
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
    #####:  146:  Region *topRegion = firstOverwriteCandidate->getParentRegion();
branch  0 never executed
branch  1 never executed
    #####:  147:  Operation *writeAncestor = findAncestorOpInRegion(topRegion, write);
call    0 never executed
    #####:  148:  assert(writeAncestor &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  149:         "write op should be recursively part of the top region");
        -:  150:
    #####:  151:  for (Operation *access : blockingAccesses) {
branch  0 never executed
branch  1 never executed
    #####:  152:    Operation *accessAncestor = findAncestorOpInRegion(topRegion, access);
call    0 never executed
        -:  153:    // TODO: if the access and write have the same ancestor we could recurse in
        -:  154:    // the region to know if the access is reachable with more precision.
    #####:  155:    if (accessAncestor == nullptr ||
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  156:        !isReachable(writeAncestor, accessAncestor))
call    0 never executed
    #####:  157:      continue;
    #####:  158:    if (!dominators.dominates(firstOverwriteCandidate, accessAncestor)) {
branch  0 never executed
branch  1 never executed
    #####:  159:      LLVM_DEBUG(DBGS() << "Store may not be dead due to op: "
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
        -:  160:                        << *accessAncestor << "\n");
    #####:  161:      return;
        -:  162:    }
        -:  163:  }
    #####:  164:  LLVM_DEBUG(DBGS() << "Found dead store: " << *write.getOperation()
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
call   13 never executed
call   14 never executed
        -:  165:                    << " overwritten by: " << *firstOverwriteCandidate << "\n");
    #####:  166:  opToErase.push_back(write.getOperation());
call    0 never executed
call    1 never executed
        -:  167:}
        -:  168:
        -:  169:/// A transfer_write candidate to storeToLoad forwarding must:
        -:  170:/// 1. Access the same memref with the same indices and vector type as the
        -:  171:/// transfer_read.
        -:  172:/// 2. Dominate the transfer_read operation.
        -:  173:/// If several candidates are available, one must be dominated by all the others
        -:  174:/// since they are all dominating the same transfer_read. We only consider the
        -:  175:/// transfer_write dominated by all the other candidates as this will be the
        -:  176:/// last transfer_write executed before the transfer_read.
        -:  177:/// If we found such a candidate we can do the forwarding if all the other
        -:  178:/// potentially aliasing ops that may reach the transfer_read are post-dominated
        -:  179:/// by the transfer_write.
function _ZN12_GLOBAL__N_120TransferOptimization21storeToLoadForwardingEN4mlir6vector14TransferReadOpE called 1750 returned 100% blocks executed 40%
     1750:  180:void TransferOptimization::storeToLoadForwarding(vector::TransferReadOp read) {
     1750:  181:  if (read.hasOutOfBoundsDim())
call    0 returned 100%
branch  1 taken 71% (fallthrough)
branch  2 taken 29%
     1750:  182:    return;
    1237*:  183:  LLVM_DEBUG(DBGS() << "Candidate for Forwarding: " << *read.getOperation()
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
        -:  184:                    << "\n");
    1237*:  185:  SmallVector<Operation *, 8> blockingWrites;
call    0 returned 100%
     1237:  186:  vector::TransferWriteOp lastwrite = nullptr;
call    0 returned 100%
     1237:  187:  Value source = read.getSource();
call    0 returned 100%
        -:  188:  // Skip subview ops.
    1237*:  189:  while (auto subView = source.getDefiningOp<memref::SubViewOp>())
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  190:    source = subView.getSource();
call    0 never executed
     1237:  191:  llvm::SmallVector<Operation *, 32> users(source.getUsers().begin(),
call    0 returned 100%
    1237*:  192:                                           source.getUsers().end());
call    0 returned 100%
branch  1 never executed
branch  2 never executed
    1237*:  193:  llvm::SmallDenseSet<Operation *, 32> processed;
call    0 returned 100%
branch  1 never executed
branch  2 never executed
    15764:  194:  while (!users.empty()) {
branch  0 taken 92% (fallthrough)
branch  1 taken 8%
    14527:  195:    Operation *user = users.pop_back_val();
call    0 returned 100%
        -:  196:    // If the user has already been processed skip.
    14527:  197:    if (!processed.insert(user).second)
call    0 returned 100%
branch  1 taken 1% (fallthrough)
branch  2 taken 99%
     2653:  198:      continue;
   14451*:  199:    if (auto subView = dyn_cast<memref::SubViewOp>(user)) {
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  200:      users.append(subView->getUsers().begin(), subView->getUsers().end());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  201:      continue;
        -:  202:    }
    14451:  203:    if (isMemoryEffectFree(user) || isa<vector::TransferReadOp>(user))
call    0 returned 100%
branch  1 taken 95% (fallthrough)
branch  2 taken 5%
call    3 returned 100%
branch  4 taken 87% (fallthrough)
branch  5 taken 13%
     2486:  204:      continue;
    11965:  205:    if (auto write = dyn_cast<vector::TransferWriteOp>(user)) {
call    0 returned 100%
branch  1 taken 2%
branch  2 taken 98%
        -:  206:      // If there is a write, but we can prove that it is disjoint we can ignore
        -:  207:      // the write.
      284:  208:      if (vector::isDisjointTransferSet(
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
branch  3 taken 32% (fallthrough)
branch  4 taken 68%
        -:  209:              cast<VectorTransferOpInterface>(write.getOperation()),
        -:  210:              cast<VectorTransferOpInterface>(read.getOperation())))
       91:  211:        continue;
     193*:  212:      if (write.getSource() == read.getSource() &&
call    0 returned 100%
call    1 returned 100%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
      386:  213:          dominators.dominates(write, read) && checkSameValueRAW(write, read)) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
call    4 returned 100%
branch  5 taken 100% (fallthrough)
branch  6 taken 0%
    #####:  214:        if (lastwrite == nullptr || dominators.dominates(lastwrite, write))
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  215:          lastwrite = write;
        -:  216:        else
    #####:  217:          assert(dominators.dominates(write, lastwrite));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  218:        continue;
        -:  219:      }
        -:  220:    }
    11874:  221:    blockingWrites.push_back(user);
call    0 returned 100%
        -:  222:  }
        -:  223:
     1237:  224:  if (lastwrite == nullptr)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
     1237:  225:    return;
branch  0 taken 1% (fallthrough)
branch  1 taken 100%
        -:  226:
    #####:  227:  Region *topRegion = lastwrite->getParentRegion();
branch  0 never executed
branch  1 never executed
    #####:  228:  Operation *readAncestor = findAncestorOpInRegion(topRegion, read);
call    0 never executed
    #####:  229:  assert(readAncestor &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  230:         "read op should be recursively part of the top region");
        -:  231:
    #####:  232:  for (Operation *write : blockingWrites) {
branch  0 never executed
branch  1 never executed
    #####:  233:    Operation *writeAncestor = findAncestorOpInRegion(topRegion, write);
call    0 never executed
        -:  234:    // TODO: if the store and read have the same ancestor we could recurse in
        -:  235:    // the region to know if the read is reachable with more precision.
    #####:  236:    if (writeAncestor == nullptr || !isReachable(writeAncestor, readAncestor))
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  237:      continue;
    #####:  238:    if (!postDominators.postDominates(lastwrite, write)) {
branch  0 never executed
branch  1 never executed
    #####:  239:      LLVM_DEBUG(DBGS() << "Fail to do write to read forwarding due to op: "
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
        -:  240:                        << *write << "\n");
    #####:  241:      return;
        -:  242:    }
        -:  243:  }
        -:  244:
    #####:  245:  LLVM_DEBUG(DBGS() << "Forward value from " << *lastwrite.getOperation()
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
call    8 never executed
call    9 never executed
call   10 never executed
call   11 never executed
call   12 never executed
call   13 never executed
call   14 never executed
        -:  246:                    << " to: " << *read.getOperation() << "\n");
    #####:  247:  read.replaceAllUsesWith(lastwrite.getVector());
call    0 never executed
call    1 never executed
    #####:  248:  opToErase.push_back(read.getOperation());
call    0 never executed
call    1 never executed
        -:  249:}
        -:  250:
        -:  251:/// Drops unit dimensions from the input MemRefType.
function _ZN12_GLOBAL__N_1L12dropUnitDimsEN4mlir10MemRefTypeEN4llvm8ArrayRefIlEES4_S4_ called 0 returned 0% blocks executed 0%
    #####:  252:static MemRefType dropUnitDims(MemRefType inputType, ArrayRef<int64_t> offsets,
        -:  253:                               ArrayRef<int64_t> sizes,
        -:  254:                               ArrayRef<int64_t> strides) {
    #####:  255:  SmallVector<int64_t> targetShape = llvm::to_vector(
    #####:  256:      llvm::make_filter_range(sizes, [](int64_t sz) { return sz != 1; }));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  257:  Type rankReducedType = memref::SubViewOp::inferRankReducedResultType(
call    0 never executed
    #####:  258:      targetShape, inputType, offsets, sizes, strides);
call    0 never executed
    #####:  259:  return canonicalizeStridedLayout(rankReducedType.cast<MemRefType>());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  260:}
        -:  261:
        -:  262:/// Creates a rank-reducing memref.subview op that drops unit dims from its
        -:  263:/// input. Or just returns the input if it was already without unit dims.
function _ZN12_GLOBAL__N_1L35rankReducingSubviewDroppingUnitDimsERN4mlir15PatternRewriterENS0_8LocationENS0_5ValueE called 0 returned 0% blocks executed 0%
    #####:  264:static Value rankReducingSubviewDroppingUnitDims(PatternRewriter &rewriter,
        -:  265:                                                 mlir::Location loc,
        -:  266:                                                 Value input) {
    #####:  267:  MemRefType inputType = input.getType().cast<MemRefType>();
call    0 never executed
    #####:  268:  assert(inputType.hasStaticShape());
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  269:  SmallVector<int64_t> subViewOffsets(inputType.getRank(), 0);
call    0 never executed
call    1 never executed
    #####:  270:  SmallVector<int64_t> subViewStrides(inputType.getRank(), 1);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  271:  ArrayRef<int64_t> subViewSizes = inputType.getShape();
call    0 never executed
    #####:  272:  MemRefType resultType =
call    0 never executed
    #####:  273:      dropUnitDims(inputType, subViewOffsets, subViewSizes, subViewStrides);
call    0 never executed
    #####:  274:  if (canonicalizeStridedLayout(resultType) ==
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  275:      canonicalizeStridedLayout(inputType))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  276:    return input;
    #####:  277:  return rewriter.create<memref::SubViewOp>(
    #####:  278:      loc, resultType, input, subViewOffsets, subViewSizes, subViewStrides);
call    0 never executed
        -:  279:}
        -:  280:
        -:  281:/// Returns the number of dims that aren't unit dims.
    #####:  282:static int getReducedRank(ArrayRef<int64_t> shape) {
    #####:  283:  return llvm::count_if(shape, [](int64_t dimSize) { return dimSize != 1; });
        -:  284:}
        -:  285:
        -:  286:/// Returns true if all values are `arith.constant 0 : index`
function _ZN12_GLOBAL__N_1L6isZeroEN4mlir5ValueE called 0 returned 0% blocks executed 0%
    #####:  287:static bool isZero(Value v) {
    #####:  288:  auto cst = v.getDefiningOp<arith::ConstantIndexOp>();
call    0 never executed
    #####:  289:  return cst && cst.value() == 0;
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  290:}
        -:  291:
        -:  292:/// Rewrites vector.transfer_read ops where the source has unit dims, by
        -:  293:/// inserting a memref.subview dropping those unit dims.
        -:  294:class TransferReadDropUnitDimsPattern
        -:  295:    : public OpRewritePattern<vector::TransferReadOp> {
        -:  296:  using OpRewritePattern::OpRewritePattern;
        -:  297:
function _ZNK12_GLOBAL__N_131TransferReadDropUnitDimsPattern15matchAndRewriteEN4mlir6vector14TransferReadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  298:  LogicalResult matchAndRewrite(vector::TransferReadOp transferReadOp,
        -:  299:                                PatternRewriter &rewriter) const override {
    #####:  300:    auto loc = transferReadOp.getLoc();
call    0 never executed
    #####:  301:    Value vector = transferReadOp.getVector();
call    0 never executed
    #####:  302:    VectorType vectorType = vector.getType().cast<VectorType>();
call    0 never executed
    #####:  303:    Value source = transferReadOp.getSource();
call    0 never executed
    #####:  304:    MemRefType sourceType = source.getType().dyn_cast<MemRefType>();
call    0 never executed
        -:  305:    // TODO: support tensor types.
    #####:  306:    if (!sourceType || !sourceType.hasStaticShape())
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  307:      return failure();
    #####:  308:    if (sourceType.getNumElements() != vectorType.getNumElements())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  309:      return failure();
        -:  310:    // TODO: generalize this pattern, relax the requirements here.
    #####:  311:    if (transferReadOp.hasOutOfBoundsDim())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  312:      return failure();
    #####:  313:    if (!transferReadOp.getPermutationMap().isMinorIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  314:      return failure();
    #####:  315:    int reducedRank = getReducedRank(sourceType.getShape());
call    0 never executed
    #####:  316:    if (reducedRank == sourceType.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  317:      return failure(); // The source shape can't be further reduced.
    #####:  318:    if (reducedRank != vectorType.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  319:      return failure(); // This pattern requires the vector shape to match the
        -:  320:                        // reduced source shape.
    #####:  321:    if (llvm::any_of(transferReadOp.getIndices(),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  322:                     [](Value v) { return !isZero(v); }))
    #####:  323:      return failure();
    #####:  324:    Value reducedShapeSource =
    #####:  325:        rankReducingSubviewDroppingUnitDims(rewriter, loc, source);
call    0 never executed
    #####:  326:    Value c0 = rewriter.create<arith::ConstantIndexOp>(loc, 0);
call    0 never executed
call    1 never executed
    #####:  327:    SmallVector<Value> zeros(reducedRank, c0);
call    0 never executed
    #####:  328:    auto identityMap = rewriter.getMultiDimIdentityMap(reducedRank);
call    0 never executed
    #####:  329:    rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
    #####:  330:        transferReadOp, vectorType, reducedShapeSource, zeros, identityMap);
call    0 never executed
    #####:  331:    return success();
branch  0 never executed
branch  1 never executed
        -:  332:  }
        -:  333:};
        -:  334:
        -:  335:/// Rewrites vector.transfer_write ops where the "source" (i.e. destination) has
        -:  336:/// unit dims, by inserting a memref.subview dropping those unit dims.
        -:  337:class TransferWriteDropUnitDimsPattern
        -:  338:    : public OpRewritePattern<vector::TransferWriteOp> {
        -:  339:  using OpRewritePattern::OpRewritePattern;
        -:  340:
function _ZNK12_GLOBAL__N_132TransferWriteDropUnitDimsPattern15matchAndRewriteEN4mlir6vector15TransferWriteOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  341:  LogicalResult matchAndRewrite(vector::TransferWriteOp transferWriteOp,
        -:  342:                                PatternRewriter &rewriter) const override {
    #####:  343:    auto loc = transferWriteOp.getLoc();
call    0 never executed
    #####:  344:    Value vector = transferWriteOp.getVector();
call    0 never executed
    #####:  345:    VectorType vectorType = vector.getType().cast<VectorType>();
call    0 never executed
    #####:  346:    Value source = transferWriteOp.getSource();
call    0 never executed
    #####:  347:    MemRefType sourceType = source.getType().dyn_cast<MemRefType>();
call    0 never executed
        -:  348:    // TODO: support tensor type.
    #####:  349:    if (!sourceType || !sourceType.hasStaticShape())
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  350:      return failure();
    #####:  351:    if (sourceType.getNumElements() != vectorType.getNumElements())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  352:      return failure();
        -:  353:    // TODO: generalize this pattern, relax the requirements here.
    #####:  354:    if (transferWriteOp.hasOutOfBoundsDim())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  355:      return failure();
    #####:  356:    if (!transferWriteOp.getPermutationMap().isMinorIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  357:      return failure();
    #####:  358:    int reducedRank = getReducedRank(sourceType.getShape());
call    0 never executed
    #####:  359:    if (reducedRank == sourceType.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  360:      return failure(); // The source shape can't be further reduced.
    #####:  361:    if (reducedRank != vectorType.getRank())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  362:      return failure(); // This pattern requires the vector shape to match the
        -:  363:                        // reduced source shape.
    #####:  364:    if (llvm::any_of(transferWriteOp.getIndices(),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  365:                     [](Value v) { return !isZero(v); }))
    #####:  366:      return failure();
    #####:  367:    Value reducedShapeSource =
    #####:  368:        rankReducingSubviewDroppingUnitDims(rewriter, loc, source);
call    0 never executed
    #####:  369:    Value c0 = rewriter.create<arith::ConstantIndexOp>(loc, 0);
call    0 never executed
call    1 never executed
    #####:  370:    SmallVector<Value> zeros(reducedRank, c0);
call    0 never executed
    #####:  371:    auto identityMap = rewriter.getMultiDimIdentityMap(reducedRank);
call    0 never executed
    #####:  372:    rewriter.replaceOpWithNewOp<vector::TransferWriteOp>(
    #####:  373:        transferWriteOp, vector, reducedShapeSource, zeros, identityMap);
call    0 never executed
    #####:  374:    return success();
branch  0 never executed
branch  1 never executed
        -:  375:  }
        -:  376:};
        -:  377:
        -:  378:/// Return true if the memref type has its inner dimension matching the given
        -:  379:/// shape. Otherwise return false.
function _ZN12_GLOBAL__N_1L30hasMatchingInnerContigousShapeEN4mlir10MemRefTypeEN4llvm8ArrayRefIlEE called 328 returned 100% blocks executed 94%
      328:  380:static int64_t hasMatchingInnerContigousShape(MemRefType memrefType,
        -:  381:                                              ArrayRef<int64_t> targetShape) {
      328:  382:  auto shape = memrefType.getShape();
call    0 returned 100%
      328:  383:  SmallVector<int64_t> strides;
call    0 returned 100%
      328:  384:  int64_t offset;
      328:  385:  if (!succeeded(getStridesAndOffset(memrefType, strides, offset)))
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
        -:  386:    return false;
      328:  387:  if (strides.back() != 1)
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
        -:  388:    return false;
      328:  389:  strides.pop_back();
call    0 returned 100%
      328:  390:  int64_t flatDim = 1;
      378:  391:  for (auto [targetDim, memrefDim, memrefStride] :
      378:  392:       llvm::reverse(llvm::zip(targetShape, shape, strides))) {
branch  0 taken 87% (fallthrough)
branch  1 taken 13%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
      328:  393:    flatDim *= memrefDim;
      328:  394:    if (flatDim != memrefStride || targetDim != memrefDim)
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 85% (fallthrough)
branch  3 taken 15%
      278:  395:      return false;
        -:  396:  }
       50:  397:  return true;
        -:  398:}
        -:  399:
        -:  400:/// Creates a memref.collapse_shape collapsing all inner dimensions of the
        -:  401:/// input starting at `firstDimToCollapse`.
function _ZN12_GLOBAL__N_1L17collapseInnerDimsERN4mlir15PatternRewriterENS0_8LocationENS0_5ValueEl called 0 returned 0% blocks executed 0%
    #####:  402:static Value collapseInnerDims(PatternRewriter &rewriter, mlir::Location loc,
        -:  403:                               Value input, int64_t firstDimToCollapse) {
    #####:  404:  ShapedType inputType = input.getType().cast<ShapedType>();
call    0 never executed
    #####:  405:  if (inputType.getRank() == 1)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  406:    return input;
    #####:  407:  SmallVector<ReassociationIndices> reassociation;
    #####:  408:  for (int64_t i = 0; i < firstDimToCollapse; ++i)
branch  0 never executed
branch  1 never executed
    #####:  409:    reassociation.push_back(ReassociationIndices{i});
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  410:  ReassociationIndices collapsedIndices;
call    0 never executed
    #####:  411:  for (int64_t i = firstDimToCollapse; i < inputType.getRank(); ++i)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  412:    collapsedIndices.push_back(i);
call    0 never executed
    #####:  413:  reassociation.push_back(collapsedIndices);
call    0 never executed
    #####:  414:  return rewriter.create<memref::CollapseShapeOp>(loc, input, reassociation);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  415:}
        -:  416:
        -:  417:/// Checks that the indices corresponding to dimensions starting at
        -:  418:/// `firstDimToCollapse` are constant 0, and writes to `outIndices`
        -:  419:/// the truncated indices where `firstDimToCollapse` is now the innermost dim.
        -:  420:static LogicalResult
function _ZN12_GLOBAL__N_1L32checkAndCollapseInnerZeroIndicesEN4mlir10ValueRangeElRN4llvm11SmallVectorINS0_5ValueELj6EEE called 0 returned 0% blocks executed 0%
    #####:  421:checkAndCollapseInnerZeroIndices(ValueRange indices, int64_t firstDimToCollapse,
        -:  422:                                 SmallVector<Value> &outIndices) {
    #####:  423:  int64_t rank = indices.size();
branch  0 never executed
branch  1 never executed
    #####:  424:  if (firstDimToCollapse >= rank)
branch  0 never executed
branch  1 never executed
    #####:  425:    return failure();
    #####:  426:  for (int64_t i = firstDimToCollapse; i < rank; ++i) {
branch  0 never executed
branch  1 never executed
    #####:  427:    arith::ConstantIndexOp cst =
    #####:  428:        indices[i].getDefiningOp<arith::ConstantIndexOp>();
call    0 never executed
call    1 never executed
    #####:  429:    if (!cst || cst.value() != 0)
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  430:      return failure();
        -:  431:  }
    #####:  432:  outIndices = indices;
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  433:  outIndices.resize(firstDimToCollapse + 1);
call    0 never executed
    #####:  434:  return success();
        -:  435:}
        -:  436:
        -:  437:/// Rewrites contiguous row-major vector.transfer_read ops by inserting
        -:  438:/// memref.collapse_shape on the source so that the resulting
        -:  439:/// vector.transfer_read has a 1D source. Requires the source shape to be
        -:  440:/// already reduced i.e. without unit dims.
        -:  441:class FlattenContiguousRowMajorTransferReadPattern
        -:  442:    : public OpRewritePattern<vector::TransferReadOp> {
        -:  443:  using OpRewritePattern::OpRewritePattern;
        -:  444:
function _ZNK12_GLOBAL__N_144FlattenContiguousRowMajorTransferReadPattern15matchAndRewriteEN4mlir6vector14TransferReadOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  445:  LogicalResult matchAndRewrite(vector::TransferReadOp transferReadOp,
        -:  446:                                PatternRewriter &rewriter) const override {
    #####:  447:    auto loc = transferReadOp.getLoc();
call    0 never executed
    #####:  448:    Value vector = transferReadOp.getVector();
call    0 never executed
    #####:  449:    VectorType vectorType = vector.getType().cast<VectorType>();
call    0 never executed
    #####:  450:    Value source = transferReadOp.getSource();
call    0 never executed
    #####:  451:    MemRefType sourceType = source.getType().dyn_cast<MemRefType>();
call    0 never executed
        -:  452:    // Contiguity check is valid on tensors only.
    #####:  453:    if (!sourceType)
branch  0 never executed
branch  1 never executed
    #####:  454:      return failure();
    #####:  455:    if (vectorType.getRank() <= 1)
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  456:      // Already 0D/1D, nothing to do.
    #####:  457:      return failure();
    #####:  458:    if (!hasMatchingInnerContigousShape(
call    0 never executed
        -:  459:            sourceType,
    #####:  460:            vectorType.getShape().take_back(vectorType.getRank() - 1)))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  461:      return failure();
    #####:  462:    int64_t firstContiguousInnerDim =
    #####:  463:        sourceType.getRank() - vectorType.getRank();
call    0 never executed
call    1 never executed
        -:  464:    // TODO: generalize this pattern, relax the requirements here.
    #####:  465:    if (transferReadOp.hasOutOfBoundsDim())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  466:      return failure();
    #####:  467:    if (!transferReadOp.getPermutationMap().isMinorIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  468:      return failure();
    #####:  469:    if (transferReadOp.getMask())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  470:      return failure();
    #####:  471:    SmallVector<Value> collapsedIndices;
call    0 never executed
    #####:  472:    if (failed(checkAndCollapseInnerZeroIndices(transferReadOp.getIndices(),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  473:                                                firstContiguousInnerDim,
        -:  474:                                                collapsedIndices)))
    #####:  475:      return failure();
    #####:  476:    Value collapsedSource =
    #####:  477:        collapseInnerDims(rewriter, loc, source, firstContiguousInnerDim);
call    0 never executed
    #####:  478:    MemRefType collapsedSourceType =
    #####:  479:        collapsedSource.getType().dyn_cast<MemRefType>();
call    0 never executed
    #####:  480:    int64_t collapsedRank = collapsedSourceType.getRank();
call    0 never executed
    #####:  481:    assert(collapsedRank == firstContiguousInnerDim + 1);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  482:    SmallVector<AffineExpr, 1> dimExprs{
    #####:  483:        getAffineDimExpr(firstContiguousInnerDim, rewriter.getContext())};
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  484:    auto collapsedMap =
    #####:  485:        AffineMap::get(collapsedRank, 0, dimExprs, rewriter.getContext());
call    0 never executed
    #####:  486:    VectorType flatVectorType = VectorType::get({vectorType.getNumElements()},
call    0 never executed
    #####:  487:                                                vectorType.getElementType());
call    0 never executed
call    1 never executed
    #####:  488:    vector::TransferReadOp flatRead = rewriter.create<vector::TransferReadOp>(
    #####:  489:        loc, flatVectorType, collapsedSource, collapsedIndices, collapsedMap);
call    0 never executed
    #####:  490:    flatRead.setInBoundsAttr(rewriter.getBoolArrayAttr({true}));
call    0 never executed
call    1 never executed
    #####:  491:    rewriter.replaceOpWithNewOp<vector::ShapeCastOp>(
    #####:  492:        transferReadOp, vector.getType().cast<VectorType>(), flatRead);
call    0 never executed
call    1 never executed
    #####:  493:    return success();
branch  0 never executed
branch  1 never executed
        -:  494:  }
        -:  495:};
        -:  496:
        -:  497:/// Rewrites contiguous row-major vector.transfer_write ops by inserting
        -:  498:/// memref.collapse_shape on the source so that the resulting
        -:  499:/// vector.transfer_write has a 1D source. Requires the source shape to be
        -:  500:/// already reduced i.e. without unit dims.
        -:  501:class FlattenContiguousRowMajorTransferWritePattern
        -:  502:    : public OpRewritePattern<vector::TransferWriteOp> {
        -:  503:  using OpRewritePattern::OpRewritePattern;
        -:  504:
function _ZNK12_GLOBAL__N_145FlattenContiguousRowMajorTransferWritePattern15matchAndRewriteEN4mlir6vector15TransferWriteOpERNS1_15PatternRewriterE called 3358 returned 100% blocks executed 37%
     3358:  505:  LogicalResult matchAndRewrite(vector::TransferWriteOp transferWriteOp,
        -:  506:                                PatternRewriter &rewriter) const override {
     3358:  507:    auto loc = transferWriteOp.getLoc();
call    0 returned 100%
     3358:  508:    Value vector = transferWriteOp.getVector();
call    0 returned 100%
     3358:  509:    VectorType vectorType = vector.getType().cast<VectorType>();
call    0 returned 100%
     3358:  510:    Value source = transferWriteOp.getSource();
call    0 returned 100%
     3358:  511:    MemRefType sourceType = source.getType().dyn_cast<MemRefType>();
call    0 returned 100%
        -:  512:    // Contiguity check is valid on tensors only.
     3358:  513:    if (!sourceType)
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  514:      return failure();
     3358:  515:    if (vectorType.getRank() <= 1)
call    0 returned 100%
branch  1 taken 90% (fallthrough)
branch  2 taken 10%
        -:  516:      // Already 0D/1D, nothing to do.
     3030:  517:      return failure();
      328:  518:    if (!hasMatchingInnerContigousShape(
call    0 returned 100%
        -:  519:            sourceType,
      656:  520:            vectorType.getShape().take_back(vectorType.getRank() - 1)))
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
branch  4 taken 85% (fallthrough)
branch  5 taken 15%
      278:  521:      return failure();
       50:  522:    int64_t firstContiguousInnerDim =
       50:  523:        sourceType.getRank() - vectorType.getRank();
call    0 returned 100%
call    1 returned 100%
        -:  524:    // TODO: generalize this pattern, relax the requirements here.
       50:  525:    if (transferWriteOp.hasOutOfBoundsDim())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
       50:  526:      return failure();
    #####:  527:    if (!transferWriteOp.getPermutationMap().isMinorIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  528:      return failure();
    #####:  529:    if (transferWriteOp.getMask())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  530:      return failure();
    #####:  531:    SmallVector<Value> collapsedIndices;
call    0 never executed
    #####:  532:    if (failed(checkAndCollapseInnerZeroIndices(transferWriteOp.getIndices(),
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  533:                                                firstContiguousInnerDim,
        -:  534:                                                collapsedIndices)))
    #####:  535:      return failure();
    #####:  536:    Value collapsedSource =
    #####:  537:        collapseInnerDims(rewriter, loc, source, firstContiguousInnerDim);
call    0 never executed
    #####:  538:    MemRefType collapsedSourceType =
    #####:  539:        collapsedSource.getType().cast<MemRefType>();
call    0 never executed
    #####:  540:    int64_t collapsedRank = collapsedSourceType.getRank();
call    0 never executed
    #####:  541:    assert(collapsedRank == firstContiguousInnerDim + 1);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  542:    SmallVector<AffineExpr, 1> dimExprs{
    #####:  543:        getAffineDimExpr(firstContiguousInnerDim, rewriter.getContext())};
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  544:    auto collapsedMap =
    #####:  545:        AffineMap::get(collapsedRank, 0, dimExprs, rewriter.getContext());
call    0 never executed
    #####:  546:    VectorType flatVectorType = VectorType::get({vectorType.getNumElements()},
call    0 never executed
    #####:  547:                                                vectorType.getElementType());
call    0 never executed
call    1 never executed
    #####:  548:    Value flatVector =
    #####:  549:        rewriter.create<vector::ShapeCastOp>(loc, flatVectorType, vector);
call    0 never executed
call    1 never executed
    #####:  550:    vector::TransferWriteOp flatWrite =
        -:  551:        rewriter.create<vector::TransferWriteOp>(
    #####:  552:            loc, flatVector, collapsedSource, collapsedIndices, collapsedMap);
call    0 never executed
    #####:  553:    flatWrite.setInBoundsAttr(rewriter.getBoolArrayAttr({true}));
call    0 never executed
call    1 never executed
    #####:  554:    rewriter.eraseOp(transferWriteOp);
call    0 never executed
    #####:  555:    return success();
branch  0 never executed
branch  1 never executed
        -:  556:  }
        -:  557:};
        -:  558:
        -:  559:} // namespace
        -:  560:
function _ZN4mlir6vector17transferOpflowOptEPNS_9OperationE called 403 returned 98% blocks executed 100%
      403:  561:void mlir::vector::transferOpflowOpt(Operation *rootOp) {
      797:  562:  TransferOptimization opt(rootOp);
call    0 returned 99%
call    1 returned 100%
        -:  563:  // Run store to load forwarding first since it can expose more dead store
        -:  564:  // opportunity.
function _ZZN4mlir6vector17transferOpflowOptEPNS_9OperationEENKUlNS0_14TransferReadOpEE_clES3_.isra.0 called 4799 returned 100% blocks executed 100%
      400:  565:  rootOp->walk([&](vector::TransferReadOp read) {
call    0 returned 99%
     4799:  566:    if (read.getShapedType().isa<MemRefType>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 36% (fallthrough)
branch  3 taken 64%
     1750:  567:      opt.storeToLoadForwarding(read);
call    0 returned 100%
     4799:  568:  });
      396:  569:  opt.removeDeadOp();
call    0 returned 100%
function _ZZN4mlir6vector17transferOpflowOptEPNS_9OperationEENKUlNS0_15TransferWriteOpEE0_clES3_.isra.0 called 4206 returned 100% blocks executed 100%
      397:  570:  rootOp->walk([&](vector::TransferWriteOp write) {
call    0 returned 99%
     4206:  571:    if (write.getShapedType().isa<MemRefType>())
call    0 returned 100%
call    1 returned 100%
branch  2 taken 35% (fallthrough)
branch  3 taken 65%
     1489:  572:      opt.deadStoreOp(write);
call    0 returned 100%
     4206:  573:  });
      395:  574:  opt.removeDeadOp();
call    0 returned 100%
      394:  575:}
        -:  576:
function _ZN4mlir6vector42populateVectorTransferDropUnitDimsPatternsERNS_17RewritePatternSetENS_14PatternBenefitE called 0 returned 0% blocks executed 0%
    #####:  577:void mlir::vector::populateVectorTransferDropUnitDimsPatterns(
        -:  578:    RewritePatternSet &patterns, PatternBenefit benefit) {
    #####:  579:  patterns
        -:  580:      .add<TransferReadDropUnitDimsPattern, TransferWriteDropUnitDimsPattern>(
    #####:  581:          patterns.getContext(), benefit);
call    0 never executed
    #####:  582:  populateShapeCastFoldingPatterns(patterns);
call    0 never executed
call    1 never executed
    #####:  583:}
        -:  584:
function _ZN4mlir6vector37populateFlattenVectorTransferPatternsERNS_17RewritePatternSetENS_14PatternBenefitE called 426 returned 98% blocks executed 100%
      426:  585:void mlir::vector::populateFlattenVectorTransferPatterns(
        -:  586:    RewritePatternSet &patterns, PatternBenefit benefit) {
      426:  587:  patterns.add<FlattenContiguousRowMajorTransferReadPattern,
        -:  588:               FlattenContiguousRowMajorTransferWritePattern>(
      426:  589:      patterns.getContext(), benefit);
call    0 returned 95%
      406:  590:  populateShapeCastFoldingPatterns(patterns, benefit);
call    0 returned 103%
      419:  591:}
