        -:    0:Source:/data/xcy/llvm-project-fdbc55a5/mlir/lib/Conversion/VectorToGPU/VectorToGPU.cpp
        -:    0:Graph:../tools/mlir/lib/Conversion/VectorToGPU/CMakeFiles/obj.MLIRVectorToGPU.dir/VectorToGPU.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Conversion/VectorToGPU/CMakeFiles/obj.MLIRVectorToGPU.dir/VectorToGPU.cpp.gcda
        -:    0:Runs:116161
        -:    1://===- VectorToGPU.cpp - Convert vector to GPU dialect ----------*- C++ -*-===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements lowering of vector operations to GPU dialect ops.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#include "mlir/Conversion/VectorToGPU/VectorToGPU.h"
        -:   14:
        -:   15:#include <type_traits>
        -:   16:
        -:   17:#include "mlir/Analysis/SliceAnalysis.h"
        -:   18:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   19:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   20:#include "mlir/Dialect/GPU/IR/GPUDialect.h"
        -:   21:#include "mlir/Dialect/MemRef/IR/MemRef.h"
        -:   22:#include "mlir/Dialect/NVGPU/IR/NVGPUDialect.h"
        -:   23:#include "mlir/Dialect/NVGPU/Utils/MMAUtils.h"
        -:   24:#include "mlir/Dialect/SCF/IR/SCF.h"
        -:   25:#include "mlir/Dialect/Utils/StructuredOpsUtils.h"
        -:   26:#include "mlir/Dialect/Vector/IR/VectorOps.h"
        -:   27:#include "mlir/Dialect/Vector/Utils/VectorUtils.h"
        -:   28:#include "mlir/IR/Builders.h"
        -:   29:#include "mlir/Pass/Pass.h"
        -:   30:#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
        -:   31:#include "mlir/Transforms/Passes.h"
        -:   32:#include "llvm/ADT/TypeSwitch.h"
        -:   33:
        -:   34:namespace mlir {
        -:   35:#define GEN_PASS_DEF_CONVERTVECTORTOGPU
        -:   36:#include "mlir/Conversion/Passes.h.inc"
        -:   37:} // namespace mlir
        -:   38:
        -:   39:using namespace mlir;
        -:   40:
        -:   41:/// For a vector TransferOpType `xferOp`, an empty `indices` vector, and an
        -:   42:/// AffineMap representing offsets to apply to indices, the function fills
        -:   43:/// `indices` with the original indices plus the offsets. The offsets are
        -:   44:/// applied by taking into account the permutation map of the transfer op. If
        -:   45:/// the `offsetMap` has dimension placeholders, those should be provided in
        -:   46:/// `dimValues`.
        -:   47:template <typename TransferOpType>
    #####:   48:static void getXferIndices(OpBuilder &b, TransferOpType xferOp,
        -:   49:                           AffineMap offsetMap, ArrayRef<Value> dimValues,
        -:   50:                           SmallVector<Value, 4> &indices) {
    #####:   51:  indices.append(xferOp.getIndices().begin(), xferOp.getIndices().end());
    #####:   52:  Location loc = xferOp.getLoc();
    #####:   53:  unsigned offsetsIdx = 0;
    #####:   54:  for (auto expr : xferOp.getPermutationMap().getResults()) {
    #####:   55:    if (auto dim = expr.template dyn_cast<AffineDimExpr>()) {
    #####:   56:      Value prevIdx = indices[dim.getPosition()];
    #####:   57:      SmallVector<Value, 3> dims(dimValues.begin(), dimValues.end());
    #####:   58:      dims.push_back(prevIdx);
    #####:   59:      AffineExpr d0 = b.getAffineDimExpr(offsetMap.getNumDims());
    #####:   60:      indices[dim.getPosition()] = makeComposedAffineApply(
        -:   61:          b, loc, d0 + offsetMap.getResult(offsetsIdx++), dims);
        -:   62:      continue;
        -:   63:    }
        -:   64:  }
    #####:   65:}
------------------
_Z14getXferIndicesIN4mlir6vector15TransferWriteOpEEvRNS0_9OpBuilderET_NS0_9AffineMapEN4llvm8ArrayRefINS0_5ValueEEERNS7_11SmallVectorIS9_Lj4EEE:
function _Z14getXferIndicesIN4mlir6vector15TransferWriteOpEEvRNS0_9OpBuilderET_NS0_9AffineMapEN4llvm8ArrayRefINS0_5ValueEEERNS7_11SmallVectorIS9_Lj4EEE called 0 returned 0% blocks executed 0%
    #####:   48:static void getXferIndices(OpBuilder &b, TransferOpType xferOp,
        -:   49:                           AffineMap offsetMap, ArrayRef<Value> dimValues,
        -:   50:                           SmallVector<Value, 4> &indices) {
    #####:   51:  indices.append(xferOp.getIndices().begin(), xferOp.getIndices().end());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:   52:  Location loc = xferOp.getLoc();
call    0 never executed
    #####:   53:  unsigned offsetsIdx = 0;
    #####:   54:  for (auto expr : xferOp.getPermutationMap().getResults()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   55:    if (auto dim = expr.template dyn_cast<AffineDimExpr>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   56:      Value prevIdx = indices[dim.getPosition()];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   57:      SmallVector<Value, 3> dims(dimValues.begin(), dimValues.end());
call    0 never executed
    #####:   58:      dims.push_back(prevIdx);
call    0 never executed
    #####:   59:      AffineExpr d0 = b.getAffineDimExpr(offsetMap.getNumDims());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:   60:      indices[dim.getPosition()] = makeComposedAffineApply(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
        -:   61:          b, loc, d0 + offsetMap.getResult(offsetsIdx++), dims);
        -:   62:      continue;
        -:   63:    }
        -:   64:  }
    #####:   65:}
------------------
_Z14getXferIndicesIN4mlir6vector14TransferReadOpEEvRNS0_9OpBuilderET_NS0_9AffineMapEN4llvm8ArrayRefINS0_5ValueEEERNS7_11SmallVectorIS9_Lj4EEE:
function _Z14getXferIndicesIN4mlir6vector14TransferReadOpEEvRNS0_9OpBuilderET_NS0_9AffineMapEN4llvm8ArrayRefINS0_5ValueEEERNS7_11SmallVectorIS9_Lj4EEE called 0 returned 0% blocks executed 0%
    #####:   48:static void getXferIndices(OpBuilder &b, TransferOpType xferOp,
        -:   49:                           AffineMap offsetMap, ArrayRef<Value> dimValues,
        -:   50:                           SmallVector<Value, 4> &indices) {
    #####:   51:  indices.append(xferOp.getIndices().begin(), xferOp.getIndices().end());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:   52:  Location loc = xferOp.getLoc();
call    0 never executed
    #####:   53:  unsigned offsetsIdx = 0;
    #####:   54:  for (auto expr : xferOp.getPermutationMap().getResults()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:   55:    if (auto dim = expr.template dyn_cast<AffineDimExpr>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   56:      Value prevIdx = indices[dim.getPosition()];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   57:      SmallVector<Value, 3> dims(dimValues.begin(), dimValues.end());
call    0 never executed
    #####:   58:      dims.push_back(prevIdx);
call    0 never executed
    #####:   59:      AffineExpr d0 = b.getAffineDimExpr(offsetMap.getNumDims());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:   60:      indices[dim.getPosition()] = makeComposedAffineApply(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
        -:   61:          b, loc, d0 + offsetMap.getResult(offsetsIdx++), dims);
        -:   62:      continue;
        -:   63:    }
        -:   64:  }
    #####:   65:}
------------------
        -:   66:
        -:   67:// Return true if the contract op can be convert to MMA matmul.
function _ZL29contractSupportsMMAMatrixTypeN4mlir6vector13ContractionOpEb called 0 returned 0% blocks executed 0%
    #####:   68:static bool contractSupportsMMAMatrixType(vector::ContractionOp contract,
        -:   69:                                          bool useNvGpu) {
    #####:   70:  if (!contract.getMasks().empty())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:   71:    return false;
        -:   72:
    #####:   73:  using MapList = ArrayRef<ArrayRef<AffineExpr>>;
    #####:   74:  auto infer = [](MapList m) { return AffineMap::inferFromExprList(m); };
call    0 never executed
call    1 never executed
    #####:   75:  AffineExpr m, n, k;
    #####:   76:  bindDims(contract.getContext(), m, n, k);
call    0 never executed
call    1 never executed
    #####:   77:  auto iteratorTypes = contract.getIteratorTypes().getValue();
call    0 never executed
call    1 never executed
    #####:   78:  if (!(vector::isParallelIterator(iteratorTypes[0]) &&
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
    #####:   79:        vector::isParallelIterator(iteratorTypes[1]) &&
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:   80:        vector::isReductionIterator(iteratorTypes[2])))
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:   81:    return false;
        -:   82:
        -:   83:  // The contract needs to represent a matmul to be able to convert to
        -:   84:  // MMAMatrix matmul.
    #####:   85:  if (!useNvGpu &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:   86:      contract.getIndexingMapsArray() != infer({{m, k}, {k, n}, {m, n}}))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
        -:   87:    return false;
    #####:   88:  if (useNvGpu &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:   89:      contract.getIndexingMapsArray() != infer({{m, k}, {n, k}, {m, n}}))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
branch  9 never executed
    #####:   90:    return false;
        -:   91:
        -:   92:  return true;
        -:   93:}
        -:   94:
        -:   95:// Return the stide for the dimension 0 of |type| if it is a memref and has a
        -:   96:// constant stride.
        -:   97:static llvm::Optional<int64_t>
function _ZL33getMemrefConstantHorizontalStrideN4mlir10ShapedTypeE called 0 returned 0% blocks executed 0%
    #####:   98:getMemrefConstantHorizontalStride(ShapedType type) {
    #####:   99:  auto memrefType = type.dyn_cast<MemRefType>();
call    0 never executed
    #####:  100:  if (!memrefType)
branch  0 never executed
branch  1 never executed
    #####:  101:    return false;
        -:  102:  // If the memref is 0 or 1D the horizontal stride is 0.
    #####:  103:  if (memrefType.getRank() < 2)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  104:    return 0;
    #####:  105:  int64_t offset = 0;
    #####:  106:  SmallVector<int64_t, 2> strides;
call    0 never executed
    #####:  107:  if (failed(getStridesAndOffset(memrefType, strides, offset)) ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  108:      strides.back() != 1)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  109:    return llvm::None;
    #####:  110:  int64_t stride = strides[strides.size() - 2];
branch  0 never executed
branch  1 never executed
    #####:  111:  if (stride == ShapedType::kDynamicStrideOrOffset)
branch  0 never executed
branch  1 never executed
    #####:  112:    return llvm::None;
    #####:  113:  return stride;
        -:  114:}
        -:  115:
        -:  116:// Return true if the transfer op can be converted to a MMA matrix load.
function _ZL33transferReadSupportsMMAMatrixTypeN4mlir6vector14TransferReadOpEb called 0 returned 0% blocks executed 0%
    #####:  117:static bool transferReadSupportsMMAMatrixType(vector::TransferReadOp readOp,
        -:  118:                                              bool useNvGpu) {
    #####:  119:  if (readOp.getMask() || readOp.hasOutOfBoundsDim() ||
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  120:      readOp.getVectorType().getRank() != 2)
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  121:    return false;
    #####:  122:  if (!getMemrefConstantHorizontalStride(readOp.getShapedType()))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  123:    return false;
    #####:  124:  AffineMap map = readOp.getPermutationMap();
call    0 never executed
    #####:  125:  OpBuilder b(readOp.getContext());
call    0 never executed
call    1 never executed
    #####:  126:  AffineExpr innerDim = b.getAffineDimExpr(map.getNumDims() - 1);
call    0 never executed
call    1 never executed
    #####:  127:  AffineExpr zero = b.getAffineConstantExpr(0);
call    0 never executed
    #####:  128:  auto broadcastInnerDim = AffineMap::get(map.getNumDims(), 0, {zero, innerDim},
call    0 never executed
    #####:  129:                                          readOp.getContext());
call    0 never executed
call    1 never executed
        -:  130:
    #####:  131:  if (!useNvGpu) {
branch  0 never executed
branch  1 never executed
        -:  132:    // TODO: Support transpose once it is added to GPU dialect ops.
        -:  133:    // For now we only support (d0, d1) -> (d0, d1) and (d0, d1) -> (0, d1).
    #####:  134:    return map.isMinorIdentity() || map == broadcastInnerDim;
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -:  135:  }
        -:  136:
        -:  137:  return true;
        -:  138:}
        -:  139:
        -:  140:// Return true if the transfer op can be converted to a MMA matrix store.
        -:  141:static bool
function _ZL34transferWriteSupportsMMAMatrixTypeN4mlir6vector15TransferWriteOpE called 0 returned 0% blocks executed 0%
    #####:  142:transferWriteSupportsMMAMatrixType(vector::TransferWriteOp writeOp) {
        -:  143:  // TODO: support 0-d corner case.
    #####:  144:  if (writeOp.getTransferRank() == 0)
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  145:    return false;
        -:  146:
    #####:  147:  if (writeOp.getMask() || writeOp.hasOutOfBoundsDim() ||
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  148:      writeOp.getVectorType().getRank() != 2)
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  149:    return false;
    #####:  150:  if (!getMemrefConstantHorizontalStride(writeOp.getShapedType()))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  151:    return false;
        -:  152:  // TODO: Support transpose once it is added to GPU dialect ops.
    #####:  153:  if (!writeOp.getPermutationMap().isMinorIdentity())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  154:    return false;
        -:  155:  return true;
        -:  156:}
        -:  157:
        -:  158:/// Return true if the constant is a splat to a 2D vector so that it can be
        -:  159:/// converted to a MMA constant matrix op.
function _ZL29constantSupportsMMAMatrixTypeN4mlir5arith10ConstantOpE called 0 returned 0% blocks executed 0%
    #####:  160:static bool constantSupportsMMAMatrixType(arith::ConstantOp constantOp) {
    #####:  161:  auto vecType = constantOp.getType().dyn_cast<VectorType>();
call    0 never executed
    #####:  162:  if (!vecType || vecType.getRank() != 2)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  163:    return false;
    #####:  164:  return constantOp.getValue().isa<SplatElementsAttr>();
call    0 never executed
call    1 never executed
        -:  165:}
        -:  166:
        -:  167:/// Return true if this is a broadcast from scalar to a 2D vector.
function _ZL30broadcastSupportsMMAMatrixTypeN4mlir6vector11BroadcastOpE called 0 returned 0% blocks executed 0%
    #####:  168:static bool broadcastSupportsMMAMatrixType(vector::BroadcastOp broadcastOp) {
    #####:  169:  return broadcastOp.getVectorType().getRank() == 2 &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  170:         broadcastOp.getSource().getType().isa<FloatType>();
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  171:}
        -:  172:
        -:  173:/// Return the MMA elementwise enum associated with `op` if it is supported.
        -:  174:/// Return `llvm::None` otherwise.
        -:  175:static llvm::Optional<gpu::MMAElementwiseOp>
function _ZL25convertElementwiseOpToMMAPN4mlir9OperationE called 0 returned 0% blocks executed 0%
    #####:  176:convertElementwiseOpToMMA(Operation *op) {
    #####:  177:  if (isa<arith::AddFOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  178:    return gpu::MMAElementwiseOp::ADDF;
    #####:  179:  if (isa<arith::MulFOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  180:    return gpu::MMAElementwiseOp::MULF;
    #####:  181:  if (isa<arith::MaxFOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  182:    return gpu::MMAElementwiseOp::MAXF;
    #####:  183:  if (isa<arith::MinFOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  184:    return gpu::MMAElementwiseOp::MINF;
    #####:  185:  if (isa<arith::DivFOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  186:    return gpu::MMAElementwiseOp::DIVF;
    #####:  187:  return llvm::None;
        -:  188:}
        -:  189:
        -:  190:/// Return true if the op is supported as elementwise op on MMAMatrix type.
    #####:  191:static bool elementwiseSupportsMMAMatrixType(Operation *op) {
    #####:  192:  return convertElementwiseOpToMMA(op).has_value();
        -:  193:}
        -:  194:
        -:  195:/// Returns true if the extract strided slice op is supported with `mma.sync`
        -:  196:/// path.
        -:  197:static bool
function _ZL40extractStridedSliceSupportsMMAMatrixTypeN4mlir6vector21ExtractStridedSliceOpE called 0 returned 0% blocks executed 0%
    #####:  198:extractStridedSliceSupportsMMAMatrixType(vector::ExtractStridedSliceOp op) {
        -:  199:
    #####:  200:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  201:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  202:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
        -:  203:    return false;
        -:  204:
    #####:  205:  FailureOr<vector::ContractionOp> contractOp = nvgpu::getUserContract(op);
call    0 never executed
    #####:  206:  if (failed(contractOp))
branch  0 never executed
branch  1 never executed
        -:  207:    return false;
        -:  208:
        -:  209:  // Handle vector.extract_strided_slice on registers containing
        -:  210:  // matrixB and matrixC operands. vector.extract_strided_slice op
        -:  211:  // is not supported on registers containing matrixA operands.
    #####:  212:  if (warpMatrixInfo->operandRole == nvgpu::MatMulOperandRole::B)
branch  0 never executed
branch  1 never executed
    #####:  213:    return (op->getResult(0).getType().cast<VectorType>() ==
call    0 never executed
    #####:  214:            (*contractOp).getRhs().getType().cast<VectorType>());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  215:  else if (warpMatrixInfo->operandRole == nvgpu::MatMulOperandRole::C)
branch  0 never executed
branch  1 never executed
    #####:  216:    return (op->getResult(0).getType().cast<VectorType>() ==
call    0 never executed
    #####:  217:            (*contractOp).getAcc().getType().cast<VectorType>());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  218:
        -:  219:  return false;
        -:  220:}
        -:  221:
function _ZL21supportsMMaMatrixTypePN4mlir9OperationEb called 0 returned 0% blocks executed 0%
    #####:  222:static bool supportsMMaMatrixType(Operation *op, bool useNvGpu) {
    #####:  223:  if (isa<scf::ForOp, scf::YieldOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  224:    return true;
    #####:  225:  if (auto transferRead = dyn_cast<vector::TransferReadOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  226:    return transferReadSupportsMMAMatrixType(transferRead, useNvGpu);
call    0 never executed
    #####:  227:  if (auto transferWrite = dyn_cast<vector::TransferWriteOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  228:    return transferWriteSupportsMMAMatrixType(transferWrite);
call    0 never executed
    #####:  229:  if (auto extractStridedSlice = dyn_cast<vector::ExtractStridedSliceOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  230:    return useNvGpu &&
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  231:           extractStridedSliceSupportsMMAMatrixType(extractStridedSlice);
call    0 never executed
    #####:  232:  if (auto contract = dyn_cast<vector::ContractionOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  233:    return contractSupportsMMAMatrixType(contract, useNvGpu);
call    0 never executed
    #####:  234:  if (auto constant = dyn_cast<arith::ConstantOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  235:    return constantSupportsMMAMatrixType(constant);
call    0 never executed
    #####:  236:  if (auto broadcast = dyn_cast<vector::BroadcastOp>(op))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  237:    return broadcastSupportsMMAMatrixType(broadcast);
call    0 never executed
    #####:  238:  return elementwiseSupportsMMAMatrixType(op);
call    0 never executed
        -:  239:}
        -:  240:
        -:  241:/// Return an unsorted slice handling scf.for region differently than
        -:  242:/// `getSlice`. In scf.for we only want to include as part of the slice elements
        -:  243:/// that are part of the use/def chain.
function _ZL16getSliceContractPN4mlir9OperationEN4llvm12function_refIFbS1_EEES5_ called 0 returned 0% blocks executed 0%
    #####:  244:static SetVector<Operation *> getSliceContract(Operation *op,
        -:  245:                                               TransitiveFilter backwardFilter,
        -:  246:                                               TransitiveFilter forwardFilter) {
    #####:  247:  SetVector<Operation *> slice;
call    0 never executed
    #####:  248:  slice.insert(op);
call    0 never executed
    #####:  249:  unsigned currentIndex = 0;
    #####:  250:  SetVector<Operation *> backwardSlice;
call    0 never executed
call    1 never executed
    #####:  251:  SetVector<Operation *> forwardSlice;
call    0 never executed
    #####:  252:  while (currentIndex != slice.size()) {
branch  0 never executed
branch  1 never executed
    #####:  253:    auto *currentOp = (slice)[currentIndex];
call    0 never executed
        -:  254:    // Compute and insert the backwardSlice starting from currentOp.
    #####:  255:    backwardSlice.clear();
call    0 never executed
    #####:  256:    getBackwardSlice(currentOp, &backwardSlice, backwardFilter);
call    0 never executed
    #####:  257:    slice.insert(backwardSlice.begin(), backwardSlice.end());
call    0 never executed
        -:  258:
        -:  259:    // Compute and insert the forwardSlice starting from currentOp.
    #####:  260:    forwardSlice.clear();
call    0 never executed
        -:  261:    // Special case for ForOp, we don't want to include the whole region but
        -:  262:    // only the value using the region arguments.
        -:  263:    // TODO: We should refine this to only care about the region arguments being
        -:  264:    // converted to matrix type.
    #####:  265:    if (auto forOp = dyn_cast<scf::ForOp>(currentOp)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  266:      for (Value forOpResult : forOp.getResults())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  267:        getForwardSlice(forOpResult, &forwardSlice, forwardFilter);
call    0 never executed
    #####:  268:      for (BlockArgument &arg : forOp.getRegionIterArgs())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  269:        getForwardSlice(arg, &forwardSlice, forwardFilter);
call    0 never executed
        -:  270:    } else {
    #####:  271:      getForwardSlice(currentOp, &forwardSlice, forwardFilter);
call    0 never executed
        -:  272:    }
    #####:  273:    slice.insert(forwardSlice.begin(), forwardSlice.end());
call    0 never executed
    #####:  274:    ++currentIndex;
        -:  275:  }
    #####:  276:  return slice;
call    0 never executed
        -:  277:}
        -:  278:
        -:  279:// Analyze slice of operations based on convert op to figure out if the whole
        -:  280:// slice can be converted to MMA operations.
function _ZL14getOpToConvertPN4mlir9OperationEb called 416 returned 100% blocks executed 100%
      416:  281:static SetVector<Operation *> getOpToConvert(mlir::Operation *op,
        -:  282:                                             bool useNvGpu) {
function _ZZL14getOpToConvertPN4mlir9OperationEbENKUlS1_E_clES1_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  283:  auto hasVectorDest = [](Operation *op) {
    #####:  284:    return llvm::any_of(op->getResultTypes(),
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  285:                        [](Type t) { return t.isa<VectorType>(); });
        -:  286:  };
     416*:  287:  auto hasVectorSrc = [](Operation *op) {
    #####:  288:    return llvm::any_of(op->getOperandTypes(),
call    0 never executed
call    1 never executed
        -:  289:                        [](Type t) { return t.isa<VectorType>(); });
        -:  290:  };
      832:  291:  SetVector<Operation *> opToConvert;
call    0 returned 100%
call    1 returned 100%
function _ZZL14getOpToConvertPN4mlir9OperationEbENKUlNS_6vector13ContractionOpEE1_clES3_ called 0 returned 0% blocks executed 0%
     416*:  292:  op->walk([&](vector::ContractionOp contract) {
call    0 returned 100%
    #####:  293:    if (opToConvert.contains(contract.getOperation()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  294:      return;
    #####:  295:    SetVector<Operation *> dependentOps =
    #####:  296:        getSliceContract(contract, hasVectorDest, hasVectorSrc);
call    0 never executed
call    1 never executed
        -:  297:    // If any instruction cannot use MMA matrix type drop the whole
        -:  298:    // chain. MMA matrix are stored in an opaque type so they cannot be used
        -:  299:    // by all operations.
    #####:  300:    if (llvm::any_of(dependentOps, [useNvGpu](Operation *op) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  301:          return !supportsMMaMatrixType(op, useNvGpu);
        -:  302:        }))
    #####:  303:      return;
call    0 never executed
    #####:  304:    opToConvert.insert(dependentOps.begin(), dependentOps.end());
call    0 never executed
        -:  305:  });
        -:  306:  // Sort the operations so that we can convert them in topological order.
      416:  307:  return topologicalSort(opToConvert);
call    0 returned 100%
        -:  308:}
        -:  309:
        -:  310:namespace {
        -:  311:// Transform contract into (m, k)x(k, n)x(m, n) form so that it can be converted
        -:  312:// to MMA matmul.
        -:  313:struct PrepareContractToGPUMMA
        -:  314:    : public OpRewritePattern<vector::ContractionOp> {
        -:  315:  using OpRewritePattern<vector::ContractionOp>::OpRewritePattern;
        -:  316:
function _ZNK12_GLOBAL__N_123PrepareContractToGPUMMA15matchAndRewriteEN4mlir6vector13ContractionOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  317:  LogicalResult matchAndRewrite(vector::ContractionOp op,
        -:  318:                                PatternRewriter &rewriter) const override {
    #####:  319:    Location loc = op.getLoc();
call    0 never executed
    #####:  320:    Value lhs = op.getLhs(), rhs = op.getRhs(), res = op.getAcc();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  321:
        -:  322:    // Set up the parallel/reduction structure in right form.
    #####:  323:    using MapList = ArrayRef<ArrayRef<AffineExpr>>;
    #####:  324:    auto infer = [](MapList m) { return AffineMap::inferFromExprList(m); };
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
call    8 never executed
    #####:  325:    AffineExpr m, n, k;
    #####:  326:    bindDims(rewriter.getContext(), m, n, k);
call    0 never executed
    #####:  327:    static constexpr std::array<int64_t, 2> perm = {1, 0};
    #####:  328:    auto iteratorTypes = op.getIteratorTypes().getValue();
call    0 never executed
call    1 never executed
    #####:  329:    SmallVector<AffineMap, 4> maps = op.getIndexingMapsArray();
call    0 never executed
    #####:  330:    if (!(vector::isParallelIterator(iteratorTypes[0]) &&
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
branch  8 never executed
    #####:  331:          vector::isParallelIterator(iteratorTypes[1]) &&
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  332:          vector::isReductionIterator(iteratorTypes[2])))
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  333:      return failure();
        -:  334:    //
        -:  335:    // Two outer parallel, one inner reduction (matmat flavor).
        -:  336:    //
    #####:  337:    if (maps == infer({{m, k}, {k, n}, {m, n}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
        -:  338:      // This is the classical row-major matmul, nothing to do.
    #####:  339:      return failure();
        -:  340:    }
    #####:  341:    if (maps == infer({{m, k}, {n, k}, {m, n}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  342:      rhs = rewriter.create<vector::TransposeOp>(loc, rhs, perm);
call    0 never executed
    #####:  343:    } else if (maps == infer({{k, m}, {k, n}, {m, n}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  344:      lhs = rewriter.create<vector::TransposeOp>(loc, lhs, perm);
call    0 never executed
    #####:  345:    } else if (maps == infer({{k, m}, {n, k}, {m, n}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  346:      rhs = rewriter.create<vector::TransposeOp>(loc, rhs, perm);
call    0 never executed
call    1 never executed
    #####:  347:      lhs = rewriter.create<vector::TransposeOp>(loc, lhs, perm);
call    0 never executed
    #####:  348:    } else if (maps == infer({{m, k}, {k, n}, {n, m}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  349:      std::swap(rhs, lhs);
call    0 never executed
    #####:  350:      rhs = rewriter.create<vector::TransposeOp>(loc, rhs, perm);
call    0 never executed
call    1 never executed
    #####:  351:      lhs = rewriter.create<vector::TransposeOp>(loc, lhs, perm);
call    0 never executed
    #####:  352:    } else if (maps == infer({{m, k}, {n, k}, {n, m}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  353:      std::swap(rhs, lhs);
call    0 never executed
    #####:  354:      rhs = rewriter.create<vector::TransposeOp>(loc, rhs, perm);
call    0 never executed
    #####:  355:    } else if (maps == infer({{k, m}, {k, n}, {n, m}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  356:      std::swap(lhs, rhs);
call    0 never executed
    #####:  357:      lhs = rewriter.create<vector::TransposeOp>(loc, lhs, perm);
call    0 never executed
    #####:  358:    } else if (maps == infer({{k, m}, {n, k}, {n, m}})) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:  359:      std::swap(lhs, rhs);
        -:  360:    } else {
    #####:  361:      return failure();
        -:  362:    }
    #####:  363:    rewriter.replaceOpWithNewOp<vector::ContractionOp>(
        -:  364:        op, lhs, rhs, res,
    #####:  365:        rewriter.getAffineMapArrayAttr(infer({{m, k}, {k, n}, {m, n}})),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  366:        op.getIteratorTypes());
call    0 never executed
call    1 never executed
    #####:  367:    return success();
branch  0 never executed
branch  1 never executed
        -:  368:  }
        -:  369:};
        -:  370:
        -:  371:// Fold transpose op into the transfer read op. Nvgpu mma.sync op only supports
        -:  372:// row-, column-, and row-major layout for matrixA, matrixB, and matrixC,
        -:  373:// respectively. We can fold the transpose operation when loading the data from
        -:  374:// Shared Memory to registers.
        -:  375:struct CombineTransferReadOpTranspose final
        -:  376:    : public OpRewritePattern<vector::TransposeOp> {
        -:  377:  using OpRewritePattern<vector::TransposeOp>::OpRewritePattern;
        -:  378:
function _ZNK12_GLOBAL__N_130CombineTransferReadOpTranspose15matchAndRewriteEN4mlir6vector11TransposeOpERNS1_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  379:  LogicalResult matchAndRewrite(vector::TransposeOp op,
        -:  380:                                PatternRewriter &rewriter) const override {
    #####:  381:    auto transferReadOp =
    #####:  382:        op.getVector().getDefiningOp<vector::TransferReadOp>();
call    0 never executed
call    1 never executed
    #####:  383:    if (!transferReadOp)
branch  0 never executed
branch  1 never executed
    #####:  384:      return failure();
        -:  385:
        -:  386:    // TODO: support 0-d corner case.
    #####:  387:    if (transferReadOp.getTransferRank() == 0)
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  388:      return failure();
        -:  389:
    #####:  390:    if (transferReadOp.getMask() || transferReadOp.hasOutOfBoundsDim())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  391:      return failure();
    #####:  392:    SmallVector<int64_t, 2> perm;
call    0 never executed
    #####:  393:    op.getTransp(perm);
call    0 never executed
    #####:  394:    SmallVector<unsigned, 2> permU;
branch  0 never executed
branch  1 never executed
    #####:  395:    for (int64_t o : perm)
branch  0 never executed
branch  1 never executed
    #####:  396:      permU.push_back(unsigned(o));
call    0 never executed
    #####:  397:    AffineMap permutationMap =
call    0 never executed
    #####:  398:        AffineMap::getPermutationMap(permU, op.getContext());
call    0 never executed
    #####:  399:    AffineMap newMap =
    #####:  400:        permutationMap.compose(transferReadOp.getPermutationMap());
call    0 never executed
call    1 never executed
    #####:  401:    rewriter.replaceOpWithNewOp<vector::TransferReadOp>(
    #####:  402:        op, op.getType(), transferReadOp.getSource(),
call    0 never executed
call    1 never executed
    #####:  403:        transferReadOp.getIndices(), AffineMapAttr::get(newMap),
call    0 never executed
call    1 never executed
    #####:  404:        transferReadOp.getPadding(), transferReadOp.getMask(),
call    0 never executed
call    1 never executed
    #####:  405:        transferReadOp.getInBoundsAttr());
call    0 never executed
call    1 never executed
    #####:  406:    return success();
branch  0 never executed
branch  1 never executed
        -:  407:  }
        -:  408:};
        -:  409:
        -:  410:} // namespace
        -:  411:
        -:  412:// MMA types have different layout based on how they are used in matmul ops.
        -:  413:// Figure the right layout to use by looking at op uses.
        -:  414:// TODO: Change the GPU dialect to abstract the layout at the this level and
        -:  415:// only care about it during lowering to NVVM.
        -:  416:template <typename OpTy>
    #####:  417:static const char *inferFragType(OpTy op) {
    #####:  418:  for (Operation *users : op->getUsers()) {
    #####:  419:    auto contract = dyn_cast<vector::ContractionOp>(users);
    #####:  420:    if (!contract)
    #####:  421:      continue;
    #####:  422:    if (contract.getLhs() == op.getResult())
    #####:  423:      return "AOp";
    #####:  424:    if (contract.getRhs() == op.getResult())
        -:  425:      return "BOp";
        -:  426:  }
    #####:  427:  return "COp";
        -:  428:}
------------------
_Z13inferFragTypeIN4mlir6vector11BroadcastOpEEPKcT_:
function _Z13inferFragTypeIN4mlir6vector11BroadcastOpEEPKcT_ called 0 returned 0% blocks executed 0%
    #####:  417:static const char *inferFragType(OpTy op) {
    #####:  418:  for (Operation *users : op->getUsers()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  419:    auto contract = dyn_cast<vector::ContractionOp>(users);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  420:    if (!contract)
branch  0 never executed
branch  1 never executed
    #####:  421:      continue;
    #####:  422:    if (contract.getLhs() == op.getResult())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  423:      return "AOp";
    #####:  424:    if (contract.getRhs() == op.getResult())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  425:      return "BOp";
        -:  426:  }
    #####:  427:  return "COp";
        -:  428:}
------------------
_Z13inferFragTypeIN4mlir5arith10ConstantOpEEPKcT_:
function _Z13inferFragTypeIN4mlir5arith10ConstantOpEEPKcT_ called 0 returned 0% blocks executed 0%
    #####:  417:static const char *inferFragType(OpTy op) {
    #####:  418:  for (Operation *users : op->getUsers()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  419:    auto contract = dyn_cast<vector::ContractionOp>(users);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  420:    if (!contract)
branch  0 never executed
branch  1 never executed
    #####:  421:      continue;
    #####:  422:    if (contract.getLhs() == op.getResult())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  423:      return "AOp";
    #####:  424:    if (contract.getRhs() == op.getResult())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  425:      return "BOp";
        -:  426:  }
    #####:  427:  return "COp";
        -:  428:}
------------------
_Z13inferFragTypeIN4mlir6vector14TransferReadOpEEPKcT_:
function _Z13inferFragTypeIN4mlir6vector14TransferReadOpEEPKcT_ called 0 returned 0% blocks executed 0%
    #####:  417:static const char *inferFragType(OpTy op) {
    #####:  418:  for (Operation *users : op->getUsers()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  419:    auto contract = dyn_cast<vector::ContractionOp>(users);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  420:    if (!contract)
branch  0 never executed
branch  1 never executed
    #####:  421:      continue;
    #####:  422:    if (contract.getLhs() == op.getResult())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  423:      return "AOp";
    #####:  424:    if (contract.getRhs() == op.getResult())
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  425:      return "BOp";
        -:  426:  }
    #####:  427:  return "COp";
        -:  428:}
------------------
        -:  429:
function _ZL21convertTransferReadOpN4mlir6vector14TransferReadOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  430:static void convertTransferReadOp(vector::TransferReadOp op,
        -:  431:                                  llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  432:  assert(op.getTransferRank() > 0 && "unexpected 0-d transfer");
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  433:  assert(transferReadSupportsMMAMatrixType(op, /*useNvGpu=*/false));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  434:  Optional<int64_t> stride =
    #####:  435:      getMemrefConstantHorizontalStride(op.getShapedType());
call    0 never executed
call    1 never executed
    #####:  436:  AffineMap map = op.getPermutationMap();
call    0 never executed
        -:  437:  // Handle broadcast by setting the stride to 0.
    #####:  438:  if (map.getResult(0).isa<AffineConstantExpr>()) {
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  439:    assert(map.getResult(0).cast<AffineConstantExpr>().getValue() == 0);
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
    #####:  440:    stride = 0;
branch  0 never executed
branch  1 never executed
        -:  441:  }
    #####:  442:  assert(stride);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  443:  const char *fragType = inferFragType(op);
call    0 never executed
    #####:  444:  gpu::MMAMatrixType type =
branch  0 never executed
branch  1 never executed
    #####:  445:      gpu::MMAMatrixType::get(op.getVectorType().getShape(),
call    0 never executed
    #####:  446:                              op.getVectorType().getElementType(), fragType);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  447:  OpBuilder b(op);
call    0 never executed
    #####:  448:  Value load = b.create<gpu::SubgroupMmaLoadMatrixOp>(
call    0 never executed
    #####:  449:      op.getLoc(), type, op.getSource(), op.getIndices(),
call    0 never executed
call    1 never executed
    #####:  450:      b.getIndexAttr(*stride));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  451:  valueMapping[op.getResult()] = load;
call    0 never executed
    #####:  452:}
        -:  453:
function _ZL22convertTransferWriteOpN4mlir6vector15TransferWriteOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  454:static void convertTransferWriteOp(vector::TransferWriteOp op,
        -:  455:                                   llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  456:  assert(transferWriteSupportsMMAMatrixType(op));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  457:  Optional<int64_t> stride =
    #####:  458:      getMemrefConstantHorizontalStride(op.getShapedType());
call    0 never executed
call    1 never executed
    #####:  459:  assert(stride);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  460:  OpBuilder b(op);
call    0 never executed
    #####:  461:  Value matrix = valueMapping.find(op.getVector())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  462:  b.create<gpu::SubgroupMmaStoreMatrixOp>(op.getLoc(), matrix, op.getSource(),
call    0 never executed
call    1 never executed
    #####:  463:                                          op.getIndices(),
call    0 never executed
    #####:  464:                                          b.getIndexAttr(*stride));
call    0 never executed
call    1 never executed
    #####:  465:  op.erase();
call    0 never executed
    #####:  466:}
        -:  467:
        -:  468:/// Returns the vector type which represents a matrix fragment.
        -:  469:static VectorType
function _ZL27getMmaSyncVectorOperandTypeRKN4mlir5nvgpu19FragmentElementInfoE called 0 returned 0% blocks executed 0%
    #####:  470:getMmaSyncVectorOperandType(const nvgpu::FragmentElementInfo &regInfo) {
    #####:  471:  SmallVector<int64_t> shape{regInfo.numRegistersPerFragment,
    #####:  472:                             regInfo.elementsPerRegister};
call    0 never executed
    #####:  473:  Type elType = regInfo.registerLLVMType;
    #####:  474:  if (auto vecType = elType.dyn_cast<VectorType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  475:    elType = vecType.getElementType();
call    0 never executed
    #####:  476:  return VectorType::get(shape, elType);
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  477:}
        -:  478:
        -:  479:/// Convert a 2D splat ConstantOp to a SubgroupMmaConstantMatrix op.
        -:  480:static LogicalResult
function _ZL24convertConstantOpMmaSyncN4mlir5arith10ConstantOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  481:convertConstantOpMmaSync(arith::ConstantOp op,
        -:  482:                         llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  483:  OpBuilder b(op);
call    0 never executed
    #####:  484:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  485:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  486:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  487:    return failure();
        -:  488:
    #####:  489:  FailureOr<nvgpu::FragmentElementInfo> regInfo =
call    0 never executed
    #####:  490:      nvgpu::getMmaSyncRegisterType(*warpMatrixInfo);
call    0 never executed
    #####:  491:  if (failed(regInfo))
branch  0 never executed
branch  1 never executed
    #####:  492:    return failure();
        -:  493:
    #####:  494:  VectorType vectorType = getMmaSyncVectorOperandType(*regInfo);
call    0 never executed
    #####:  495:  auto dense = op.getValue().dyn_cast<SplatElementsAttr>();
call    0 never executed
call    1 never executed
    #####:  496:  if (!dense)
branch  0 never executed
branch  1 never executed
    #####:  497:    return failure();
    #####:  498:  Value result = b.create<arith::ConstantOp>(
        -:  499:      op.getLoc(), vectorType,
    #####:  500:      DenseElementsAttr::get(vectorType, dense.getSplatValue<Attribute>()));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  501:  valueMapping[op.getResult()] = result;
call    0 never executed
call    1 never executed
    #####:  502:  return success();
        -:  503:}
        -:  504:
        -:  505:static LogicalResult
function _ZL28creatLdMatrixCompatibleLoadsN4mlir6vector14TransferReadOpERNS_9OpBuilderERN4llvm8DenseMapINS_5ValueES6_NS4_12DenseMapInfoIS6_vEENS4_6detail12DenseMapPairIS6_S6_EEEE called 0 returned 0% blocks executed 0%
    #####:  506:creatLdMatrixCompatibleLoads(vector::TransferReadOp op, OpBuilder &builder,
        -:  507:                             llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  508:  Location loc = op->getLoc();
call    0 never executed
        -:  509:
    #####:  510:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  511:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  512:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  513:    return failure();
        -:  514:
    #####:  515:  FailureOr<nvgpu::FragmentElementInfo> regInfo =
call    0 never executed
    #####:  516:      nvgpu::getMmaSyncRegisterType(*warpMatrixInfo);
call    0 never executed
    #####:  517:  if (failed(regInfo))
branch  0 never executed
branch  1 never executed
    #####:  518:    return failure();
        -:  519:
    #####:  520:  FailureOr<nvgpu::LdMatrixParams> params = nvgpu::getLdMatrixParams(
    #####:  521:      *warpMatrixInfo,
    #####:  522:      /*transpose=*/!op.getPermutationMap().isMinorIdentity());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  523:  if (failed(params)) {
branch  0 never executed
branch  1 never executed
    #####:  524:    return op->emitError()
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  525:           << "failed to convert vector.transfer_read to ldmatrix; this op "
        -:  526:              "likely "
    #####:  527:              "should not be converted to a nvgpu.ldmatrix call.";
call    0 never executed
        -:  528:  }
        -:  529:
        -:  530:  // Adjust the load offset.
    #####:  531:  auto laneId = builder.create<gpu::LaneIdOp>(loc);
call    0 never executed
    #####:  532:  FailureOr<AffineMap> offsets =
branch  0 never executed
branch  1 never executed
    #####:  533:      nvgpu::getLaneIdToLdMatrixMatrixCoord(loc, builder, *params);
call    0 never executed
    #####:  534:  if (failed(offsets))
branch  0 never executed
branch  1 never executed
    #####:  535:    return failure();
        -:  536:
    #####:  537:  VectorType vectorType = getMmaSyncVectorOperandType(*regInfo);
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  538:
    #####:  539:  SmallVector<Value, 4> indices;
call    0 never executed
    #####:  540:  getXferIndices<vector::TransferReadOp>(builder, op, *offsets, {laneId},
call    0 never executed
    #####:  541:                                         indices);
call    0 never executed
    #####:  542:  nvgpu::LdMatrixOp newOp = builder.create<nvgpu::LdMatrixOp>(
branch  0 never executed
branch  1 never executed
    #####:  543:      loc, vectorType, op.getSource(), indices,
    #####:  544:      !op.getPermutationMap().isMinorIdentity(), params->numTiles);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  545:  valueMapping[op] = newOp->getResult(0);
call    0 never executed
    #####:  546:  return success();
branch  0 never executed
branch  1 never executed
        -:  547:}
        -:  548:
        -:  549:static LogicalResult
function _ZL22createNonLdMatrixLoadsN4mlir6vector14TransferReadOpERNS_9OpBuilderERN4llvm8DenseMapINS_5ValueES6_NS4_12DenseMapInfoIS6_vEENS4_6detail12DenseMapPairIS6_S6_EEEE called 0 returned 0% blocks executed 0%
    #####:  550:createNonLdMatrixLoads(vector::TransferReadOp op, OpBuilder &builder,
        -:  551:                       llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  552:  Location loc = op.getLoc();
call    0 never executed
    #####:  553:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  554:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  555:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  556:    return failure();
    #####:  557:  FailureOr<nvgpu::FragmentElementInfo> regInfo =
call    0 never executed
    #####:  558:      nvgpu::getMmaSyncRegisterType(*warpMatrixInfo);
call    0 never executed
    #####:  559:  if (failed(regInfo)) {
branch  0 never executed
branch  1 never executed
    #####:  560:    op->emitError() << "Failed to deduce register fragment type during "
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  561:                       "conversion to distributed non-ldmatrix compatible load";
call    0 never executed
    #####:  562:    return failure();
        -:  563:  }
        -:  564:
    #####:  565:  Value laneId = builder.create<gpu::LaneIdOp>(loc);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  566:  SmallVector<Value, 4> elements;
branch  0 never executed
branch  1 never executed
        -:  567:
        -:  568:  // This is the individual element type.
    #####:  569:  Type loadedElType = regInfo->registerLLVMType;
branch  0 never executed
branch  1 never executed
    #####:  570:  VectorType vectorType = getMmaSyncVectorOperandType(*regInfo);
call    0 never executed
        -:  571:
    #####:  572:  Value fill = builder.create<arith::ConstantOp>(
    #####:  573:      op.getLoc(), vectorType.getElementType(),
call    0 never executed
    #####:  574:      builder.getZeroAttr(vectorType.getElementType()));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  575:  Value result = builder.create<vector::SplatOp>(op.getLoc(), fill, vectorType);
call    0 never executed
call    1 never executed
        -:  576:
    #####:  577:  bool isTransposeLoad = !op.getPermutationMap().isMinorIdentity();
call    0 never executed
call    1 never executed
        -:  578:
        -:  579:  // If we are not transposing, then we can use vectorized loads. Otherwise, we
        -:  580:  // must load each element individually.
    #####:  581:  if (!isTransposeLoad) {
branch  0 never executed
branch  1 never executed
    #####:  582:    if (!loadedElType.isa<VectorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  583:      loadedElType = VectorType::get({1}, loadedElType);
call    0 never executed
        -:  584:    }
        -:  585:
    #####:  586:    for (int i = 0; i < vectorType.getShape()[0]; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  587:      FailureOr<AffineMap> coords = nvgpu::getLaneIdAndValueIdToOperandCoord(
branch  0 never executed
branch  1 never executed
    #####:  588:          op.getLoc(), builder, *warpMatrixInfo);
call    0 never executed
    #####:  589:      if (failed(coords))
branch  0 never executed
branch  1 never executed
    #####:  590:        return failure();
    #####:  591:      Value logicalValueId = builder.create<arith::ConstantOp>(
    #####:  592:          loc, builder.getIndexType(),
call    0 never executed
    #####:  593:          builder.getIndexAttr(i * regInfo->elementsPerRegister));
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  594:      SmallVector<Value, 4> newIndices;
call    0 never executed
    #####:  595:      getXferIndices<vector::TransferReadOp>(
call    0 never executed
    #####:  596:          builder, op, *coords, {laneId, logicalValueId}, newIndices);
call    0 never executed
        -:  597:
    #####:  598:      Value el = builder.create<vector::LoadOp>(loc, loadedElType,
    #####:  599:                                                op.getSource(), newIndices);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  600:      result = builder.create<vector::InsertOp>(loc, el, result,
branch  0 never executed
branch  1 never executed
    #####:  601:                                                builder.getI64ArrayAttr(i));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  602:    }
        -:  603:  } else {
    #####:  604:    if (auto vecType = loadedElType.dyn_cast<VectorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  605:      loadedElType = vecType.getElementType();
call    0 never executed
        -:  606:    }
    #####:  607:    for (int i = 0; i < vectorType.getShape()[0]; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  608:      for (unsigned innerIdx = 0; innerIdx < vectorType.getShape()[1];
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
        -:  609:           innerIdx++) {
        -:  610:
    #####:  611:        Value logicalValueId = builder.create<arith::ConstantOp>(
    #####:  612:            loc, builder.getIndexType(),
call    0 never executed
    #####:  613:            builder.getIndexAttr(i * regInfo->elementsPerRegister + innerIdx));
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  614:        FailureOr<AffineMap> coords = nvgpu::getLaneIdAndValueIdToOperandCoord(
branch  0 never executed
branch  1 never executed
    #####:  615:            op.getLoc(), builder, *warpMatrixInfo);
call    0 never executed
    #####:  616:        if (failed(coords))
branch  0 never executed
branch  1 never executed
    #####:  617:          return failure();
        -:  618:
    #####:  619:        SmallVector<Value, 4> newIndices;
call    0 never executed
    #####:  620:        getXferIndices<vector::TransferReadOp>(
call    0 never executed
    #####:  621:            builder, op, *coords, {laneId, logicalValueId}, newIndices);
call    0 never executed
    #####:  622:        Value el = builder.create<memref::LoadOp>(op.getLoc(), loadedElType,
    #####:  623:                                                  op.getSource(), newIndices);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  624:        result = builder.create<vector::InsertOp>(
branch  0 never executed
branch  1 never executed
    #####:  625:            op.getLoc(), el, result, builder.getI64ArrayAttr({i, innerIdx}));
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  626:      }
        -:  627:    }
        -:  628:  }
        -:  629:
    #####:  630:  valueMapping[op.getResult()] = result;
call    0 never executed
    #####:  631:  return success();
branch  0 never executed
branch  1 never executed
        -:  632:}
        -:  633:
        -:  634:/// Converts a `vector.transfer_read` operation directly to either a
        -:  635:/// `vector.load` or a `nvgpu.ldmatrix` operation. This function should only be
        -:  636:/// used when converting to `nvgpu.mma.sync` operations.
        -:  637:static LogicalResult
function _ZL26convertTransferReadToLoadsN4mlir6vector14TransferReadOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  638:convertTransferReadToLoads(vector::TransferReadOp op,
        -:  639:                           llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  640:  OpBuilder b(op);
call    0 never executed
        -:  641:
    #####:  642:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  643:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  644:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  645:    return failure();
        -:  646:
    #####:  647:  bool isLdMatrixCompatible =
    #####:  648:      op.getSource().getType().cast<MemRefType>().getMemorySpaceAsInt() == 3 &&
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
branch  7 never executed
    #####:  649:      nvgpu::inferTileWidthInBits(*warpMatrixInfo) == 128;
call    0 never executed
        -:  650:
    #####:  651:  VectorType vecTy = op.getVectorType();
call    0 never executed
    #####:  652:  int64_t bitWidth = vecTy.getElementType().getIntOrFloatBitWidth();
call    0 never executed
call    1 never executed
        -:  653:
        -:  654:  // When we are transposing the B operand, ldmatrix will only work if we have
        -:  655:  // at least 8 rows to read and the width to read for the transpose is 128
        -:  656:  // bits.
    #####:  657:  if (!op.getPermutationMap().isMinorIdentity() &&
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  658:      (bitWidth != 16 || vecTy.getDimSize(1) < 8 ||
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  659:       vecTy.getDimSize(0) * bitWidth < 128))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  660:    isLdMatrixCompatible = false;
        -:  661:
    #####:  662:  if (!isLdMatrixCompatible)
branch  0 never executed
branch  1 never executed
    #####:  663:    return createNonLdMatrixLoads(op, b, valueMapping);
call    0 never executed
        -:  664:
    #####:  665:  return creatLdMatrixCompatibleLoads(op, b, valueMapping);
call    0 never executed
        -:  666:}
        -:  667:
        -:  668:static LogicalResult
function _ZL28convertTransferWriteToStoresN4mlir6vector15TransferWriteOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  669:convertTransferWriteToStores(vector::TransferWriteOp op,
        -:  670:                             llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  671:  OpBuilder b(op);
call    0 never executed
    #####:  672:  Location loc = op->getLoc();
call    0 never executed
    #####:  673:  Value matrix = valueMapping.find(op.getVector())->second;
call    0 never executed
call    1 never executed
call    2 never executed
        -:  674:
    #####:  675:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  676:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  677:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  678:    return failure();
    #####:  679:  FailureOr<nvgpu::FragmentElementInfo> regInfo =
call    0 never executed
    #####:  680:      nvgpu::getMmaSyncRegisterType(*warpMatrixInfo);
call    0 never executed
    #####:  681:  if (failed(regInfo))
branch  0 never executed
branch  1 never executed
    #####:  682:    return failure();
        -:  683:
    #####:  684:  VectorType vectorType = getMmaSyncVectorOperandType(*regInfo);
call    0 never executed
    #####:  685:  Value laneId = b.create<gpu::LaneIdOp>(loc);
call    0 never executed
        -:  686:
    #####:  687:  for (unsigned i = 0; i < vectorType.getShape()[0]; i++) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  688:    Value logicalValueId = b.create<arith::ConstantOp>(
    #####:  689:        loc, b.getIndexType(),
call    0 never executed
    #####:  690:        b.getIndexAttr(i * regInfo->elementsPerRegister));
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  691:    FailureOr<AffineMap> coords = nvgpu::getLaneIdAndValueIdToOperandCoord(
branch  0 never executed
branch  1 never executed
    #####:  692:        op.getLoc(), b, *warpMatrixInfo);
call    0 never executed
    #####:  693:    if (failed(coords))
branch  0 never executed
branch  1 never executed
    #####:  694:      return failure();
        -:  695:
    #####:  696:    Value el = b.create<vector::ExtractOp>(loc, matrix, ArrayRef<int64_t>{i});
call    0 never executed
call    1 never executed
    #####:  697:    SmallVector<Value, 4> newIndices;
call    0 never executed
    #####:  698:    getXferIndices<vector::TransferWriteOp>(
call    0 never executed
    #####:  699:        b, op, *coords, {laneId, logicalValueId}, newIndices);
call    0 never executed
    #####:  700:    b.create<vector::StoreOp>(loc, el, op.getSource(), newIndices);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  701:  }
    #####:  702:  op->erase();
call    0 never executed
    #####:  703:  return success();
        -:  704:}
        -:  705:
function _ZL26populateFromInt64AttrArrayN4mlir9ArrayAttrERN4llvm15SmallVectorImplIlEE called 0 returned 0% blocks executed 0%
    #####:  706:static void populateFromInt64AttrArray(ArrayAttr arrayAttr,
        -:  707:                                       SmallVectorImpl<int64_t> &results) {
    #####:  708:  for (auto attr : arrayAttr)
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  709:    results.push_back(attr.cast<IntegerAttr>().getInt());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  710:}
        -:  711:
        -:  712:static LogicalResult
function _ZL26convertExtractStridedSliceN4mlir6vector21ExtractStridedSliceOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  713:convertExtractStridedSlice(vector::ExtractStridedSliceOp op,
        -:  714:                           llvm::DenseMap<Value, Value> &valueMapping) {
        -:  715:
    #####:  716:  OpBuilder b(op);
call    0 never executed
    #####:  717:  Location loc = op->getLoc();
call    0 never executed
        -:  718:
    #####:  719:  FailureOr<nvgpu::WarpMatrixInfo> warpMatrixInfo =
    #####:  720:      nvgpu::getWarpMatrixInfo(op);
call    0 never executed
    #####:  721:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  722:    return failure();
        -:  723:
    #####:  724:  FailureOr<nvgpu::FragmentElementInfo> mmaSyncFragmentInfo =
call    0 never executed
    #####:  725:      nvgpu::getMmaSyncRegisterType(*warpMatrixInfo);
call    0 never executed
    #####:  726:  if (failed(mmaSyncFragmentInfo))
branch  0 never executed
branch  1 never executed
    #####:  727:    return failure();
        -:  728:
        -:  729:  // Find the vector.transer_read whose result vector is being sliced.
    #####:  730:  auto transferReadOp = op.getVector().getDefiningOp<vector::TransferReadOp>();
call    0 never executed
call    1 never executed
    #####:  731:  if (!transferReadOp)
branch  0 never executed
branch  1 never executed
    #####:  732:    return failure();
        -:  733:
    #####:  734:  warpMatrixInfo = nvgpu::getWarpMatrixInfo(transferReadOp);
call    0 never executed
    #####:  735:  if (failed(warpMatrixInfo))
branch  0 never executed
branch  1 never executed
    #####:  736:    return failure();
        -:  737:
    #####:  738:  FailureOr<nvgpu::FragmentElementInfo> ldFragmentInfo =
call    0 never executed
    #####:  739:      nvgpu::getMmaSyncRegisterType(*warpMatrixInfo);
call    0 never executed
    #####:  740:  if (failed(ldFragmentInfo))
branch  0 never executed
branch  1 never executed
    #####:  741:    return failure();
        -:  742:
    #####:  743:  assert(
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  744:      (mmaSyncFragmentInfo->elementsPerRegister ==
        -:  745:       ldFragmentInfo->elementsPerRegister) &&
        -:  746:      "Number of elements per register should be same for load and mma.sync");
        -:  747:
        -:  748:  // Create vector.extract_strided_slice op for thread-owned fragments.
    #####:  749:  std::array<int64_t, 2> strides = {1,
        -:  750:                                    1}; // stride for extract slice is always 1.
    #####:  751:  std::array<int64_t, 2> sliceShape = {
call    0 never executed
    #####:  752:      mmaSyncFragmentInfo->numRegistersPerFragment,
    #####:  753:      mmaSyncFragmentInfo->elementsPerRegister};
    #####:  754:  auto sourceVector = valueMapping.find(transferReadOp)->second;
call    0 never executed
call    1 never executed
        -:  755:
        -:  756:  // offset and sizes at warp-level of onwership.
    #####:  757:  SmallVector<int64_t> offsets;
call    0 never executed
    #####:  758:  populateFromInt64AttrArray(op.getOffsets(), offsets);
call    0 never executed
call    1 never executed
        -:  759:
    #####:  760:  SmallVector<int64_t> sizes;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  761:  populateFromInt64AttrArray(op.getSizes(), sizes);
call    0 never executed
call    1 never executed
    #####:  762:  ArrayRef<int64_t> warpVectorShape = op.getVectorType().getShape();
call    0 never executed
call    1 never executed
        -:  763:
        -:  764:  // Compute offset in vector registers. Note that the mma.sync vector registers
        -:  765:  // are shaped as numberOfFragments x numberOfRegistersPerfFragment. The vector
        -:  766:  // registers can only be sliced along numberOfFragments, i.e., sliceOffset[0].
    #####:  767:  std::array<int64_t, 2> sliceOffset = {0, 0};
        -:  768:
    #####:  769:  if (offsets[0] && offsets[1])
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  770:    return op->emitError() << "Slicing fragments in 2D is not supported. ";
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  771:  else if (offsets[0])
branch  0 never executed
branch  1 never executed
    #####:  772:    sliceOffset[0] = (warpVectorShape[0] / offsets[0]);
branch  0 never executed
branch  1 never executed
    #####:  773:  else if (offsets[1])
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  774:    sliceOffset[0] = (warpVectorShape[1] / offsets[1]);
branch  0 never executed
branch  1 never executed
        -:  775:
    #####:  776:  Value newOp = b.create<vector::ExtractStridedSliceOp>(
    #####:  777:      loc, sourceVector, sliceOffset, sliceShape, strides);
call    0 never executed
call    1 never executed
        -:  778:
    #####:  779:  valueMapping[op] = newOp;
call    0 never executed
    #####:  780:  return success();
branch  0 never executed
branch  1 never executed
        -:  781:}
        -:  782:
function _ZL17convertContractOpN4mlir6vector13ContractionOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  783:static void convertContractOp(vector::ContractionOp op,
        -:  784:                              llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  785:  OpBuilder b(op);
call    0 never executed
    #####:  786:  Value opA = valueMapping.find(op.getLhs())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  787:  Value opB = valueMapping.find(op.getRhs())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  788:  Value opC = valueMapping.find(op.getAcc())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  789:  Value matmul = b.create<gpu::SubgroupMmaComputeOp>(op.getLoc(), opC.getType(),
call    0 never executed
    #####:  790:                                                     opA, opB, opC);
call    0 never executed
call    1 never executed
    #####:  791:  valueMapping[op.getResult()] = matmul;
call    0 never executed
    #####:  792:}
        -:  793:
        -:  794:static LogicalResult
function _ZL26convertContractOpToMmaSyncN4mlir6vector13ContractionOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  795:convertContractOpToMmaSync(vector::ContractionOp op,
        -:  796:                           llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  797:  OpBuilder b(op);
call    0 never executed
    #####:  798:  Value opA = valueMapping.find(op.getLhs())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  799:  Value opB = valueMapping.find(op.getRhs())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  800:  Value opC = valueMapping.find(op.getAcc())->second;
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  801:  int64_t m = op.getLhs().getType().cast<VectorType>().getShape()[0];
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  802:  int64_t n = op.getRhs().getType().cast<VectorType>().getShape()[0];
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  803:  int64_t k = op.getLhs().getType().cast<VectorType>().getShape()[1];
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  804:  Value matmul = b.create<nvgpu::MmaSyncOp>(op.getLoc(), opA, opB, opC,
    #####:  805:                                            b.getI64ArrayAttr({m, n, k}));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  806:  valueMapping[op.getResult()] = matmul;
call    0 never executed
    #####:  807:  return success();
        -:  808:}
        -:  809:
        -:  810:/// Convert a 2D splat ConstantOp to a SubgroupMmaConstantMatrix op.
function _ZL17convertConstantOpN4mlir5arith10ConstantOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  811:static void convertConstantOp(arith::ConstantOp op,
        -:  812:                              llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  813:  assert(constantSupportsMMAMatrixType(op));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  814:  OpBuilder b(op);
call    0 never executed
    #####:  815:  auto splat =
    #####:  816:      op.getValue().cast<SplatElementsAttr>().getSplatValue<TypedAttr>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  817:  auto scalarConstant =
    #####:  818:      b.create<arith::ConstantOp>(op.getLoc(), splat.getType(), splat);
call    0 never executed
call    1 never executed
    #####:  819:  const char *fragType = inferFragType(op);
call    0 never executed
    #####:  820:  auto vecType = op.getType().cast<VectorType>();
call    0 never executed
    #####:  821:  gpu::MMAMatrixType type = gpu::MMAMatrixType::get(
branch  0 never executed
branch  1 never executed
    #####:  822:      vecType.getShape(), vecType.getElementType(), llvm::StringRef(fragType));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  823:  auto matrix = b.create<gpu::SubgroupMmaConstantMatrixOp>(op.getLoc(), type,
    #####:  824:                                                           scalarConstant);
call    0 never executed
    #####:  825:  valueMapping[op.getResult()] = matrix;
call    0 never executed
call    1 never executed
    #####:  826:}
        -:  827:
        -:  828:/// Convert a vector.broadcast from scalar to a SubgroupMmaConstantMatrix op.
function _ZL18convertBroadcastOpN4mlir6vector11BroadcastOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  829:static void convertBroadcastOp(vector::BroadcastOp op,
        -:  830:                               llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  831:  assert(broadcastSupportsMMAMatrixType(op));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  832:  OpBuilder b(op);
call    0 never executed
    #####:  833:  const char *fragType = inferFragType(op);
call    0 never executed
    #####:  834:  auto vecType = op.getVectorType();
call    0 never executed
    #####:  835:  gpu::MMAMatrixType type = gpu::MMAMatrixType::get(
branch  0 never executed
branch  1 never executed
    #####:  836:      vecType.getShape(), vecType.getElementType(), llvm::StringRef(fragType));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  837:  auto matrix = b.create<gpu::SubgroupMmaConstantMatrixOp>(op.getLoc(), type,
    #####:  838:                                                           op.getSource());
call    0 never executed
call    1 never executed
    #####:  839:  valueMapping[op.getResult()] = matrix;
call    0 never executed
    #####:  840:}
        -:  841:
        -:  842:// Replace ForOp with a new ForOp with extra operands. The YieldOp is not
        -:  843:// updated and needs to be updated separatly for the loop to be correct.
function _ZL28replaceForOpWithNewSignatureRN4mlir9OpBuilderENS_3scf5ForOpENS_10ValueRangeE called 0 returned 0% blocks executed 0%
    #####:  844:static scf::ForOp replaceForOpWithNewSignature(OpBuilder &b, scf::ForOp loop,
        -:  845:                                               ValueRange newIterOperands) {
        -:  846:  // Create a new loop before the existing one, with the extra operands.
    #####:  847:  OpBuilder::InsertionGuard g(b);
call    0 never executed
    #####:  848:  b.setInsertionPoint(loop);
call    0 never executed
    #####:  849:  auto operands = llvm::to_vector<4>(loop.getIterOperands());
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  850:  operands.append(newIterOperands.begin(), newIterOperands.end());
call    0 never executed
    #####:  851:  scf::ForOp newLoop =
    #####:  852:      b.create<scf::ForOp>(loop.getLoc(), loop.getLowerBound(),
call    0 never executed
    #####:  853:                           loop.getUpperBound(), loop.getStep(), operands);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  854:  newLoop.getBody()->erase();
call    0 never executed
call    1 never executed
    #####:  855:  newLoop.getLoopBody().getBlocks().splice(
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  856:      newLoop.getLoopBody().getBlocks().begin(),
call    0 never executed
call    1 never executed
    #####:  857:      loop.getLoopBody().getBlocks());
call    0 never executed
call    1 never executed
    #####:  858:  for (Value operand : newIterOperands)
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  859:    newLoop.getBody()->addArgument(operand.getType(), operand.getLoc());
call    0 never executed
call    1 never executed
call    2 never executed
        -:  860:
    #####:  861:  for (auto it : llvm::zip(loop.getResults(), newLoop.getResults().take_front(
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  862:                                                  loop.getNumResults())))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  863:    std::get<0>(it).replaceAllUsesWith(std::get<1>(it));
call    0 never executed
    #####:  864:  loop.erase();
call    0 never executed
    #####:  865:  return newLoop;
branch  0 never executed
branch  1 never executed
        -:  866:}
        -:  867:
function _ZL12convertForOpN4mlir3scf5ForOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  868:static void convertForOp(scf::ForOp op,
        -:  869:                         llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  870:  SmallVector<Value> newOperands;
call    0 never executed
    #####:  871:  SmallVector<std::pair<size_t, size_t>> argMapping;
branch  0 never executed
branch  1 never executed
    #####:  872:  for (const auto &operand : llvm::enumerate(op.getIterOperands())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
    #####:  873:    auto it = valueMapping.find(operand.value());
call    0 never executed
    #####:  874:    if (it == valueMapping.end())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  875:      continue;
    #####:  876:    argMapping.push_back(std::make_pair(
call    0 never executed
    #####:  877:        operand.index(), op.getNumIterOperands() + newOperands.size()));
call    0 never executed
call    1 never executed
    #####:  878:    newOperands.push_back(it->second);
call    0 never executed
call    1 never executed
        -:  879:  }
    #####:  880:  OpBuilder b(op);
call    0 never executed
    #####:  881:  scf::ForOp newForOp = replaceForOpWithNewSignature(b, op, newOperands);
call    0 never executed
call    1 never executed
    #####:  882:  Block &loopBody = *newForOp.getBody();
call    0 never executed
    #####:  883:  for (auto mapping : argMapping) {
branch  0 never executed
branch  1 never executed
    #####:  884:    valueMapping[newForOp.getResult(mapping.first)] =
call    0 never executed
    #####:  885:        newForOp.getResult(mapping.second);
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  886:    valueMapping[loopBody.getArgument(mapping.first +
    #####:  887:                                      newForOp.getNumInductionVars())] =
call    0 never executed
    #####:  888:        loopBody.getArgument(mapping.second + newForOp.getNumInductionVars());
call    0 never executed
        -:  889:  }
    #####:  890:}
        -:  891:
function _ZL14convertYieldOpN4mlir3scf7YieldOpERN4llvm8DenseMapINS_5ValueES4_NS2_12DenseMapInfoIS4_vEENS2_6detail12DenseMapPairIS4_S4_EEEE called 0 returned 0% blocks executed 0%
    #####:  892:static void convertYieldOp(scf::YieldOp op,
        -:  893:                           llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  894:  OpBuilder b(op);
call    0 never executed
    #####:  895:  auto loop = cast<scf::ForOp>(op->getParentOp());
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  896:  auto yieldOperands = llvm::to_vector<4>(op.getOperands());
call    0 never executed
call    1 never executed
    #####:  897:  for (const auto &operand : llvm::enumerate(op.getOperands())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
    #####:  898:    auto it = valueMapping.find(operand.value());
call    0 never executed
    #####:  899:    if (it == valueMapping.end())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  900:      continue;
        -:  901:    // Replace the yield of old value with the for op argument to make it easier
        -:  902:    // to remove the dead code.
    #####:  903:    yieldOperands[operand.index()] = loop.getIterOperands()[operand.index()];
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  904:    yieldOperands.push_back(it->second);
call    0 never executed
call    1 never executed
        -:  905:  }
    #####:  906:  b.create<scf::YieldOp>(op.getLoc(), yieldOperands);
call    0 never executed
    #####:  907:  op.erase();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  908:}
        -:  909:
        -:  910:/// Convert an elementwise op to the equivalent elementwise op on MMA matrix.
function _ZL20convertElementwiseOpPN4mlir9OperationENS_3gpu16MMAElementwiseOpERN4llvm8DenseMapINS_5ValueES6_NS4_12DenseMapInfoIS6_vEENS4_6detail12DenseMapPairIS6_S6_EEEE called 0 returned 0% blocks executed 0%
    #####:  911:static void convertElementwiseOp(Operation *op, gpu::MMAElementwiseOp opType,
        -:  912:                                 llvm::DenseMap<Value, Value> &valueMapping) {
    #####:  913:  OpBuilder b(op);
call    0 never executed
    #####:  914:  SmallVector<Value> matrixOperands;
call    0 never executed
    #####:  915:  for (Value operand : op->getOperands())
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  916:    matrixOperands.push_back(valueMapping.find(operand)->second);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  917:  Value newOp = b.create<gpu::SubgroupMmaElementwiseOp>(
branch  0 never executed
branch  1 never executed
    #####:  918:      op->getLoc(), matrixOperands[0].getType(), matrixOperands, opType);
call    0 never executed
call    1 never executed
    #####:  919:  valueMapping[op->getResult(0)] = newOp;
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  920:}
        -:  921:
function _ZN4mlir34populatePrepareVectorToMMAPatternsERNS_17RewritePatternSetEb called 416 returned 100% blocks executed 67%
      416:  922:void mlir::populatePrepareVectorToMMAPatterns(RewritePatternSet &patterns,
        -:  923:                                              bool useNvGpu) {
      416:  924:  if (!useNvGpu) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
      416:  925:    patterns.add<PrepareContractToGPUMMA, CombineTransferReadOpTranspose>(
      416:  926:        patterns.getContext());
call    0 returned 100%
      416:  927:    return;
        -:  928:  }
    #####:  929:  patterns
        -:  930:      .add<nvgpu::PrepareContractToGPUMMASync, CombineTransferReadOpTranspose>(
    #####:  931:          patterns.getContext());
call    0 never executed
        -:  932:}
        -:  933:
function _ZN4mlir21convertVectorToMMAOpsEPNS_9OperationE called 416 returned 100% blocks executed 18%
      416:  934:void mlir::convertVectorToMMAOps(Operation *rootOp) {
      416:  935:  SetVector<Operation *> ops = getOpToConvert(rootOp, /*useNvGpu=*/false);
call    0 returned 100%
      832:  936:  llvm::DenseMap<Value, Value> valueMapping;
call    0 returned 100%
call    1 returned 100%
     416*:  937:  for (Operation *op : ops) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 never executed
    #####:  938:    if (auto transferRead = dyn_cast<vector::TransferReadOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  939:      convertTransferReadOp(transferRead, valueMapping);
call    0 never executed
    #####:  940:    } else if (auto transferWrite = dyn_cast<vector::TransferWriteOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  941:      convertTransferWriteOp(transferWrite, valueMapping);
call    0 never executed
    #####:  942:    } else if (auto contractOp = dyn_cast<vector::ContractionOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  943:      convertContractOp(contractOp, valueMapping);
call    0 never executed
    #####:  944:    } else if (auto constantOp = dyn_cast<arith::ConstantOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  945:      convertConstantOp(constantOp, valueMapping);
call    0 never executed
    #####:  946:    } else if (auto broadcastOp = dyn_cast<vector::BroadcastOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  947:      convertBroadcastOp(broadcastOp, valueMapping);
call    0 never executed
    #####:  948:    } else if (auto forOp = dyn_cast<scf::ForOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  949:      convertForOp(forOp, valueMapping);
call    0 never executed
    #####:  950:    } else if (auto yiledOp = dyn_cast<scf::YieldOp>(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  951:      convertYieldOp(yiledOp, valueMapping);
call    0 never executed
    #####:  952:    } else if (auto elementwiseType = convertElementwiseOpToMMA(op)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  953:      convertElementwiseOp(op, *elementwiseType, valueMapping);
call    0 never executed
        -:  954:    }
        -:  955:  }
      416:  956:}
        -:  957:
function _ZN4mlir36convertVectorToNVVMCompatibleMMASyncEPNS_9OperationE called 0 returned 0% blocks executed 0%
    #####:  958:LogicalResult mlir::convertVectorToNVVMCompatibleMMASync(Operation *rootOp) {
    #####:  959:  SetVector<Operation *> ops = getOpToConvert(rootOp, /*useNvGpu=*/true);
call    0 never executed
    #####:  960:  llvm::DenseMap<Value, Value> valueMapping;
call    0 never executed
call    1 never executed
    #####:  961:  for (Operation *op : ops) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  962:    if (llvm::TypeSwitch<Operation *, LogicalResult>(op)
call    0 never executed
    #####:  963:            .Case([&](vector::TransferReadOp transferReadOp) {
    #####:  964:              return convertTransferReadToLoads(transferReadOp, valueMapping);
call    0 never executed
    #####:  965:            })
call    0 never executed
    #####:  966:            .Case([&](vector::TransferWriteOp transferWriteOp) {
    #####:  967:              return convertTransferWriteToStores(transferWriteOp,
    #####:  968:                                                  valueMapping);
call    0 never executed
    #####:  969:            })
call    0 never executed
    #####:  970:            .Case([&](vector::ExtractStridedSliceOp extractStridedSliceOp) {
    #####:  971:              return convertExtractStridedSlice(extractStridedSliceOp,
    #####:  972:                                                valueMapping);
call    0 never executed
    #####:  973:            })
call    0 never executed
    #####:  974:            .Case([&](vector::ContractionOp contractionOp) {
    #####:  975:              return convertContractOpToMmaSync(contractionOp, valueMapping);
call    0 never executed
    #####:  976:            })
call    0 never executed
    #####:  977:            .Case([&](scf::ForOp forOp) {
    #####:  978:              convertForOp(forOp, valueMapping);
call    0 never executed
    #####:  979:              return success();
branch  0 never executed
branch  1 never executed
    #####:  980:            })
call    0 never executed
    #####:  981:            .Case([&](scf::YieldOp yieldOp) {
    #####:  982:              convertYieldOp(yieldOp, valueMapping);
call    0 never executed
    #####:  983:              return success();
branch  0 never executed
branch  1 never executed
    #####:  984:            })
call    0 never executed
    #####:  985:            .Case([&](arith::ConstantOp constOp) {
    #####:  986:              return convertConstantOpMmaSync(constOp, valueMapping);
call    0 never executed
    #####:  987:            })
call    0 never executed
branch  1 never executed
branch  2 never executed
function _ZZN4mlir36convertVectorToNVVMCompatibleMMASyncEPNS_9OperationEENKUlS1_E6_clES1_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  988:            .Default([&](Operation *op) {
    #####:  989:              op->emitError() << "unhandled vector to mma type: " << *op;
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  990:              return failure();
    #####:  991:            })
branch  0 never executed
branch  1 never executed
    #####:  992:            .failed()) {
branch  0 never executed
branch  1 never executed
    #####:  993:      op->emitError() << "Failed to convert op " << *op;
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  994:      return failure();
        -:  995:    }
        -:  996:  }
    #####:  997:  return success();
call    0 never executed
        -:  998:}
        -:  999:
        -: 1000:namespace {
        -: 1001:
    #####: 1002:struct ConvertVectorToGPUPass
call    0 never executed
        -: 1003:    : public impl::ConvertVectorToGPUBase<ConvertVectorToGPUPass> {
        -: 1004:
        -: 1005:  explicit ConvertVectorToGPUPass(bool useNvGpu_) {
        -: 1006:    useNvGpu.setValue(useNvGpu_);
        -: 1007:  }
        -: 1008:
function _ZN12_GLOBAL__N_122ConvertVectorToGPUPass14runOnOperationEv called 416 returned 100% blocks executed 56%
      416: 1009:  void runOnOperation() override {
      832: 1010:    RewritePatternSet patterns(&getContext());
call    0 returned 100%
call    1 returned 100%
      416: 1011:    populatePrepareVectorToMMAPatterns(patterns, useNvGpu.getValue());
call    0 returned 100%
      416: 1012:    if (failed(
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
call    3 returned 100%
call    4 returned 100%
branch  5 taken 0% (fallthrough)
branch  6 taken 100%
      416: 1013:            applyPatternsAndFoldGreedily(getOperation(), std::move(patterns))))
call    0 returned 100%
    #####: 1014:      return signalPassFailure();
call    0 never executed
        -: 1015:
      416: 1016:    if (useNvGpu.getValue()) {
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####: 1017:      if (failed(convertVectorToNVVMCompatibleMMASync(getOperation())))
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####: 1018:        return signalPassFailure();
call    0 never executed
        -: 1019:    }
        -: 1020:
      416: 1021:    (void)convertVectorToMMAOps(getOperation());
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
call    2 returned 100%
call    3 returned 100%
        -: 1022:  }
        -: 1023:};
        -: 1024:
        -: 1025:} // namespace
        -: 1026:
function _ZN4mlir28createConvertVectorToGPUPassEb called 116697 returned 100% blocks executed 100%
   116697: 1027:std::unique_ptr<Pass> mlir::createConvertVectorToGPUPass(bool useNvGpu) {
   116697: 1028:  return std::make_unique<ConvertVectorToGPUPass>(useNvGpu);
call    0 returned 100%
        -: 1029:}
