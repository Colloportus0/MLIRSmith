        -:    0:Source:/data/xcy/llvm-project-fdbc55a5-1/mlir/lib/Dialect/SparseTensor/Transforms/CodegenUtils.h
        -:    0:Graph:../tools/mlir/lib/Dialect/SparseTensor/Transforms/CMakeFiles/obj.MLIRSparseTensorTransforms.dir/Sparsification.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/SparseTensor/Transforms/CMakeFiles/obj.MLIRSparseTensorTransforms.dir/Sparsification.cpp.gcda
        -:    0:Runs:325552
        -:    1://===- CodegenUtils.h - Utilities for generating MLIR -----------*- C++ -*-===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This header file defines utilities for generating MLIR.
        -:   10://
        -:   11://===----------------------------------------------------------------------===//
        -:   12:
        -:   13:#ifndef MLIR_DIALECT_SPARSETENSOR_TRANSFORMS_CODEGENUTILS_H_
        -:   14:#define MLIR_DIALECT_SPARSETENSOR_TRANSFORMS_CODEGENUTILS_H_
        -:   15:
        -:   16:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   17:#include "mlir/Dialect/Complex/IR/Complex.h"
        -:   18:#include "mlir/Dialect/Func/IR/FuncOps.h"
        -:   19:#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
        -:   20:#include "mlir/Dialect/SparseTensor/IR/Enums.h"
        -:   21:#include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
        -:   22:#include "mlir/Dialect/Utils/ReshapeOpsUtils.h"
        -:   23:#include "mlir/IR/Builders.h"
        -:   24:
        -:   25:namespace mlir {
        -:   26:
        -:   27:class Location;
        -:   28:class Type;
        -:   29:class Value;
        -:   30:
        -:   31:namespace sparse_tensor {
        -:   32:
        -:   33:/// Shorthand aliases for the `emitCInterface` argument to `getFunc()`,
        -:   34:/// `createFuncCall()`, and `replaceOpWithFuncCall()`.
        -:   35:enum class EmitCInterface : bool { Off = false, On = true };
        -:   36:
        -:   37://===----------------------------------------------------------------------===//
        -:   38:// ExecutionEngine/SparseTensorUtils helper functions.
        -:   39://===----------------------------------------------------------------------===//
        -:   40:
        -:   41:/// Converts an overhead storage bitwidth to its internal type-encoding.
        -:   42:OverheadType overheadTypeEncoding(unsigned width);
        -:   43:
        -:   44:/// Converts an overhead storage type to its internal type-encoding.
        -:   45:OverheadType overheadTypeEncoding(Type tp);
        -:   46:
        -:   47:/// Converts the internal type-encoding for overhead storage to an mlir::Type.
        -:   48:Type getOverheadType(Builder &builder, OverheadType ot);
        -:   49:
        -:   50:/// Returns the OverheadType for pointer overhead storage.
        -:   51:OverheadType pointerOverheadTypeEncoding(const SparseTensorEncodingAttr &enc);
        -:   52:
        -:   53:/// Returns the OverheadType for index overhead storage.
        -:   54:OverheadType indexOverheadTypeEncoding(const SparseTensorEncodingAttr &enc);
        -:   55:
        -:   56:/// Returns the mlir::Type for pointer overhead storage.
        -:   57:Type getPointerOverheadType(Builder &builder,
        -:   58:                            const SparseTensorEncodingAttr &enc);
        -:   59:
        -:   60:/// Returns the mlir::Type for index overhead storage.
        -:   61:Type getIndexOverheadType(Builder &builder,
        -:   62:                          const SparseTensorEncodingAttr &enc);
        -:   63:
        -:   64:/// Convert OverheadType to its function-name suffix.
        -:   65:StringRef overheadTypeFunctionSuffix(OverheadType ot);
        -:   66:
        -:   67:/// Converts an overhead storage type to its function-name suffix.
        -:   68:StringRef overheadTypeFunctionSuffix(Type overheadTp);
        -:   69:
        -:   70:/// Converts a primary storage type to its internal type-encoding.
        -:   71:PrimaryType primaryTypeEncoding(Type elemTp);
        -:   72:
        -:   73:/// Convert PrimaryType to its function-name suffix.
        -:   74:StringRef primaryTypeFunctionSuffix(PrimaryType pt);
        -:   75:
        -:   76:/// Converts a primary storage type to its function-name suffix.
        -:   77:StringRef primaryTypeFunctionSuffix(Type elemTp);
        -:   78:
        -:   79://===----------------------------------------------------------------------===//
        -:   80:// Misc code generators and utilities.
        -:   81://===----------------------------------------------------------------------===//
        -:   82:
        -:   83:/// Generates a 1-valued attribute of the given type.  This supports
        -:   84:/// all the same types as `getZeroAttr`; however, unlike `getZeroAttr`,
        -:   85:/// for unsupported types we raise `llvm_unreachable` rather than
        -:   86:/// returning a null attribute.
        -:   87:Attribute getOneAttr(Builder &builder, Type tp);
        -:   88:
        -:   89:/// Generates the comparison `v != 0` where `v` is of numeric type.
        -:   90:/// For floating types, we use the "unordered" comparator (i.e., returns
        -:   91:/// true if `v` is NaN).
        -:   92:Value genIsNonzero(OpBuilder &builder, Location loc, Value v);
        -:   93:
        -:   94:/// Computes the shape of destination tensor of a reshape operator. This is only
        -:   95:/// used when operands have dynamic shape. The shape of the destination is
        -:   96:/// stored into dstShape.
        -:   97:void genReshapeDstShape(Location loc, PatternRewriter &rewriter,
        -:   98:                        SmallVector<Value, 4> &dstShape,
        -:   99:                        ArrayRef<Value> srcShape,
        -:  100:                        ArrayRef<int64_t> staticDstShape,
        -:  101:                        ArrayRef<ReassociationIndices> reassociation);
        -:  102:
        -:  103:/// Translate indices during a reshaping operation.
        -:  104:void translateIndicesArray(OpBuilder &builder, Location loc,
        -:  105:                           ArrayRef<ReassociationIndices> reassociation,
        -:  106:                           ValueRange srcIndices, ArrayRef<Value> srcShape,
        -:  107:                           ArrayRef<Value> dstShape,
        -:  108:                           SmallVectorImpl<Value> &dstIndices);
        -:  109:
        -:  110:/// Returns a function reference (first hit also inserts into module). Sets
        -:  111:/// the "_emit_c_interface" on the function declaration when requested,
        -:  112:/// so that LLVM lowering generates a wrapper function that takes care
        -:  113:/// of ABI complications with passing in and returning MemRefs to C functions.
        -:  114:FlatSymbolRefAttr getFunc(ModuleOp module, StringRef name, TypeRange resultType,
        -:  115:                          ValueRange operands, EmitCInterface emitCInterface);
        -:  116:
        -:  117:/// Creates a `CallOp` to the function reference returned by `getFunc()` in
        -:  118:/// the builder's module.
        -:  119:func::CallOp createFuncCall(OpBuilder &builder, Location loc, StringRef name,
        -:  120:                            TypeRange resultType, ValueRange operands,
        -:  121:                            EmitCInterface emitCInterface);
        -:  122:
        -:  123:/// Returns the equivalent of `void*` for opaque arguments to the
        -:  124:/// execution engine.
        -:  125:Type getOpaquePointerType(OpBuilder &builder);
        -:  126:
        -:  127:/// Generates an uninitialized temporary buffer of the given size and
        -:  128:/// type, but returns it as type `memref<? x $tp>` (rather than as type
        -:  129:/// `memref<$sz x $tp>`).
        -:  130:Value genAlloca(OpBuilder &builder, Location loc, Value sz, Type tp);
        -:  131:
        -:  132:/// Generates an uninitialized temporary buffer of the given size and
        -:  133:/// type, but returns it as type `memref<? x $tp>` (rather than as type
        -:  134:/// `memref<$sz x $tp>`).
        -:  135:Value genAlloca(OpBuilder &builder, Location loc, unsigned sz, Type tp);
        -:  136:
        -:  137:/// Generates an uninitialized temporary buffer with room for one value
        -:  138:/// of the given type, and returns the `memref<$tp>`.
        -:  139:Value genAllocaScalar(OpBuilder &builder, Location loc, Type tp);
        -:  140:
        -:  141:/// Generates code to allocate a buffer of the given type, and zero
        -:  142:/// initialize it.  If the buffer type has any dynamic sizes, then the
        -:  143:/// `sizes` parameter should be as filled by sizesFromPtr(); that way
        -:  144:/// we can reuse the genDimSizeCall() results generated by sizesFromPtr().
        -:  145:Value allocDenseTensor(OpBuilder &builder, Location loc,
        -:  146:                       RankedTensorType tensorTp, ValueRange sizes);
        -:  147:
        -:  148:/// Generates the code to read the value from tensor[ivs]. The generated code
        -:  149:/// looks like the following and the insertion point after this routine is
        -:  150:/// inside the if-then branch behind the assignment to ind.
        -:  151:///    if (tensor[ivs] != 0)
        -:  152:///      insert_point
        -:  153:Value genValueForDense(OpBuilder &builder, Location loc, Value tensor,
        -:  154:                       ValueRange ivs);
        -:  155:
        -:  156:/// Generates the loop structure to iterate over a dense tensor or a sparse
        -:  157:/// tensor constant to support the lowering of dense-to-sparse convert operator.
        -:  158://
        -:  159:// The loop to iterate a dense tensor:
        -:  160://   for i1 in dim1
        -:  161://    ..
        -:  162://     for ik in dimk
        -:  163://       val = a[i1,..,ik]
        -:  164://       if val != 0
        -:  165://         loop-body
        -:  166://
        -:  167:// The loop to iterate a sparse tensor constant:
        -:  168://   for i in range(NNZ)
        -:  169://     val = values[i]
        -:  170://     [i1,..,ik] = indices[i]
        -:  171://     loop-body
        -:  172:void genDenseTensorOrSparseConstantIterLoop(
        -:  173:    OpBuilder &builder, Location loc, Value src, unsigned rank,
        -:  174:    function_ref<void(OpBuilder &, Location, Value, ValueRange)> bodyBuilder);
        -:  175:
        -:  176:/// Populates given sizes array from dense tensor or sparse tensor constant.
        -:  177:void sizesFromSrc(OpBuilder &builder, SmallVector<Value, 4> &sizes,
        -:  178:                  Location loc, Value src);
        -:  179:
        -:  180:/// Scans to top of generated loop.
        -:  181:Operation *getTop(Operation *op);
        -:  182:
        -:  183://===----------------------------------------------------------------------===//
        -:  184:// Inlined constant generators.
        -:  185://
        -:  186:// All these functions are just wrappers to improve code legibility;
        -:  187:// therefore, we mark them as `inline` to avoid introducing any additional
        -:  188:// overhead due to the legibility.
        -:  189://
        -:  190:// TODO: Ideally these should move upstream, so that we don't
        -:  191:// develop a design island.  However, doing so will involve
        -:  192:// substantial design work.  For related prior discussion, see
        -:  193:// <https://llvm.discourse.group/t/evolving-builder-apis-based-on-lessons-learned-from-edsc/879>
        -:  194://===----------------------------------------------------------------------===//
        -:  195:
        -:  196:/// Generates a 0-valued constant of the given type.  In addition to
        -:  197:/// the scalar types (`ComplexType`, ``FloatType`, `IndexType`, `IntegerType`),
        -:  198:/// this also works for `RankedTensorType` and `VectorType` (for which it
        -:  199:/// generates a constant `DenseElementsAttr` of zeros).
function _ZN4mlir13sparse_tensor12constantZeroERNS_9OpBuilderENS_8LocationENS_4TypeE called 0 returned 0% blocks executed 0%
    #####:  200:inline Value constantZero(OpBuilder &builder, Location loc, Type tp) {
    #####:  201:  if (auto ctp = tp.dyn_cast<ComplexType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  202:    auto zeroe = builder.getZeroAttr(ctp.getElementType());
call    0 never executed
call    1 never executed
    #####:  203:    auto zeroa = builder.getArrayAttr({zeroe, zeroe});
call    0 never executed
    #####:  204:    return builder.create<complex::ConstantOp>(loc, tp, zeroa);
call    0 never executed
        -:  205:  }
    #####:  206:  return builder.create<arith::ConstantOp>(loc, tp, builder.getZeroAttr(tp));
call    0 never executed
call    1 never executed
        -:  207:}
        -:  208:
        -:  209:/// Generates a 1-valued constant of the given type.  This supports all
        -:  210:/// the same types as `constantZero`.
        -:  211:inline Value constantOne(OpBuilder &builder, Location loc, Type tp) {
        -:  212:  if (auto ctp = tp.dyn_cast<ComplexType>()) {
        -:  213:    auto zeroe = builder.getZeroAttr(ctp.getElementType());
        -:  214:    auto onee = getOneAttr(builder, ctp.getElementType());
        -:  215:    auto zeroa = builder.getArrayAttr({onee, zeroe});
        -:  216:    return builder.create<complex::ConstantOp>(loc, tp, zeroa);
        -:  217:  }
        -:  218:  return builder.create<arith::ConstantOp>(loc, tp, getOneAttr(builder, tp));
        -:  219:}
        -:  220:
        -:  221:/// Generates a constant of `index` type.
    #####:  222:inline Value constantIndex(OpBuilder &builder, Location loc, int64_t i) {
    #####:  223:  return builder.create<arith::ConstantIndexOp>(loc, i);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  224:}
        -:  225:
        -:  226:/// Generates a constant of `i32` type.
        -:  227:inline Value constantI32(OpBuilder &builder, Location loc, int32_t i) {
        -:  228:  return builder.create<arith::ConstantIntOp>(loc, i, 32);
        -:  229:}
        -:  230:
        -:  231:/// Generates a constant of `i16` type.
        -:  232:inline Value constantI16(OpBuilder &builder, Location loc, int16_t i) {
        -:  233:  return builder.create<arith::ConstantIntOp>(loc, i, 16);
        -:  234:}
        -:  235:
        -:  236:/// Generates a constant of `i8` type.
        -:  237:inline Value constantI8(OpBuilder &builder, Location loc, int8_t i) {
        -:  238:  return builder.create<arith::ConstantIntOp>(loc, i, 8);
        -:  239:}
        -:  240:
        -:  241:/// Generates a constant of `i1` type.
    #####:  242:inline Value constantI1(OpBuilder &builder, Location loc, bool b) {
call    0 never executed
    #####:  243:  return builder.create<arith::ConstantIntOp>(loc, b, 1);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  244:}
        -:  245:
        -:  246:/// Generates a constant of the given `Action`.
        -:  247:inline Value constantAction(OpBuilder &builder, Location loc, Action action) {
        -:  248:  return constantI32(builder, loc, static_cast<uint32_t>(action));
        -:  249:}
        -:  250:
        -:  251:/// Generates a constant of the internal type-encoding for overhead storage.
        -:  252:inline Value constantOverheadTypeEncoding(OpBuilder &builder, Location loc,
        -:  253:                                          unsigned width) {
        -:  254:  return constantI32(builder, loc,
        -:  255:                     static_cast<uint32_t>(overheadTypeEncoding(width)));
        -:  256:}
        -:  257:
        -:  258:/// Generates a constant of the internal type-encoding for pointer
        -:  259:/// overhead storage.
        -:  260:inline Value constantPointerTypeEncoding(OpBuilder &builder, Location loc,
        -:  261:                                         const SparseTensorEncodingAttr &enc) {
        -:  262:  return constantOverheadTypeEncoding(builder, loc, enc.getPointerBitWidth());
        -:  263:}
        -:  264:
        -:  265:/// Generates a constant of the internal type-encoding for index overhead
        -:  266:/// storage.
        -:  267:inline Value constantIndexTypeEncoding(OpBuilder &builder, Location loc,
        -:  268:                                       const SparseTensorEncodingAttr &enc) {
        -:  269:  return constantOverheadTypeEncoding(builder, loc, enc.getIndexBitWidth());
        -:  270:}
        -:  271:
        -:  272:/// Generates a constant of the internal type-encoding for primary storage.
        -:  273:inline Value constantPrimaryTypeEncoding(OpBuilder &builder, Location loc,
        -:  274:                                         Type elemTp) {
        -:  275:  return constantI32(builder, loc,
        -:  276:                     static_cast<uint32_t>(primaryTypeEncoding(elemTp)));
        -:  277:}
        -:  278:
        -:  279:/// Generates a constant of the internal dimension level type encoding.
        -:  280:inline Value constantDimLevelTypeEncoding(OpBuilder &builder, Location loc,
        -:  281:                                          DimLevelType dlt) {
        -:  282:  return constantI8(builder, loc, static_cast<uint8_t>(dlt));
        -:  283:}
        -:  284:
        -:  285:inline bool isZeroRankedTensorOrScalar(Type type) {
        -:  286:  auto rtp = type.dyn_cast<RankedTensorType>();
        -:  287:  return !rtp || rtp.getRank() == 0;
        -:  288:}
        -:  289:
        -:  290://===----------------------------------------------------------------------===//
        -:  291:// SparseTensorLoopEmiter class, manages sparse tensors and helps to generate
        -:  292:// loop structure to (co)-iterate sparse tensors.
        -:  293://
        -:  294:// An example usage:
        -:  295:// To generate the following loops over T1<?x?> and T2<?x?>
        -:  296://
        -:  297:// for i in TENSOR_1_0 {
        -:  298://   for j : TENSOR_2_0 {
        -:  299://     for k : TENSOR_1_1 {}
        -:  300://     for k : TENSOR_2_1 {}
        -:  301://   }
        -:  302:// }
        -:  303://
        -:  304:// One can use
        -:  305://
        -:  306:// SparseTensorLoopEmiter loopEmiter({T1, T1});
        -:  307:// loopEmiter.initializeLoopEmit();
        -:  308:// loopEmiter.enterLoopOverTensorAtDim(T1, 0);
        -:  309:// loopEmiter.enterLoopOverTensorAtDim(T2, 0);
        -:  310:// loopEmiter.enterLoopOverTensorAtDim(T1, 1);
        -:  311:// loopEmiter.exitCurrentLoop();
        -:  312:// loopEmiter.enterLoopOverTensorAtDim(T2, 1);
        -:  313:// loopEmiter.exitCurrentLoop(); // exit k
        -:  314:// loopEmiter.exitCurrentLoop(); // exit j
        -:  315:// loopEmiter.exitCurrentLoop(); // exit i
        -:  316://===----------------------------------------------------------------------===//
        -:  317:
        -:  318:// TODO: Sparsification should also rely on this class to generate loops.
        -:  319:class SparseTensorLoopEmitter {
        -:  320:public:
        -:  321:  /// Optional callback function to setup dense output tensors when
        -:  322:  /// initializing the loop emitter (e.g., to fill a dense output with zeros).
        -:  323:  using OutputUpdater = function_ref<Value(OpBuilder &builder, Location loc,
        -:  324:                                           Value memref, Value tensor)>;
        -:  325:
        -:  326:  /// Constructor: take an array of tensors inputs, on which the generated loops
        -:  327:  /// will iterate on. The index of the tensor in the array is also the
        -:  328:  /// tensor id (tid) used in related functions.
        -:  329:  /// If isSparseOut is set, loop emitter assume that the sparse output tensor
        -:  330:  /// is empty, and will always generate loops on it based on the dim sizes.
        -:  331:  explicit SparseTensorLoopEmitter(ValueRange tensors, bool hasOutput = false,
        -:  332:                                   bool isSparseOut = false);
        -:  333:
        -:  334:  /// Starts a loop emitting session by generating all the buffers needed to
        -:  335:  /// iterate tensors.
        -:  336:  void initializeLoopEmit(OpBuilder &builder, Location loc,
        -:  337:                          OutputUpdater updater = nullptr);
        -:  338:
        -:  339:  /// Enters a new loop sequence, the loops within the same sequence starts from
        -:  340:  /// the break points of previous loop instead of starting over from 0.
        -:  341:  /// e.g.,
        -:  342:  /// {
        -:  343:  ///   // loop sequence start.
        -:  344:  ///   p0 = while(xxx)
        -:  345:  ///     ...
        -:  346:  ///     break p0
        -:  347:  ///
        -:  348:  ///   // Starts loop from p0
        -:  349:  ///   for (i = p0; i < end; i++)
        -:  350:  ///     ...
        -:  351:  ///   // loop sequence end.
        -:  352:  /// }
        -:  353:  void enterNewLoopSeq(OpBuilder &builder, Location loc, ArrayRef<size_t> tids,
        -:  354:                       ArrayRef<size_t> dims);
        -:  355:
        -:  356:  // exit the current loop sequence, this will reset universal index to 0.
function _ZN4mlir13sparse_tensor23SparseTensorLoopEmitter18exitCurrentLoopSeqEv called 0 returned 0% blocks executed 0%
    #####:  357:  void exitCurrentLoopSeq() {
    #####:  358:    assert(loopSeqStack.size() == loopStack.size() + 1);
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  359:    loopSeqStack.pop_back();
    #####:  360:  }
        -:  361:
        -:  362:  // TODO: Gets rid of `dim` in the argument list? Track the dimension we
        -:  363:  // are currently at internally. Then it would be enterNextDimForTensor.
        -:  364:  // Still need a way to specify the dim for non annoated dense tensor though,
        -:  365:  // as it can be accessed out of order.
        -:  366:  /// Emits loop over tensor_tid_dim, it assumes that loops between
        -:  367:  /// tensor_tid_[0, dim - 1] have already been generated.
        -:  368:  /// The function will also perform in-place update on the `reduc` vector to
        -:  369:  /// return the reduction variable used inside the generated loop.
        -:  370:  Operation *enterLoopOverTensorAtDim(OpBuilder &builder, Location loc,
        -:  371:                                      size_t tid, size_t dim,
        -:  372:                                      MutableArrayRef<Value> reduc = {},
        -:  373:                                      bool isParallel = false,
        -:  374:                                      ArrayRef<size_t> extraTids = {},
        -:  375:                                      ArrayRef<size_t> extraDims = {});
        -:  376:
        -:  377:  /// Emits a co-iteration loop over a set of tensors.
        -:  378:  Operation *enterCoIterationOverTensorsAtDims(
        -:  379:      OpBuilder &builder, Location loc, ArrayRef<size_t> tids,
        -:  380:      ArrayRef<size_t> dims, bool needsUniv, MutableArrayRef<Value> reduc = {},
        -:  381:      ArrayRef<size_t> extraTids = {}, ArrayRef<size_t> extraDims = {});
        -:  382:
        -:  383:  SmallVector<Value, 2> exitCurrentLoop(OpBuilder &builder, Location loc,
        -:  384:                                        ArrayRef<Value> reduc = {});
        -:  385:
        -:  386:  /// Returns the array of coordinate for all the loop generated till now.
        -:  387:  void getCoordinateArray(SmallVectorImpl<Value> &coords) const {
        -:  388:    for (auto &l : loopStack)
        -:  389:      coords.push_back(l.iv);
        -:  390:  }
        -:  391:
        -:  392:  /// Gets loop induction variable at the given level.
    #####:  393:  Value getLoopIV(size_t level) const {
    #####:  394:    if (level < loopStack.size())
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
branch  6 never executed
branch  7 never executed
    #####:  395:      return loopStack[level].iv;
    #####:  396:    return nullptr;
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
        -:  397:  }
        -:  398:
        -:  399:  ///
        -:  400:  /// Getters.
        -:  401:  ///
    #####:  402:  const std::vector<std::vector<Value>> &getPidxs() const { return pidxs; };
branch  0 never executed
branch  1 never executed
    #####:  403:  const std::vector<std::vector<Value>> &getCoord() const { return coord; };
call    0 never executed
        -:  404:  const std::vector<std::vector<Value>> &getHighs() const { return highs; };
        -:  405:  const std::vector<std::vector<Value>> &getPtrBuffer() const {
        -:  406:    return ptrBuffer;
        -:  407:  };
        -:  408:  const std::vector<std::vector<Value>> &getIdxBuffer() const {
        -:  409:    return idxBuffer;
        -:  410:  };
    #####:  411:  const std::vector<Value> &getValBuffer() const { return valBuffer; };
        -:  412:
        -:  413:private:
        -:  414:  struct LoopLevelInfo {
        -:  415:    LoopLevelInfo(ArrayRef<size_t> tids, ArrayRef<size_t> dims, Operation *loop,
        -:  416:                  Value iv)
        -:  417:        : tids(tids), dims(dims), loop(loop), iv(iv) {}
        -:  418:    // TODO: maybe use a vector<pair> for tid and dim?
        -:  419:    // The set of tensors that the loop is operating on
        -:  420:    const llvm::SmallVector<size_t, 4> tids;
        -:  421:    // The corresponding dims for the tensors
        -:  422:    const llvm::SmallVector<size_t, 4> dims;
        -:  423:    const Operation *loop; // the loop operation
        -:  424:    const Value iv;        // the induction variable for the loop
        -:  425:  };
        -:  426:
        -:  427:  /// Linearizes address for dense dimension (i.e., p = (i * d0) + j).
        -:  428:  Value genAddress(OpBuilder &builder, Location loc, size_t tid, size_t dim,
        -:  429:                   Value iv) {
        -:  430:    Value p = dim == 0 ? constantIndex(builder, loc, 0) : pidxs[tid][dim - 1];
        -:  431:    Value mul = builder.create<arith::MulIOp>(loc, highs[tid][dim], p);
        -:  432:    Value add = builder.create<arith::AddIOp>(loc, mul, iv);
        -:  433:    return add;
        -:  434:  }
        -:  435:
        -:  436:  bool isOutputTensor(size_t tid) {
        -:  437:    return hasOutput && tid == tensors.size() - 1;
        -:  438:  }
        -:  439:
        -:  440:  bool isSparseOutput(size_t tid) { return isOutputTensor(tid) && isSparseOut; }
        -:  441:
        -:  442:  /// Setups [lo, hi] for iterating tensor[dim], it assumes that tensor[0
        -:  443:  /// ...dims-1] has already been setup.
        -:  444:  void prepareLoopOverTensorAtDim(OpBuilder &builder, Location loc, size_t tid,
        -:  445:                                  size_t dim);
        -:  446:
        -:  447:  /// Emits extra locals, since the locals might not be in simplified lattices
        -:  448:  /// point used to generate the loops, but are still required to generates
        -:  449:  /// expressions.
        -:  450:  void emitExtraLocalsForTensorsAtDenseDims(OpBuilder &builder, Location loc,
        -:  451:                                            ArrayRef<size_t> tids,
        -:  452:                                            ArrayRef<size_t> dims);
        -:  453:
        -:  454:  /// Exits a for loop, returns the reduction results, e.g.,
        -:  455:  /// %ret = for () {
        -:  456:  ///   ...
        -:  457:  ///   yield %val
        -:  458:  /// }
        -:  459:  /// Return %ret to user, while %val is provided by users (`reduc`)
        -:  460:  SmallVector<Value, 2> exitForLoop(OpBuilder &builder, Location loc,
        -:  461:                                    ArrayRef<Value> reduc);
        -:  462:
        -:  463:  /// Exits a while loop, returns the reduction results.
        -:  464:  SmallVector<Value, 2> exitCoiterationLoop(OpBuilder &builder, Location loc,
        -:  465:                                            ArrayRef<Value> reduc);
        -:  466:
        -:  467:  // Whether the loop emitter needs to treat the last tensor as the output
        -:  468:  // tensor.
        -:  469:  bool hasOutput;
        -:  470:  bool isSparseOut;
        -:  471:  /// Input and (optional) output tensors.
        -:  472:  std::vector<Value> tensors;
        -:  473:  /// The dim type array for each tensor.
        -:  474:  std::vector<std::vector<DimLevelType>> dimTypes;
        -:  475:  /// Sparse iteration information (by tensor and dim). These arrays
        -:  476:  /// are updated to remain current within the current loop.
        -:  477:  std::vector<std::vector<Value>> pidxs;
        -:  478:  std::vector<std::vector<Value>> coord;
        -:  479:  std::vector<std::vector<Value>> highs;
        -:  480:  std::vector<std::vector<Value>> ptrBuffer; // to_pointers
        -:  481:  std::vector<std::vector<Value>> idxBuffer; // to_indices
        -:  482:  std::vector<Value> valBuffer;              // to_value
        -:  483:
        -:  484:  // Loop Stack, stores the information of all the nested loops that are alive.
        -:  485:  std::vector<LoopLevelInfo> loopStack;
        -:  486:
        -:  487:  // Loop Sequence Stack, stores the unversial index for the current loop
        -:  488:  // sequence.
        -:  489:  std::vector<Value> loopSeqStack;
        -:  490:
        -:  491:  // TODO: not yet used, it should track the current level for each tensor
        -:  492:  // to help eliminate `dim` paramters from above APIs.
        -:  493:  // std::vector<size_t> curLv;
        -:  494:};
        -:  495:
        -:  496:} // namespace sparse_tensor
        -:  497:} // namespace mlir
        -:  498:
        -:  499:#endif // MLIR_DIALECT_SPARSETENSOR_TRANSFORMS_CODEGENUTILS_H_
