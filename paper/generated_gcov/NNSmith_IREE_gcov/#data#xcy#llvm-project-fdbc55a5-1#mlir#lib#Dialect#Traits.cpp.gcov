        -:    0:Source:/data/xcy/llvm-project-fdbc55a5-1/mlir/lib/Dialect/Traits.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/CMakeFiles/obj.MLIRDialect.dir/Traits.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/CMakeFiles/obj.MLIRDialect.dir/Traits.cpp.gcda
        -:    0:Runs:325564
        -:    1://===- Traits.cpp - Common op traits shared by dialects -------------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8:
        -:    9:#include "mlir/Dialect/Traits.h"
        -:   10:#include "mlir/IR/BuiltinTypes.h"
        -:   11:#include "mlir/IR/TypeUtilities.h"
        -:   12:#include "llvm/Support/FormatVariadic.h"
        -:   13:
        -:   14:using namespace mlir;
        -:   15:
function _ZN4mlir7OpTrait4util28staticallyKnownBroadcastableEN4llvm8ArrayRefIlEES4_ called 0 returned 0% blocks executed 0%
    #####:   16:bool OpTrait::util::staticallyKnownBroadcastable(ArrayRef<int64_t> shape1,
        -:   17:                                                 ArrayRef<int64_t> shape2) {
    #####:   18:  SmallVector<SmallVector<int64_t, 6>, 2> extents;
call    0 never executed
    #####:   19:  extents.emplace_back(shape1.begin(), shape1.end());
call    0 never executed
    #####:   20:  extents.emplace_back(shape2.begin(), shape2.end());
call    0 never executed
    #####:   21:  return staticallyKnownBroadcastable(extents);
call    0 never executed
call    1 never executed
        -:   22:}
        -:   23:
function _ZN4mlir7OpTrait4util28staticallyKnownBroadcastableEN4llvm8ArrayRefINS2_11SmallVectorIlLj6EEEEE called 0 returned 0% blocks executed 0%
    #####:   24:bool OpTrait::util::staticallyKnownBroadcastable(
        -:   25:    ArrayRef<SmallVector<int64_t, 6>> shapes) {
    #####:   26:  assert(!shapes.empty() && "Expected at least one shape");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:   27:  size_t maxRank = shapes[0].size();
    #####:   28:  for (size_t i = 1; i != shapes.size(); ++i)
branch  0 never executed
branch  1 never executed
    #####:   29:    maxRank = std::max(maxRank, shapes[i].size());
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:   30:
        -:   31:  // We look backwards through every column of `shapes`.
    #####:   32:  for (size_t i = 0; i != maxRank; ++i) {
branch  0 never executed
branch  1 never executed
    #####:   33:    bool seenDynamic = false;
    #####:   34:    Optional<int64_t> nonOneDim;
    #####:   35:    for (ArrayRef<int64_t> extent : shapes) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:   36:      int64_t dim = i >= extent.size() ? 1 : extent[extent.size() - i - 1];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:   37:
    #####:   38:      if (dim == 1)
branch  0 never executed
branch  1 never executed
    #####:   39:        continue;
        -:   40:
        -:   41:      // Dimensions are compatible when
        -:   42:      //.  1. One is dynamic, the rest are 1
    #####:   43:      if (ShapedType::isDynamic(dim)) {
branch  0 never executed
branch  1 never executed
    #####:   44:        if (seenDynamic || nonOneDim)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:   45:          return false;
        -:   46:        seenDynamic = true;
        -:   47:      }
        -:   48:
        -:   49:      //   2. All are 1 or a specific constant.
    #####:   50:      if (nonOneDim && dim != *nonOneDim)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:   51:        return false;
        -:   52:
    #####:   53:      nonOneDim = dim;
branch  0 never executed
branch  1 never executed
        -:   54:    }
        -:   55:  }
        -:   56:  return true;
        -:   57:}
        -:   58:
function _ZN4mlir7OpTrait4util19getBroadcastedShapeEN4llvm8ArrayRefIlEES4_RNS2_15SmallVectorImplIlEE called 6070914 returned 100% blocks executed 68%
  6070914:   59:bool OpTrait::util::getBroadcastedShape(ArrayRef<int64_t> shape1,
        -:   60:                                        ArrayRef<int64_t> shape2,
        -:   61:                                        SmallVectorImpl<int64_t> &resultShape) {
        -:   62:  // To compute the result broadcasted shape, we compare operand shapes
        -:   63:  // element-wise: starting with the trailing dimensions, and working the
        -:   64:  // way backward. Two dimensions are compatible when
        -:   65:  //   1. they are equal, or
        -:   66:  //   2. one of them is 1
        -:   67:  // The result shape has the maximum among the two inputs at every
        -:   68:  // dimension index.
        -:   69:
  6070914:   70:  resultShape.clear();
branch  0 taken 33% (fallthrough)
branch  1 taken 67%
  6070914:   71:  if (shape1.size() > shape2.size()) {
branch  0 taken 33% (fallthrough)
branch  1 taken 67%
  2023638:   72:    std::copy(shape1.begin(), shape1.end(), std::back_inserter(resultShape));
        -:   73:  } else {
  4047276:   74:    std::copy(shape2.begin(), shape2.end(), std::back_inserter(resultShape));
        -:   75:  }
        -:   76:
  6070914:   77:  auto i1 = shape1.rbegin(), e1 = shape1.rend();
  6070914:   78:  auto i2 = shape2.rbegin(), e2 = shape2.rend();
  6070914:   79:  auto iR = resultShape.rbegin();
        -:   80:
        -:   81:  // Check each dimension is consistent.
 22593334:   82:  for (; i1 != e1 && i2 != e2; ++i1, ++i2, ++iR) {
branch  0 taken 82% (fallthrough)
branch  1 taken 18%
branch  2 taken 89% (fallthrough)
branch  3 taken 11%
 16522420:   83:    if (ShapedType::isDynamic(*i1) || ShapedType::isDynamic(*i2)) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 100% (fallthrough)
branch  3 taken 0%
        -:   84:      // One or both dimensions is unknown. Follow TensorFlow behavior:
        -:   85:      // - If either dimension is greater than 1, we assume that the program is
        -:   86:      //   correct, and the other dimension will be broadcast to match it.
        -:   87:      // - If either dimension is 1, the other dimension is the output.
    #####:   88:      if (*i1 > 1) {
branch  0 never executed
branch  1 never executed
    #####:   89:        *iR = *i1;
    #####:   90:      } else if (*i2 > 1) {
branch  0 never executed
branch  1 never executed
    #####:   91:        *iR = *i2;
    #####:   92:      } else if (*i1 == 1) {
branch  0 never executed
branch  1 never executed
    #####:   93:        *iR = *i2;
    #####:   94:      } else if (*i2 == 1) {
branch  0 never executed
branch  1 never executed
    #####:   95:        *iR = *i1;
        -:   96:      } else {
    #####:   97:        *iR = ShapedType::kDynamicSize;
        -:   98:      }
        -:   99:    } else {
 16522420:  100:      if (*i1 == *i2 || *i2 == 1) {
branch  0 taken 3%
branch  1 taken 97%
branch  2 taken 32% (fallthrough)
branch  3 taken 68%
 16364019:  101:        *iR = *i1;
   158401:  102:      } else if (*i1 == 1) {
branch  0 taken 100%
branch  1 taken 0%
   158401:  103:        *iR = *i2;
        -:  104:      } else {
        -:  105:        // This dimension of the two operand types is incompatible.
    #####:  106:        resultShape.clear();
    #####:  107:        return false;
        -:  108:      }
        -:  109:    }
        -:  110:  }
        -:  111:
        -:  112:  return true;
        -:  113:}
        -:  114:
        -:  115:/// Returns the shape of the given type. Scalars will be considered as having a
        -:  116:/// shape with zero dimensions.
function _ZL8getShapeN4mlir4TypeE called 8094552 returned 100% blocks executed 83%
  8094552:  117:static ArrayRef<int64_t> getShape(Type type) {
  8094552:  118:  if (auto sType = type.dyn_cast<ShapedType>())
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
  8094552:  119:    return sType.getShape();
call    0 returned 100%
    #####:  120:  return {};
        -:  121:}
        -:  122:
        -:  123:/// Returns the result broadcast composition type from the two given types by
        -:  124:/// following NumPy broadcast semantics. Returned type may have dynamic shape if
        -:  125:/// either of the input types has dynamic shape. Returns null type if the two
        -:  126:/// given types are not broadcast-compatible.
        -:  127:///
        -:  128:/// elementType, if specified, will be used as the element type of the
        -:  129:/// broadcasted result type. Otherwise it is required that the element type of
        -:  130:/// type1 and type2 is the same and this element type will be used as the
        -:  131:/// resultant element type.
function _ZN4mlir7OpTrait4util18getBroadcastedTypeENS_4TypeES2_S2_ called 0 returned 0% blocks executed 0%
    #####:  132:Type OpTrait::util::getBroadcastedType(Type type1, Type type2,
        -:  133:                                       Type elementType) {
        -:  134:  // If the elementType is not specified, then the use the common element type
        -:  135:  // of the inputs or fail if there is no common element type.
    #####:  136:  if (!elementType) {
branch  0 never executed
branch  1 never executed
    #####:  137:    elementType = getElementTypeOrSelf(type1);
call    0 never executed
    #####:  138:    if (elementType != getElementTypeOrSelf(type2))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  139:      return {};
        -:  140:  }
        -:  141:
        -:  142:  // If one of the types is unranked tensor, then the other type shouldn't be
        -:  143:  // vector and the result should have unranked tensor type.
    #####:  144:  if (type1.isa<UnrankedTensorType>() || type2.isa<UnrankedTensorType>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  145:    if (type1.isa<VectorType>() || type2.isa<VectorType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
    #####:  146:      return {};
    #####:  147:    return UnrankedTensorType::get(elementType);
call    0 never executed
        -:  148:  }
        -:  149:
        -:  150:  // Returns the type kind if the given type is a vector or ranked tensor type.
        -:  151:  // Returns llvm::None otherwise.
function _ZZN4mlir7OpTrait4util18getBroadcastedTypeENS_4TypeES2_S2_ENKUlS2_E_clES2_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  152:  auto getCompositeTypeKind = [](Type type) -> Optional<TypeID> {
    #####:  153:    if (type.isa<VectorType, RankedTensorType>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  154:      return type.getTypeID();
branch  0 never executed
branch  1 never executed
    #####:  155:    return llvm::None;
        -:  156:  };
        -:  157:
        -:  158:  // Make sure the composite type, if has, is consistent.
    #####:  159:  Optional<TypeID> compositeKind1 = getCompositeTypeKind(type1);
call    0 never executed
    #####:  160:  Optional<TypeID> compositeKind2 = getCompositeTypeKind(type2);
call    0 never executed
    #####:  161:  Optional<TypeID> resultCompositeKind;
        -:  162:
    #####:  163:  if (compositeKind1 && compositeKind2) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  164:    // Disallow mixing vector and tensor.
    #####:  165:    if (compositeKind1 != compositeKind2)
branch  0 never executed
branch  1 never executed
    #####:  166:      return {};
    #####:  167:    resultCompositeKind = compositeKind1;
    #####:  168:  } else if (compositeKind1) {
branch  0 never executed
branch  1 never executed
    #####:  169:    resultCompositeKind = compositeKind1;
    #####:  170:  } else if (compositeKind2) {
branch  0 never executed
branch  1 never executed
    #####:  171:    resultCompositeKind = compositeKind2;
        -:  172:  }
        -:  173:
        -:  174:  // Get the shape of each type.
    #####:  175:  SmallVector<int64_t, 4> resultShape;
call    0 never executed
    #####:  176:  if (!getBroadcastedShape(getShape(type1), getShape(type2), resultShape))
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  177:    return {};
        -:  178:
        -:  179:  // Compose the final broadcasted type
    #####:  180:  if (resultCompositeKind == VectorType::getTypeID())
branch  0 never executed
branch  1 never executed
    #####:  181:    return VectorType::get(resultShape, elementType);
call    0 never executed
    #####:  182:  if (resultCompositeKind == RankedTensorType::getTypeID())
branch  0 never executed
branch  1 never executed
    #####:  183:    return RankedTensorType::get(resultShape, elementType);
call    0 never executed
    #####:  184:  return elementType;
        -:  185:}
        -:  186:
        -:  187:/// Returns a tuple corresponding to whether range has tensor or vector type.
        -:  188:template <typename iterator_range>
  4047276:  189:static std::tuple<bool, bool> hasTensorOrVectorType(iterator_range types) {
        -:  190:  return std::make_tuple(
  4047276:  191:      llvm::any_of(types, [](Type t) { return t.isa<TensorType>(); }),
call    0 returned 100%
call    1 returned 100%
  6070914:  192:      llvm::any_of(types, [](Type t) { return t.isa<VectorType>(); }));
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
branch  3 taken 0% (fallthrough)
branch  4 taken 100%
        -:  193:}
        -:  194:
function _ZL31isCompatibleInferredReturnShapeN4llvm8ArrayRefIlEES1_ called 2023638 returned 100% blocks executed 64%
  2023638:  195:static bool isCompatibleInferredReturnShape(ArrayRef<int64_t> inferred,
        -:  196:                                            ArrayRef<int64_t> existing) {
 10284848:  197:  auto isCompatible = [](int64_t dim1, int64_t dim2) {
        -:  198:    // If the inferred and existing dim is the same, or one of them is unknown
        -:  199:    // then it is compatible, else if the inferred dim is 1 then it is also
        -:  200:    // compatible. But if the existing dim is 1 and the inferred is greater than
        -:  201:    // 1 then flag.
    #####:  202:    return dim1 == dim2 || ShapedType::isDynamic(dim1) ||
branch  0 never executed
branch  1 never executed
 8261210*:  203:           ShapedType::isDynamic(dim2) || dim1 == 1;
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
        -:  204:  };
  2023638:  205:  if (inferred.size() != existing.size())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
        -:  206:    return false;
 10284848:  207:  for (auto p : llvm::zip(inferred, existing))
branch  0 taken 80% (fallthrough)
branch  1 taken 20%
 8261210*:  208:    if (!isCompatible(std::get<0>(p), std::get<1>(p)))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  209:      return false;
  2023638:  210:  return true;
        -:  211:}
        -:  212:
function _ZL14getShapeStringN4llvm8ArrayRefIlEE called 0 returned 0% blocks executed 0%
    #####:  213:static std::string getShapeString(ArrayRef<int64_t> shape) {
        -:  214:  // TODO: should replace with printing shape more uniformly across here and
        -:  215:  // when in type.
    #####:  216:  std::string ret;
call    0 never executed
    #####:  217:  llvm::raw_string_ostream ss(ret);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  218:  ss << '\'';
branch  0 never executed
branch  1 never executed
    #####:  219:  llvm::interleave(
call    0 never executed
        -:  220:      shape, ss,
function _ZZL14getShapeStringN4llvm8ArrayRefIlEEENKUllE_clEl.isra.0 called 0 returned 0% blocks executed 0%
    #####:  221:      [&](int64_t dim) {
    #####:  222:        if (ShapedType::isDynamic(dim))
branch  0 never executed
branch  1 never executed
    #####:  223:          ss << '?';
branch  0 never executed
branch  1 never executed
        -:  224:        else
    #####:  225:          ss << dim;
call    0 never executed
    #####:  226:      },
        -:  227:      "x");
    #####:  228:  ss << '\'';
branch  0 never executed
branch  1 never executed
    #####:  229:  return ss.str();
call    0 never executed
call    1 never executed
        -:  230:}
        -:  231:
function _ZN4mlir7OpTrait4impl32verifyCompatibleOperandBroadcastEPNS_9OperationE called 2023638 returned 100% blocks executed 60%
  2023638:  232:LogicalResult OpTrait::impl::verifyCompatibleOperandBroadcast(Operation *op) {
        -:  233:  // Ensure broadcasting only tensor or only vector types.
  2023638:  234:  auto operandsHasTensorVectorType =
  2023638:  235:      hasTensorOrVectorType(op->getOperandTypes());
call    0 returned 100%
  4047276:  236:  auto resultsHasTensorVectorType = hasTensorOrVectorType(op->getResultTypes());
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
 2023638*:  237:  if ((std::get<0>(operandsHasTensorVectorType) ||
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
 2023638*:  238:       std::get<0>(resultsHasTensorVectorType)) &&
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
branch  2 never executed
branch  3 never executed
  2023638:  239:      (std::get<1>(operandsHasTensorVectorType) ||
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
  2023638:  240:       std::get<1>(resultsHasTensorVectorType)))
branch  0 taken 0% (fallthrough)
branch  1 taken 100%
    #####:  241:    return op->emitError("cannot broadcast vector with tensor");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  242:
  2023638:  243:  auto rankedOperands = make_filter_range(
  6070914:  244:      op->getOperandTypes(), [](Type t) { return t.isa<RankedTensorType>(); });
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
call    4 returned 100%
        -:  245:
        -:  246:  // If all operands are unranked, then all result shapes are possible.
 2023638*:  247:  if (rankedOperands.empty())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    #####:  248:    return success();
        -:  249:
        -:  250:  // Compute broadcasted shape of operands (which requires that operands are
        -:  251:  // broadcast compatible). The results need to be broadcast compatible with
        -:  252:  // this result shape.
  2023638:  253:  SmallVector<int64_t, 4> resultShape;
call    0 returned 100%
  2023638:  254:  (void)util::getBroadcastedShape(getShape(*rankedOperands.begin()), {},
call    0 returned 100%
call    1 returned 100%
call    2 returned 100%
        -:  255:                                  resultShape);
  6070914:  256:  for (auto other : make_early_inc_range(rankedOperands)) {
call    0 returned 100%
branch  1 taken 67% (fallthrough)
branch  2 taken 33%
call    3 returned 100%
  8094552:  257:    SmallVector<int64_t, 4> temp = resultShape;
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
  4047276:  258:    if (!util::getBroadcastedShape(temp, getShape(other), resultShape))
call    0 returned 100%
call    1 returned 100%
branch  2 taken 0% (fallthrough)
branch  3 taken 100%
    #####:  259:      return op->emitOpError("operands don't have broadcast-compatible shapes");
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
        -:  260:  }
        -:  261:
  2023638:  262:  auto rankedResults = make_filter_range(
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
  4047276:  263:      op->getResultTypes(), [](Type t) { return t.isa<RankedTensorType>(); });
call    0 returned 100%
branch  1 taken 100% (fallthrough)
branch  2 taken 0%
call    3 returned 100%
        -:  264:
        -:  265:  // If all of the results are unranked then no further verification.
 2023638*:  266:  if (rankedResults.empty())
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
    #####:  267:    return success();
        -:  268:
  8094552:  269:  for (auto type : rankedResults) {
branch  0 taken 100% (fallthrough)
branch  1 taken 0%
call    2 returned 100%
call    3 returned 100%
  2023638:  270:    ArrayRef<int64_t> actualSuffix =
  2023638:  271:        getShape(type).take_back(resultShape.size());
call    0 returned 100%
branch  1 taken 100%
branch  2 taken 0%
  2023638:  272:    if (!isCompatibleInferredReturnShape(resultShape, actualSuffix))
call    0 returned 100%
branch  1 taken 0% (fallthrough)
branch  2 taken 100%
    #####:  273:      return op->emitOpError()
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  274:             << "result type " << getShapeString(getShape(type))
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
call    4 never executed
    #####:  275:             << " not broadcast compatible with broadcasted operands's shapes "
call    0 never executed
    #####:  276:             << getShapeString(resultShape);
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
        -:  277:  }
  2023638:  278:  return success();
branch  0 taken 8% (fallthrough)
branch  1 taken 92%
        -:  279:}
