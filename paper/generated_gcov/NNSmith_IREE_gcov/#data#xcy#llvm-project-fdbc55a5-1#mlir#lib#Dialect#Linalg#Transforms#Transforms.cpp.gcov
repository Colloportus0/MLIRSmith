        -:    0:Source:/data/xcy/llvm-project-fdbc55a5-1/mlir/lib/Dialect/Linalg/Transforms/Transforms.cpp
        -:    0:Graph:../tools/mlir/lib/Dialect/Linalg/Transforms/CMakeFiles/obj.MLIRLinalgTransforms.dir/Transforms.cpp.gcno
        -:    0:Data:../tools/mlir/lib/Dialect/Linalg/Transforms/CMakeFiles/obj.MLIRLinalgTransforms.dir/Transforms.cpp.gcda
        -:    0:Runs:325547
        -:    1://===- Transforms.cpp - Linalg transformations as patterns ----------------===//
        -:    2://
        -:    3:// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
        -:    4:// See https://llvm.org/LICENSE.txt for license information.
        -:    5:// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
        -:    6://
        -:    7://===----------------------------------------------------------------------===//
        -:    8://
        -:    9:// This file implements logic and helpers to expose Linalg transforms as rewrite
        -:   10:// patterns.
        -:   11://
        -:   12://===----------------------------------------------------------------------===//
        -:   13:
        -:   14:#include "mlir/Dialect/Linalg/Transforms/Transforms.h"
        -:   15:#include "mlir/Dialect/Affine/IR/AffineOps.h"
        -:   16:#include "mlir/Dialect/Arith/IR/Arith.h"
        -:   17:#include "mlir/Dialect/Func/IR/FuncOps.h"
        -:   18:#include "mlir/Dialect/Linalg/Analysis/DependenceAnalysis.h"
        -:   19:#include "mlir/Dialect/Linalg/IR/Linalg.h"
        -:   20:#include "mlir/Dialect/Linalg/Transforms/HoistPadding.h"
        -:   21:#include "mlir/Dialect/Linalg/Utils/Utils.h"
        -:   22:#include "mlir/Dialect/SCF/Transforms/Transforms.h"
        -:   23:#include "mlir/Dialect/Tensor/IR/Tensor.h"
        -:   24:#include "mlir/Dialect/Tensor/IR/TensorTilingInterfaceImpl.h"
        -:   25:#include "mlir/Dialect/Utils/StaticValueUtils.h"
        -:   26:#include "mlir/Dialect/Utils/StructuredOpsUtils.h"
        -:   27:#include "mlir/Dialect/Vector/IR/VectorOps.h"
        -:   28:#include "mlir/IR/AffineExpr.h"
        -:   29:#include "mlir/IR/Matchers.h"
        -:   30:#include "mlir/Pass/Pass.h"
        -:   31:#include "mlir/Support/LLVM.h"
        -:   32:#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
        -:   33:#include "llvm/ADT/ScopeExit.h"
        -:   34:#include "llvm/ADT/TypeSwitch.h"
        -:   35:#include "llvm/Support/Debug.h"
        -:   36:#include "llvm/Support/raw_ostream.h"
        -:   37:#include <type_traits>
        -:   38:#include <utility>
        -:   39:
        -:   40:#define DEBUG_TYPE "linalg-transforms"
        -:   41:
        -:   42:using namespace mlir;
        -:   43:using namespace mlir::linalg;
        -:   44:
        -:   45:#define DBGS() (llvm::dbgs() << "[" DEBUG_TYPE << "]: ")
        -:   46:
        -:   47://===----------------------------------------------------------------------===//
        -:   48:// Transformations exposed as rewrite patterns.
        -:   49://===----------------------------------------------------------------------===//
        -:   50:
        -:   51:LinalgTilingOptions &
function _ZN4mlir6linalg19LinalgTilingOptions12setTileSizesEN4llvm8ArrayRefIlEE called 0 returned 0% blocks executed 0%
    #####:   52:mlir::linalg::LinalgTilingOptions::setTileSizes(ArrayRef<int64_t> ts) {
    #####:   53:  assert(!tileSizeComputationFunction && "tile sizes already set");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:   54:  SmallVector<int64_t, 4> tileSizes(ts.begin(), ts.end());
call    0 never executed
function _ZZN4mlir6linalg19LinalgTilingOptions12setTileSizesEN4llvm8ArrayRefIlEEENKUlRNS_9OpBuilderEPNS_9OperationEE_clES6_S8_ called 0 returned 0% blocks executed 0%
    #####:   55:  tileSizeComputationFunction = [tileSizes](OpBuilder &b, Operation *op) {
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
    #####:   56:    OpBuilder::InsertionGuard guard(b);
call    0 never executed
    #####:   57:    b.setInsertionPointToStart(
    #####:   58:        &op->getParentOfType<func::FuncOp>().getBody().front());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:   59:    return llvm::to_vector<4>(map_range(tileSizes, [&](int64_t s) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:   60:      Value v = b.create<arith::ConstantIndexOp>(op->getLoc(), s);
        -:   61:      return v;
    #####:   62:    }));
call    0 never executed
    #####:   63:  };
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   64:  return *this;
branch  0 never executed
branch  1 never executed
        -:   65:}
        -:   66:
        -:   67:/// Pad the `opOperand` in the `paddingDimensions` using the padding value and
        -:   68:/// the nofold flag found in `paddingValues` and `packPaddings`, respectively.
        -:   69:/// Exit early and return the `opOperand` value if the shape dimensions that
        -:   70:/// match `paddingDimensions` have a static size and the nofold flag is not set.
        -:   71:/// Otherwise, try to pad the shape dimensions that match the iterator
        -:   72:/// dimensions `paddingDimensions` and return the tensor::PadOp result if
        -:   73:/// padding succeeds or failure otherwise.
function _ZL37padOperandToSmallestStaticBoundingBoxRN4mlir9OpBuilderENS_6linalg8LinalgOpEPNS_9OpOperandEN4llvm8ArrayRefIlEENS7_INS_9AttributeEEENS7_IbEE called 0 returned 0% blocks executed 0%
    #####:   74:static FailureOr<Value> padOperandToSmallestStaticBoundingBox(
        -:   75:    OpBuilder &b, linalg::LinalgOp opToPad, OpOperand *opOperand,
        -:   76:    ArrayRef<int64_t> paddingDimensions, ArrayRef<Attribute> paddingValues,
        -:   77:    ArrayRef<bool> packPaddings) {
    #####:   78:  AffineMap indexingMap = opToPad.getMatchingIndexingMap(opOperand);
call    0 never executed
    #####:   79:  ArrayRef<int64_t> shape = opToPad.getShape(opOperand);
call    0 never executed
        -:   80:
        -:   81:  // Collect the shape dimension that are a function of the `paddingDimensions`.
    #####:   82:  llvm::SmallDenseSet<int64_t> shapeDimsToPad;
call    0 never executed
    #####:   83:  for (int64_t dim : paddingDimensions)
branch  0 never executed
branch  1 never executed
    #####:   84:    for (const auto &en : enumerate(indexingMap.getResults()))
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:   85:      if (en.value().isFunctionOfDim(dim))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   86:        shapeDimsToPad.insert(en.index());
call    0 never executed
        -:   87:
        -:   88:  // Return the unpadded operand if padding to a static shape is not needed and
        -:   89:  // if the nofold flag is not set.
    #####:   90:  bool nofold = opOperand->getOperandNumber() < packPaddings.size()
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:   91:                    ? packPaddings[opOperand->getOperandNumber()]
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:   92:                    : false;
branch  0 never executed
branch  1 never executed
function _ZZL37padOperandToSmallestStaticBoundingBoxRN4mlir9OpBuilderENS_6linalg8LinalgOpEPNS_9OpOperandEN4llvm8ArrayRefIlEENS7_INS_9AttributeEEENS7_IbEEENKUllE_clEl.isra.0 called 0 returned 0% blocks executed 0%
    #####:   93:  bool hasStaticShape = llvm::none_of(shapeDimsToPad, [&](int64_t dim) {
call    0 never executed
    #####:   94:    return ShapedType::isDynamic(shape[dim]);
branch  0 never executed
branch  1 never executed
        -:   95:  });
    #####:   96:  if (!nofold && hasStaticShape)
branch  0 never executed
branch  1 never executed
    #####:   97:    return opOperand->get();
        -:   98:
        -:   99:  // Fail if `paddingValues` specifies no padding value.
    #####:  100:  if (opOperand->getOperandNumber() >= paddingValues.size())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  101:    return failure();
    #####:  102:  Attribute paddingAttr = paddingValues[opOperand->getOperandNumber()];
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  103:  Type paddingType = b.getType<NoneType>();
call    0 never executed
    #####:  104:  if (auto typedAttr = paddingAttr.dyn_cast<TypedAttr>())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  105:    paddingType = typedAttr.getType();
call    0 never executed
    #####:  106:  Value paddingValue =
    #####:  107:      b.create<arith::ConstantOp>(opToPad.getLoc(), paddingType, paddingAttr);
call    0 never executed
        -:  108:
        -:  109:  // Follow the use-def chain if `currOpOperand` is defined by a LinalgOp.
    #####:  110:  OpOperand *currOpOperand = opOperand;
    #####:  111:  while (auto linalgOp = currOpOperand->get().getDefiningOp<LinalgOp>()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  112:    OpResult result = currOpOperand->get().cast<OpResult>();
call    0 never executed
    #####:  113:    currOpOperand = linalgOp.getDpsInitOperand(result.getResultNumber());
call    0 never executed
call    1 never executed
    #####:  114:  }
        -:  115:
        -:  116:  // Fail if `currOpOperand` is not defined by an ExtractSliceOp.
    #####:  117:  auto sliceOp = currOpOperand->get().getDefiningOp<tensor::ExtractSliceOp>();
call    0 never executed
    #####:  118:  if (!sliceOp)
branch  0 never executed
branch  1 never executed
    #####:  119:    return failure();
        -:  120:
        -:  121:  // Compute the dropped dimensions if `sliceOp` is ranke-reducing.
    #####:  122:  llvm::SmallBitVector droppedDims = sliceOp.getDroppedDims();
call    0 never executed
    #####:  123:  OffsetSizeAndStrideOpInterface shapedOp = sliceOp;
call    0 never executed
        -:  124:
        -:  125:  // Upper bound the `sliceOp` sizes to obtain a static bounding box.
    #####:  126:  SmallVector<int64_t> paddedShape(shape.begin(), shape.end());
call    0 never executed
call    1 never executed
    #####:  127:  int64_t shapeIdx = 0;
    #####:  128:  for (const auto &en : enumerate(shapedOp.getMixedSizes())) {
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
call    7 never executed
        -:  129:    // Skip dropped dimensions.
    #####:  130:    if (droppedDims.test(en.index()))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  131:      continue;
        -:  132:    // Skip dimensions that do not require padding.
    #####:  133:    if (!shapeDimsToPad.contains(shapeIdx)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  134:      shapeIdx++;
    #####:  135:      continue;
        -:  136:    }
        -:  137:    // If the size is an attribute add it directly to `paddedShape`.
    #####:  138:    if (en.value().is<Attribute>()) {
branch  0 never executed
branch  1 never executed
    #####:  139:      paddedShape[shapeIdx++] =
branch  0 never executed
branch  1 never executed
    #####:  140:          en.value().get<Attribute>().dyn_cast<IntegerAttr>().getInt();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  141:      continue;
        -:  142:    }
        -:  143:    // Otherwise, try to compute a constant upper bound for the size value.
    #####:  144:    FailureOr<int64_t> upperBound =
    #####:  145:        getConstantUpperBoundForIndex(en.value().get<Value>());
call    0 never executed
call    1 never executed
    #####:  146:    if (failed(upperBound)) {
branch  0 never executed
branch  1 never executed
    #####:  147:      LLVM_DEBUG(DBGS() << "No constant bounding box can be found for padding");
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
call    5 never executed
call    6 never executed
call    7 never executed
call    8 never executed
    #####:  148:      return failure();
branch  0 never executed
branch  1 never executed
        -:  149:    }
    #####:  150:    paddedShape[shapeIdx++] = *upperBound;
branch  0 never executed
branch  1 never executed
        -:  151:  }
    #####:  152:  assert(shapeIdx == static_cast<int64_t>(shape.size()) &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  153:         "expect the dynamic and static ranks to match");
        -:  154:
        -:  155:  // Pad the operand to the bounding box defined by `paddedShape`.
    #####:  156:  auto paddedTensorType = RankedTensorType::get(
    #####:  157:      paddedShape, getElementTypeOrSelf(opOperand->get()));
call    0 never executed
call    1 never executed
    #####:  158:  return makeComposedPadHighOp(b, opToPad->getLoc(), paddedTensorType,
    #####:  159:                               opOperand->get(), paddingValue, nofold);
call    0 never executed
        -:  160:}
        -:  161:
        -:  162:FailureOr<SmallVector<Value>>
function _ZN4mlir6linalg17rewriteAsPaddedOpERNS_9OpBuilderENS0_8LinalgOpEN4llvm8ArrayRefIlEENS5_INS_9AttributeEEENS5_IbEERS3_ called 0 returned 0% blocks executed 0%
    #####:  163:linalg::rewriteAsPaddedOp(OpBuilder &b, LinalgOp opToPad,
        -:  164:                          ArrayRef<int64_t> paddingDimensions,
        -:  165:                          ArrayRef<Attribute> paddingValues,
        -:  166:                          ArrayRef<bool> packPaddings, LinalgOp &paddedOp) {
    #####:  167:  Location loc = opToPad->getLoc();
call    0 never executed
        -:  168:
        -:  169:  // TODO: there are cases where we may still want to pad to larger sizes.
    #####:  170:  assert(opToPad.hasTensorSemantics() &&
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
        -:  171:         "expected operation to have tensor semantics");
        -:  172:
    #####:  173:  OpBuilder::InsertionGuard g(b);
call    0 never executed
        -:  174:  // Set IP after op because we also take the dims of the original output.
    #####:  175:  b.setInsertionPointAfter(opToPad);
call    0 never executed
        -:  176:  // Make a copy of the shaped operands and update it.
    #####:  177:  SmallVector<Value> newOperands;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  178:  newOperands.reserve(opToPad->getNumOperands());
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  179:  for (OpOperand &opOperand : opToPad->getOpOperands()) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  180:    FailureOr<Value> paddedOperand = padOperandToSmallestStaticBoundingBox(
    #####:  181:        b, opToPad, &opOperand, paddingDimensions, paddingValues, packPaddings);
call    0 never executed
        -:  182:    // Exit if `paddingDimensions` cannot be bounded statically.
    #####:  183:    if (failed(paddedOperand))
branch  0 never executed
branch  1 never executed
    #####:  184:      return failure();
    #####:  185:    newOperands.push_back(*paddedOperand);
call    0 never executed
        -:  186:  }
        -:  187:
    #####:  188:  SmallVector<SmallVector<Value>> reifiedResultShapes;
call    0 never executed
    #####:  189:  if (failed(cast<ReifyRankedShapedTypeOpInterface>(opToPad.getOperation())
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  190:                 .reifyResultShapes(b, reifiedResultShapes)))
    #####:  191:    return failure();
    #####:  192:  assert(reifiedResultShapes.size() == opToPad->getNumResults() &&
branch  0 never executed
branch  1 never executed
call    2 never executed
        -:  193:         "expected same number of results");
        -:  194:
        -:  195:  // Clone `opToPad` to operate on the statically padded shapes.
    #####:  196:  auto resultTensorTypes =
call    0 never executed
    #####:  197:      ValueRange(newOperands).take_back(opToPad.getNumDpsInits()).getTypes();
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
    #####:  198:  paddedOp = opToPad.clone(b, loc, resultTensorTypes, newOperands);
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  199:
        -:  200:  // Recover the slice out of the new static results. This keeps the original
        -:  201:  // linalg op around because it uses the dims of the original results.
    #####:  202:  SmallVector<Value> paddedSubviewResults;
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  203:  paddedSubviewResults.reserve(opToPad->getNumResults());
branch  0 never executed
branch  1 never executed
    #####:  204:  for (const auto &en : llvm::enumerate(paddedOp->getResults())) {
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
call    4 never executed
    #####:  205:    Value paddedResult = en.value();
call    0 never executed
    #####:  206:    int64_t resultNumber = en.index();
call    0 never executed
    #####:  207:    int64_t rank = paddedResult.getType().cast<RankedTensorType>().getRank();
call    0 never executed
call    1 never executed
    #####:  208:    SmallVector<OpFoldResult> offsets(rank, b.getIndexAttr(0));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  209:    SmallVector<OpFoldResult> sizes;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  210:    for (Value v : reifiedResultShapes[resultNumber])
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  211:      sizes.push_back(getAsOpFoldResult(v));
call    0 never executed
call    1 never executed
    #####:  212:    SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  213:    paddedSubviewResults.push_back(b.create<tensor::ExtractSliceOp>(
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  214:        loc, paddedResult, offsets, sizes, strides));
call    0 never executed
call    1 never executed
        -:  215:  }
    #####:  216:  return paddedSubviewResults;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  217:}
        -:  218:
        -:  219:/// Try to peel a loop `op` and return the new result.
        -:  220:// TODO: Add support for scf.parallel and affine.for loops.
function _ZN4mlir6linalg8peelLoopERNS_12RewriterBaseEPNS_9OperationE called 0 returned 0% blocks executed 0%
    #####:  221:SmallVector<Value> mlir::linalg::peelLoop(RewriterBase &rewriter,
        -:  222:                                          Operation *op) {
    #####:  223:  return llvm::TypeSwitch<Operation *, SmallVector<Value, 4>>(op)
call    0 never executed
function _ZZN4mlir6linalg8peelLoopERNS_12RewriterBaseEPNS_9OperationEENKUlNS_3scf5ForOpEE_clES6_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  224:      .Case<scf::ForOp>([&](scf::ForOp forOp) {
    #####:  225:        scf::ForOp partialIteration;
call    0 never executed
    #####:  226:        if (succeeded(scf::peelAndCanonicalizeForLoop(rewriter, forOp,
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  227:                                                      partialIteration)))
    #####:  228:          return partialIteration->getResults();
branch  0 never executed
branch  1 never executed
    #####:  229:        assert(!partialIteration && "expected that loop was not peeled");
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  230:        return forOp->getResults();
branch  0 never executed
branch  1 never executed
    #####:  231:      })
call    0 never executed
    #####:  232:      .Default([&](Operation *op) { return op->getResults(); });
call    0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
branch  6 never executed
        -:  233:}
        -:  234:
        -:  235:/// Peel and canonicalize 'loops'.
function _ZN4mlir6linalg9peelLoopsERNS_12RewriterBaseEN4llvm8ArrayRefINS_3scf5ForOpEEE called 0 returned 0% blocks executed 0%
    #####:  236:void mlir::linalg::peelLoops(RewriterBase &rewriter,
        -:  237:                             ArrayRef<scf::ForOp> loops) {
    #####:  238:  for (auto loopOp : loops)
branch  0 never executed
branch  1 never executed
    #####:  239:    peelLoop(rewriter, loopOp);
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  240:}
        -:  241:
        -:  242:/// Linalg padding pattern.
function _ZN4mlir6linalg20LinalgPaddingPatternC2EPNS_11MLIRContextENS0_20LinalgPaddingOptionsENS_14PatternBenefitE called 0 returned 0% blocks executed 0%
    #####:  243:mlir::linalg::LinalgPaddingPattern::LinalgPaddingPattern(
    #####:  244:    MLIRContext *context, LinalgPaddingOptions options, PatternBenefit benefit)
        -:  245:    : OpInterfaceRewritePattern<LinalgOp>(context, benefit),
    #####:  246:      options(std::move(options)) {}
call    0 never executed
call    1 never executed
        -:  247:
        -:  248:FailureOr<LinalgOp>
function _ZNK4mlir6linalg20LinalgPaddingPattern24returningMatchAndRewriteENS0_8LinalgOpERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  249:mlir::linalg::LinalgPaddingPattern::returningMatchAndRewrite(
        -:  250:    LinalgOp linalgOp, PatternRewriter &rewriter) const {
    #####:  251:  if (!linalgOp.hasTensorSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  252:    return failure();
        -:  253:
        -:  254:  // Pad the operation.
    #####:  255:  LinalgOp paddedOp;
call    0 never executed
    #####:  256:  FailureOr<SmallVector<Value>> newResults =
        -:  257:      rewriteAsPaddedOp(rewriter, linalgOp, options.paddingDimensions,
    #####:  258:                        options.paddingValues, options.packPaddings, paddedOp);
call    0 never executed
    #####:  259:  if (failed(newResults))
branch  0 never executed
branch  1 never executed
    #####:  260:    return failure();
        -:  261:
        -:  262:  // Hoist the padding.
    #####:  263:  for (const auto &en : enumerate(options.hoistPaddings)) {
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  264:    if (static_cast<int64_t>(en.index()) >= paddedOp->getNumOperands())
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  265:      break;
    #####:  266:    OpOperand &opOperand = paddedOp->getOpOperand(en.index());
call    0 never executed
    #####:  267:    auto padOp = opOperand.get().getDefiningOp<tensor::PadOp>();
call    0 never executed
    #####:  268:    if (!padOp || en.value() == 0)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  269:      continue;
        -:  270:
        -:  271:    // Fail hoisting if the operand shape is not fully static.
    #####:  272:    if (llvm::any_of(paddedOp.getShape(&opOperand), ShapedType::isDynamic))
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
    #####:  273:      return failure();
        -:  274:
    #####:  275:    tensor::PadOp hoistedOp;
branch  0 never executed
branch  1 never executed
    #####:  276:    SmallVector<GenericOp> transposeOps;
branch  0 never executed
branch  1 never executed
    #####:  277:    SmallVector<int64_t> transposeVector =
    #####:  278:        en.index() < options.transposePaddings.size()
branch  0 never executed
branch  1 never executed
    #####:  279:            ? options.transposePaddings[en.index()]
branch  0 never executed
branch  1 never executed
    #####:  280:            : SmallVector<int64_t>{};
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
        -:  281:
    #####:  282:    FailureOr<Value> newResult = hoistPaddingOnTensors(
call    0 never executed
    #####:  283:        padOp, en.value(), transposeVector, hoistedOp, transposeOps);
call    0 never executed
    #####:  284:    if (failed(newResult))
branch  0 never executed
branch  1 never executed
    #####:  285:      continue;
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  286:    rewriter.replaceOp(padOp, *newResult);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  287:  }
        -:  288:
        -:  289:  // Replace the original operation to pad.
    #####:  290:  rewriter.replaceOp(linalgOp, *newResults);
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
        -:  291:
    #####:  292:  return paddedOp;
branch  0 never executed
branch  1 never executed
        -:  293:}
        -:  294:
function _ZNK4mlir6linalg24CopyVectorizationPattern15matchAndRewriteENS_6memref6CopyOpERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  295:LogicalResult mlir::linalg::CopyVectorizationPattern::matchAndRewrite(
        -:  296:    memref::CopyOp copyOp, PatternRewriter &rewriter) const {
    #####:  297:  return vectorizeCopy(rewriter, copyOp);
call    0 never executed
        -:  298:}
        -:  299:
    #####:  300:static SmallVector<StringRef> getNParallelLoopsAttrs(unsigned nParallelLoops) {
    #####:  301:  return SmallVector<StringRef>(nParallelLoops, getParallelIteratorTypeName());
        -:  302:}
        -:  303:
        -:  304:/// Rewrite a tensor::PadOp into a sequence of EmptyOp, FillOp (to
        -:  305:/// initialize with pad_val) and GenericOp (to copy contents).
        -:  306:LogicalResult
function _ZNK4mlir6linalg26PadOpTransformationPattern15matchAndRewriteENS_6tensor5PadOpERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  307:PadOpTransformationPattern::matchAndRewrite(tensor::PadOp padOp,
        -:  308:                                            PatternRewriter &rewriter) const {
        -:  309:
    #####:  310:  auto inputShapedType = padOp.getSource().getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  311:  auto resultShapedType = padOp.getResult().getType().cast<ShapedType>();
call    0 never executed
call    1 never executed
call    2 never executed
        -:  312:
        -:  313:  // Bail on non-static shapes.
    #####:  314:  if (!inputShapedType.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  315:    return failure();
    #####:  316:  if (!resultShapedType.hasStaticShape())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  317:    return failure();
        -:  318:
        -:  319:  // Only support padding with a constant for now, i.e. either:
        -:  320:  //   1. A BBarg from a different block.
        -:  321:  //   2. A value defined outside of the current block.
    #####:  322:  Block &block = padOp.getRegion().front();
call    0 never executed
call    1 never executed
    #####:  323:  auto yieldOp = cast<tensor::YieldOp>(block.getTerminator());
call    0 never executed
call    1 never executed
    #####:  324:  Value padValue = yieldOp.getValue();
call    0 never executed
    #####:  325:  Operation *definingOp = padValue.getDefiningOp();
call    0 never executed
    #####:  326:  if (definingOp && definingOp->getBlock() == &block)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  327:    return failure();
    #####:  328:  if (!definingOp && padValue.cast<BlockArgument>().getOwner() == &block)
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  329:    return failure();
        -:  330:
        -:  331:  // Create tensor with the padded shape
    #####:  332:  Location loc = padOp.getLoc();
call    0 never executed
    #####:  333:  SmallVector<Value> indices(resultShapedType.getRank(),
call    0 never executed
    #####:  334:                             rewriter.create<arith::ConstantIndexOp>(loc, 0));
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  335:  Value emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  336:      loc, resultShapedType.getShape(), resultShapedType.getElementType());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  337:
        -:  338:  // Initialize tensor with the pad value
    #####:  339:  Value tmpTensor = rewriter
    #####:  340:                        .create<linalg::FillOp>(loc, ValueRange{padValue},
    #####:  341:                                                ValueRange{emptyTensor})
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  342:                        .result();
call    0 never executed
        -:  343:
        -:  344:  // Copy original contents into new tensor
        -:  345:  // Uses linalg.generic, but could be done with tensor.insert_slice
    #####:  346:  SmallVector<AffineExpr, 4> outputExprs;
branch  0 never executed
branch  1 never executed
    #####:  347:  for (unsigned i = 0; i < resultShapedType.getRank(); ++i) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  348:    outputExprs.push_back(getAffineDimExpr(i, rewriter.getContext()) +
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  349:                          padOp.getStaticLow()[i].cast<IntegerAttr>().getInt());
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  350:  }
        -:  351:
    #####:  352:  SmallVector<AffineMap, 2> transferMaps = {
    #####:  353:      rewriter.getMultiDimIdentityMap(inputShapedType.getRank()),
call    0 never executed
call    1 never executed
    #####:  354:      AffineMap::get(resultShapedType.getRank(),
call    0 never executed
    #####:  355:                     /*symbolCount=*/0, outputExprs, rewriter.getContext())};
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  356:
    #####:  357:  rewriter.replaceOpWithNewOp<linalg::GenericOp>(
    #####:  358:      padOp, resultShapedType, padOp.getSource(), tmpTensor, transferMaps,
branch  0 never executed
branch  1 never executed
    #####:  359:      getNParallelLoopsAttrs(resultShapedType.getRank()),
call    0 never executed
call    1 never executed
function _ZZNK4mlir6linalg26PadOpTransformationPattern15matchAndRewriteENS_6tensor5PadOpERNS_15PatternRewriterEENKUlRNS_9OpBuilderENS_8LocationENS_10ValueRangeEE_clES7_S8_S9_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  360:      [&](OpBuilder &nestedBuilder, Location nestedLoc, ValueRange args) {
    #####:  361:        nestedBuilder.create<linalg::YieldOp>(nestedLoc, args[0]);
call    0 never executed
call    1 never executed
    #####:  362:      });
call    0 never executed
call    1 never executed
        -:  363:
    #####:  364:  return success();
branch  0 never executed
branch  1 never executed
        -:  365:}
        -:  366:
        -:  367:/// Filling `dest` using FillOp constant padding value if possible.
        -:  368:/// Otherwise, generate a tensor::GenerateOp.
function _ZNK4mlir6linalg22GeneralizePadOpPattern22createFillOrGenerateOpERNS_15PatternRewriterENS_6tensor5PadOpENS_5ValueERKN4llvm11SmallVectorIS6_Lj6EEE called 0 returned 0% blocks executed 0%
    #####:  369:Value GeneralizePadOpPattern::createFillOrGenerateOp(
        -:  370:    PatternRewriter &rewriter, tensor::PadOp padOp, Value dest,
        -:  371:    const SmallVector<Value> &dynSizes) const {
    #####:  372:  auto padValue = padOp.getConstantPaddingValue();
call    0 never executed
    #####:  373:  if (padValue)
branch  0 never executed
branch  1 never executed
    #####:  374:    return rewriter.create<FillOp>(padOp.getLoc(), padValue, dest).result();
call    0 never executed
call    1 never executed
        -:  375:
        -:  376:  // Fill could not be optimized: Lower to tensor::GenerateOp with region.
    #####:  377:  auto generateOp = rewriter.create<tensor::GenerateOp>(
    #####:  378:      padOp.getLoc(), padOp.getResultType(), dynSizes);
call    0 never executed
call    1 never executed
        -:  379:  // Copy region to new op.
    #####:  380:  BlockAndValueMapping bvm;
call    0 never executed
call    1 never executed
    #####:  381:  padOp.getRegion().cloneInto(&generateOp.getRegion(), bvm);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  382:  return generateOp;
call    0 never executed
        -:  383:}
        -:  384:
        -:  385:LogicalResult
function _ZNK4mlir6linalg22GeneralizePadOpPattern15matchAndRewriteENS_6tensor5PadOpERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  386:GeneralizePadOpPattern::matchAndRewrite(tensor::PadOp padOp,
        -:  387:                                        PatternRewriter &rewriter) const {
        -:  388:  // Given an OpFoldResult, return an index-typed value.
function _ZZNK4mlir6linalg22GeneralizePadOpPattern15matchAndRewriteENS_6tensor5PadOpERNS_15PatternRewriterEENKUlNS_12OpFoldResultEE_clES6_.isra.0 called 0 returned 0% blocks executed 0%
    #####:  389:  auto getIdxValue = [&](OpFoldResult ofr) {
    #####:  390:    if (auto val = ofr.dyn_cast<Value>())
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  391:      return val;
    #####:  392:    return rewriter
    #####:  393:        .create<arith::ConstantIndexOp>(
    #####:  394:            padOp.getLoc(), ofr.get<Attribute>().cast<IntegerAttr>().getInt())
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
    #####:  395:        .getResult();
call    0 never executed
    #####:  396:  };
        -:  397:
    #####:  398:  auto resultType = padOp.getResultType();
call    0 never executed
        -:  399:  // Compute size of EmptyOp. Any combination of static/dynamic is supported.
    #####:  400:  SmallVector<Value> dynSizes;
    #####:  401:  SmallVector<int64_t> staticSizes;
branch  0 never executed
branch  1 never executed
    #####:  402:  for (unsigned dim = 0; dim < resultType.getRank(); ++dim) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  403:    if (resultType.isDynamicDim(dim)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  404:      auto srcSize = rewriter.createOrFold<tensor::DimOp>(
    #####:  405:          padOp.getLoc(), padOp.getSource(), dim);
call    0 never executed
call    1 never executed
        -:  406:      // Add low and high padding value.
    #####:  407:      auto plusLow = rewriter.createOrFold<arith::AddIOp>(
    #####:  408:          padOp.getLoc(), srcSize, getIdxValue(padOp.getMixedLowPad()[dim]));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####:  409:      auto plusHigh = rewriter.createOrFold<arith::AddIOp>(
    #####:  410:          padOp.getLoc(), plusLow, getIdxValue(padOp.getMixedHighPad()[dim]));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####:  411:      dynSizes.push_back(plusHigh);
call    0 never executed
        -:  412:    }
    #####:  413:    staticSizes.push_back(resultType.getDimSize(dim));
call    0 never executed
call    1 never executed
        -:  414:  }
        -:  415:
        -:  416:  // Init tensor and fill it with padding.
    #####:  417:  Value emptyTensor = rewriter.create<tensor::EmptyOp>(
    #####:  418:      padOp.getLoc(), staticSizes, resultType.getElementType(), dynSizes);
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  419:  Value fill = createFillOrGenerateOp(rewriter, padOp, emptyTensor, dynSizes);
call    0 never executed
        -:  420:
        -:  421:  // Try optimize the copy of source.
    #####:  422:  if (optimizeCopyFn && optimizeCopyFn(rewriter, padOp, fill).succeeded())
branch  0 never executed
branch  1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  423:    return success();
        -:  424:
        -:  425:  // tensor::PadOps cannot be optimized. Generate a InsertSliceOp instead
        -:  426:  // for copying the PadOp source.
    #####:  427:  auto sourceType = padOp.getSourceType();
call    0 never executed
        -:  428:  // Compute size of source of tensor::PadOp.
    #####:  429:  SmallVector<OpFoldResult> srcSizes;
branch  0 never executed
branch  1 never executed
    #####:  430:  for (unsigned dim = 0; dim < sourceType.getRank(); ++dim) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  431:    if (sourceType.isDynamicDim(dim)) {
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  432:      srcSizes.push_back(rewriter.createOrFold<tensor::DimOp>(
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  433:          padOp.getLoc(), padOp.getSource(), dim));
call    0 never executed
        -:  434:    } else {
    #####:  435:      srcSizes.push_back(rewriter.getIndexAttr(sourceType.getDimSize(dim)));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  436:    }
        -:  437:  }
        -:  438:  // Strides of InsertSliceOp are all 1.
    #####:  439:  SmallVector<OpFoldResult> strides(sourceType.getRank(),
    #####:  440:                                    rewriter.getIndexAttr(1));
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
branch  4 never executed
branch  5 never executed
    #####:  441:  rewriter.replaceOpWithNewOp<tensor::InsertSliceOp>(
    #####:  442:      padOp, padOp.getSource(), fill, padOp.getMixedLowPad(), srcSizes,
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  443:      strides);
call    0 never executed
call    1 never executed
        -:  444:
    #####:  445:  return success();
branch  0 never executed
branch  1 never executed
        -:  446:}
        -:  447:
function _ZNK4mlir6linalg34ExtractSliceOfPadTensorSwapPattern15matchAndRewriteENS_6tensor14ExtractSliceOpERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  448:LogicalResult ExtractSliceOfPadTensorSwapPattern::matchAndRewrite(
        -:  449:    tensor::ExtractSliceOp sliceOp, PatternRewriter &rewriter) const {
    #####:  450:  if (!sliceOp.hasUnitStride())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  451:    return failure();
        -:  452:
    #####:  453:  auto padOp = sliceOp.getSource().getDefiningOp<tensor::PadOp>();
call    0 never executed
call    1 never executed
    #####:  454:  if (!padOp)
branch  0 never executed
branch  1 never executed
    #####:  455:    return failure();
        -:  456:
    #####:  457:  bool zeroSliceGuard = true;
    #####:  458:  if (controlFn) {
branch  0 never executed
branch  1 never executed
    #####:  459:    if (Optional<bool> control = controlFn(sliceOp))
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  460:      zeroSliceGuard = *control;
        -:  461:    else
    #####:  462:      return failure();
        -:  463:  }
        -:  464:
    #####:  465:  Operation *tiledPadOp =
    #####:  466:      tensor::bubbleUpPadSlice(rewriter, padOp, sliceOp.getMixedOffsets(),
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
branch  4 never executed
branch  5 never executed
    #####:  467:                               sliceOp.getMixedSizes(), zeroSliceGuard);
call    0 never executed
call    1 never executed
        -:  468:  // All shapes are static and the data source is actually used. Rewrite into
        -:  469:  // pad(extract_slice(x)).
    #####:  470:  rewriter.replaceOp(sliceOp, tiledPadOp->getResults());
branch  0 never executed
branch  1 never executed
call    2 never executed
call    3 never executed
    #####:  471:  return success();
        -:  472:}
        -:  473:
        -:  474:// The following are patterns for downscaling convolution ops with size-1
        -:  475:// window dimensions.
        -:  476://
        -:  477:// Note that we'd eventually want to write such transformations in a generic
        -:  478:// way, e.g., converting to linalg.generic, removing the size-1 dimensions,
        -:  479:// and then turning back to named ops. But for now it's fine to have a few
        -:  480:// patterns matching special ops to get started.
        -:  481:
        -:  482:template <typename Conv2DOp, typename Conv1DOp>
    #####:  483:FailureOr<Conv1DOp> DownscaleSizeOneWindowed2DConvolution<Conv2DOp, Conv1DOp>::
        -:  484:    returningMatchAndRewrite(Conv2DOp convOp, PatternRewriter &rewriter) const {
    #####:  485:  if (convOp.hasBufferSemantics())
    #####:  486:    return failure(); // To be implemented.
        -:  487:
    #####:  488:  Value input = convOp.getInputs().front();
    #####:  489:  Value kernel = convOp.getInputs().back();
    #####:  490:  Value output = convOp.getOutputs().front();
        -:  491:
    #####:  492:  auto inputType = input.getType().dyn_cast<RankedTensorType>();
    #####:  493:  auto kernelType = kernel.getType().dyn_cast<RankedTensorType>();
    #####:  494:  auto outputType = output.getType().dyn_cast<RankedTensorType>();
        -:  495:
    #####:  496:  auto kernelShape = kernelType.getShape();
    #####:  497:  auto outputShape = outputType.getShape();
        -:  498:
        -:  499:  // Get domain indices based on conv2D layout.
        -:  500:  int khIndex, kwIndex, ohIndex, owIndex;
        -:  501:
    #####:  502:  TypeSwitch<Operation *>(convOp)
    #####:  503:      .Case([&](linalg::Conv2DNhwcHwcfOp op) {
    #####:  504:        khIndex = 0;
    #####:  505:        kwIndex = 1;
    #####:  506:        ohIndex = 1;
    #####:  507:        owIndex = 2;
        -:  508:      })
    #####:  509:      .Case([&](linalg::Conv2DNchwFchwOp op) {
    #####:  510:        khIndex = 2;
    #####:  511:        kwIndex = 3;
    #####:  512:        ohIndex = 2;
    #####:  513:        owIndex = 3;
        -:  514:      })
    #####:  515:      .Default([&](Operation *op) {
    #####:  516:        llvm_unreachable("unexpected conv2d operation.");
        -:  517:      });
        -:  518:
        -:  519:  // Only handle the case where at least one of the window dimensions is
        -:  520:  // of size 1. Other cases can rely on tiling to reduce to such cases.
    #####:  521:  int64_t khSize = kernelShape[khIndex], kwSize = kernelShape[kwIndex];
    #####:  522:  int64_t ohSize = outputShape[ohIndex], owSize = outputShape[owIndex];
    #####:  523:  bool removeH = (khSize == 1 && ohSize == 1);
    #####:  524:  bool removeW = (kwSize == 1 && owSize == 1);
    #####:  525:  if (!removeH && !removeW)
    #####:  526:    return failure();
        -:  527:
        -:  528:  // Get new shapes and types for all operands by removing the size-1
        -:  529:  // dimension.
        -:  530:  using RTTBuilder = RankedTensorType::Builder;
    #####:  531:  RankedTensorType newInputType =
    #####:  532:      RTTBuilder(inputType).dropDim((removeH ? ohIndex : owIndex));
    #####:  533:  RankedTensorType newKernelType =
    #####:  534:      RTTBuilder(kernelType).dropDim((removeH ? khIndex : kwIndex));
    #####:  535:  RankedTensorType newOutputType =
    #####:  536:      RTTBuilder(outputType).dropDim((removeH ? ohIndex : owIndex));
        -:  537:
        -:  538:  // Rank-reduce operands.
    #####:  539:  Location loc = convOp.getLoc();
    #####:  540:  Value newInput = tensor::createCanonicalRankReducingExtractSliceOp(
        -:  541:      rewriter, loc, input, newInputType);
    #####:  542:  Value newKernel = tensor::createCanonicalRankReducingExtractSliceOp(
        -:  543:      rewriter, loc, kernel, newKernelType);
    #####:  544:  Value newOutput = tensor::createCanonicalRankReducingExtractSliceOp(
        -:  545:      rewriter, loc, output, newOutputType);
        -:  546:
        -:  547:  // Rank-reduce strides and dilations too.
        -:  548:  // TODO: dropDim 1-liner helper.
    #####:  549:  auto strides =
    #####:  550:      llvm::to_vector<4>(convOp.getStrides().template getValues<int64_t>());
    #####:  551:  strides.erase(strides.begin() + (removeH ? 0 : 1));
    #####:  552:  auto stridesAttr = rewriter.getI64VectorAttr(strides);
        -:  553:
    #####:  554:  auto dilations =
    #####:  555:      llvm::to_vector<4>(convOp.getDilations().template getValues<int64_t>());
    #####:  556:  dilations.erase(dilations.begin() + (removeH ? 0 : 1));
    #####:  557:  auto dilationsAttr = rewriter.getI64VectorAttr(dilations);
        -:  558:
    #####:  559:  auto conv1DOp = rewriter.create<Conv1DOp>(
        -:  560:      loc, newOutputType, ValueRange{newInput, newKernel},
        -:  561:      ValueRange{newOutput}, stridesAttr, dilationsAttr);
        -:  562:
        -:  563:  // Insert back.
    #####:  564:  Value inserted = tensor::createCanonicalRankReducingInsertSliceOp(
        -:  565:      rewriter, loc, conv1DOp.getResult(0), output);
    #####:  566:  rewriter.replaceOp(convOp, inserted);
        -:  567:
    #####:  568:  return conv1DOp;
        -:  569:}
------------------
_ZNK4mlir6linalg37DownscaleSizeOneWindowed2DConvolutionINS0_16Conv2DNchwFchwOpENS0_14Conv1DNcwFcwOpEE24returningMatchAndRewriteES2_RNS_15PatternRewriterE:
function _ZNK4mlir6linalg37DownscaleSizeOneWindowed2DConvolutionINS0_16Conv2DNchwFchwOpENS0_14Conv1DNcwFcwOpEE24returningMatchAndRewriteES2_RNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  483:FailureOr<Conv1DOp> DownscaleSizeOneWindowed2DConvolution<Conv2DOp, Conv1DOp>::
        -:  484:    returningMatchAndRewrite(Conv2DOp convOp, PatternRewriter &rewriter) const {
    #####:  485:  if (convOp.hasBufferSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  486:    return failure(); // To be implemented.
        -:  487:
    #####:  488:  Value input = convOp.getInputs().front();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  489:  Value kernel = convOp.getInputs().back();
call    0 never executed
call    1 never executed
    #####:  490:  Value output = convOp.getOutputs().front();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  491:
    #####:  492:  auto inputType = input.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  493:  auto kernelType = kernel.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  494:  auto outputType = output.getType().dyn_cast<RankedTensorType>();
call    0 never executed
        -:  495:
    #####:  496:  auto kernelShape = kernelType.getShape();
call    0 never executed
    #####:  497:  auto outputShape = outputType.getShape();
call    0 never executed
        -:  498:
        -:  499:  // Get domain indices based on conv2D layout.
        -:  500:  int khIndex, kwIndex, ohIndex, owIndex;
        -:  501:
    #####:  502:  TypeSwitch<Operation *>(convOp)
call    0 never executed
        -:  503:      .Case([&](linalg::Conv2DNhwcHwcfOp op) {
        -:  504:        khIndex = 0;
        -:  505:        kwIndex = 1;
        -:  506:        ohIndex = 1;
        -:  507:        owIndex = 2;
        -:  508:      })
    #####:  509:      .Case([&](linalg::Conv2DNchwFchwOp op) {
call    0 never executed
        -:  510:        khIndex = 2;
        -:  511:        kwIndex = 3;
        -:  512:        ohIndex = 2;
        -:  513:        owIndex = 3;
        -:  514:      })
    #####:  515:      .Default([&](Operation *op) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  516:        llvm_unreachable("unexpected conv2d operation.");
        -:  517:      });
        -:  518:
        -:  519:  // Only handle the case where at least one of the window dimensions is
        -:  520:  // of size 1. Other cases can rely on tiling to reduce to such cases.
    #####:  521:  int64_t khSize = kernelShape[khIndex], kwSize = kernelShape[kwIndex];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  522:  int64_t ohSize = outputShape[ohIndex], owSize = outputShape[owIndex];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  523:  bool removeH = (khSize == 1 && ohSize == 1);
    #####:  524:  bool removeW = (kwSize == 1 && owSize == 1);
    #####:  525:  if (!removeH && !removeW)
branch  0 never executed
branch  1 never executed
    #####:  526:    return failure();
        -:  527:
        -:  528:  // Get new shapes and types for all operands by removing the size-1
        -:  529:  // dimension.
        -:  530:  using RTTBuilder = RankedTensorType::Builder;
    #####:  531:  RankedTensorType newInputType =
branch  0 never executed
branch  1 never executed
    #####:  532:      RTTBuilder(inputType).dropDim((removeH ? ohIndex : owIndex));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
    #####:  533:  RankedTensorType newKernelType =
branch  0 never executed
branch  1 never executed
    #####:  534:      RTTBuilder(kernelType).dropDim((removeH ? khIndex : kwIndex));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
    #####:  535:  RankedTensorType newOutputType =
branch  0 never executed
branch  1 never executed
    #####:  536:      RTTBuilder(outputType).dropDim((removeH ? ohIndex : owIndex));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
        -:  537:
        -:  538:  // Rank-reduce operands.
    #####:  539:  Location loc = convOp.getLoc();
call    0 never executed
    #####:  540:  Value newInput = tensor::createCanonicalRankReducingExtractSliceOp(
call    0 never executed
        -:  541:      rewriter, loc, input, newInputType);
    #####:  542:  Value newKernel = tensor::createCanonicalRankReducingExtractSliceOp(
call    0 never executed
        -:  543:      rewriter, loc, kernel, newKernelType);
    #####:  544:  Value newOutput = tensor::createCanonicalRankReducingExtractSliceOp(
call    0 never executed
        -:  545:      rewriter, loc, output, newOutputType);
        -:  546:
        -:  547:  // Rank-reduce strides and dilations too.
        -:  548:  // TODO: dropDim 1-liner helper.
    #####:  549:  auto strides =
call    0 never executed
    #####:  550:      llvm::to_vector<4>(convOp.getStrides().template getValues<int64_t>());
call    0 never executed
call    1 never executed
    #####:  551:  strides.erase(strides.begin() + (removeH ? 0 : 1));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  552:  auto stridesAttr = rewriter.getI64VectorAttr(strides);
call    0 never executed
        -:  553:
    #####:  554:  auto dilations =
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  555:      llvm::to_vector<4>(convOp.getDilations().template getValues<int64_t>());
call    0 never executed
call    1 never executed
    #####:  556:  dilations.erase(dilations.begin() + (removeH ? 0 : 1));
call    0 never executed
    #####:  557:  auto dilationsAttr = rewriter.getI64VectorAttr(dilations);
call    0 never executed
        -:  558:
    #####:  559:  auto conv1DOp = rewriter.create<Conv1DOp>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  560:      loc, newOutputType, ValueRange{newInput, newKernel},
        -:  561:      ValueRange{newOutput}, stridesAttr, dilationsAttr);
        -:  562:
        -:  563:  // Insert back.
    #####:  564:  Value inserted = tensor::createCanonicalRankReducingInsertSliceOp(
call    0 never executed
        -:  565:      rewriter, loc, conv1DOp.getResult(0), output);
    #####:  566:  rewriter.replaceOp(convOp, inserted);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  567:
    #####:  568:  return conv1DOp;
branch  0 never executed
branch  1 never executed
        -:  569:}
------------------
_ZNK4mlir6linalg37DownscaleSizeOneWindowed2DConvolutionINS0_16Conv2DNhwcHwcfOpENS0_14Conv1DNwcWcfOpEE24returningMatchAndRewriteES2_RNS_15PatternRewriterE:
function _ZNK4mlir6linalg37DownscaleSizeOneWindowed2DConvolutionINS0_16Conv2DNhwcHwcfOpENS0_14Conv1DNwcWcfOpEE24returningMatchAndRewriteES2_RNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  483:FailureOr<Conv1DOp> DownscaleSizeOneWindowed2DConvolution<Conv2DOp, Conv1DOp>::
        -:  484:    returningMatchAndRewrite(Conv2DOp convOp, PatternRewriter &rewriter) const {
    #####:  485:  if (convOp.hasBufferSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  486:    return failure(); // To be implemented.
        -:  487:
    #####:  488:  Value input = convOp.getInputs().front();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  489:  Value kernel = convOp.getInputs().back();
call    0 never executed
call    1 never executed
    #####:  490:  Value output = convOp.getOutputs().front();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  491:
    #####:  492:  auto inputType = input.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  493:  auto kernelType = kernel.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  494:  auto outputType = output.getType().dyn_cast<RankedTensorType>();
call    0 never executed
        -:  495:
    #####:  496:  auto kernelShape = kernelType.getShape();
call    0 never executed
    #####:  497:  auto outputShape = outputType.getShape();
call    0 never executed
        -:  498:
        -:  499:  // Get domain indices based on conv2D layout.
        -:  500:  int khIndex, kwIndex, ohIndex, owIndex;
        -:  501:
    #####:  502:  TypeSwitch<Operation *>(convOp)
call    0 never executed
        -:  503:      .Case([&](linalg::Conv2DNhwcHwcfOp op) {
        -:  504:        khIndex = 0;
        -:  505:        kwIndex = 1;
        -:  506:        ohIndex = 1;
        -:  507:        owIndex = 2;
        -:  508:      })
    #####:  509:      .Case([&](linalg::Conv2DNchwFchwOp op) {
call    0 never executed
        -:  510:        khIndex = 2;
        -:  511:        kwIndex = 3;
        -:  512:        ohIndex = 2;
        -:  513:        owIndex = 3;
        -:  514:      })
    #####:  515:      .Default([&](Operation *op) {
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  516:        llvm_unreachable("unexpected conv2d operation.");
        -:  517:      });
        -:  518:
        -:  519:  // Only handle the case where at least one of the window dimensions is
        -:  520:  // of size 1. Other cases can rely on tiling to reduce to such cases.
    #####:  521:  int64_t khSize = kernelShape[khIndex], kwSize = kernelShape[kwIndex];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  522:  int64_t ohSize = outputShape[ohIndex], owSize = outputShape[owIndex];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  523:  bool removeH = (khSize == 1 && ohSize == 1);
    #####:  524:  bool removeW = (kwSize == 1 && owSize == 1);
    #####:  525:  if (!removeH && !removeW)
branch  0 never executed
branch  1 never executed
    #####:  526:    return failure();
        -:  527:
        -:  528:  // Get new shapes and types for all operands by removing the size-1
        -:  529:  // dimension.
        -:  530:  using RTTBuilder = RankedTensorType::Builder;
    #####:  531:  RankedTensorType newInputType =
branch  0 never executed
branch  1 never executed
    #####:  532:      RTTBuilder(inputType).dropDim((removeH ? ohIndex : owIndex));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
    #####:  533:  RankedTensorType newKernelType =
branch  0 never executed
branch  1 never executed
    #####:  534:      RTTBuilder(kernelType).dropDim((removeH ? khIndex : kwIndex));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
    #####:  535:  RankedTensorType newOutputType =
branch  0 never executed
branch  1 never executed
    #####:  536:      RTTBuilder(outputType).dropDim((removeH ? ohIndex : owIndex));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
        -:  537:
        -:  538:  // Rank-reduce operands.
    #####:  539:  Location loc = convOp.getLoc();
call    0 never executed
    #####:  540:  Value newInput = tensor::createCanonicalRankReducingExtractSliceOp(
call    0 never executed
        -:  541:      rewriter, loc, input, newInputType);
    #####:  542:  Value newKernel = tensor::createCanonicalRankReducingExtractSliceOp(
call    0 never executed
        -:  543:      rewriter, loc, kernel, newKernelType);
    #####:  544:  Value newOutput = tensor::createCanonicalRankReducingExtractSliceOp(
call    0 never executed
        -:  545:      rewriter, loc, output, newOutputType);
        -:  546:
        -:  547:  // Rank-reduce strides and dilations too.
        -:  548:  // TODO: dropDim 1-liner helper.
    #####:  549:  auto strides =
call    0 never executed
    #####:  550:      llvm::to_vector<4>(convOp.getStrides().template getValues<int64_t>());
call    0 never executed
call    1 never executed
    #####:  551:  strides.erase(strides.begin() + (removeH ? 0 : 1));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  552:  auto stridesAttr = rewriter.getI64VectorAttr(strides);
call    0 never executed
        -:  553:
    #####:  554:  auto dilations =
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  555:      llvm::to_vector<4>(convOp.getDilations().template getValues<int64_t>());
call    0 never executed
call    1 never executed
    #####:  556:  dilations.erase(dilations.begin() + (removeH ? 0 : 1));
call    0 never executed
    #####:  557:  auto dilationsAttr = rewriter.getI64VectorAttr(dilations);
call    0 never executed
        -:  558:
    #####:  559:  auto conv1DOp = rewriter.create<Conv1DOp>(
call    0 never executed
call    1 never executed
call    2 never executed
call    3 never executed
        -:  560:      loc, newOutputType, ValueRange{newInput, newKernel},
        -:  561:      ValueRange{newOutput}, stridesAttr, dilationsAttr);
        -:  562:
        -:  563:  // Insert back.
    #####:  564:  Value inserted = tensor::createCanonicalRankReducingInsertSliceOp(
call    0 never executed
        -:  565:      rewriter, loc, conv1DOp.getResult(0), output);
    #####:  566:  rewriter.replaceOp(convOp, inserted);
call    0 never executed
call    1 never executed
branch  2 never executed
branch  3 never executed
        -:  567:
    #####:  568:  return conv1DOp;
branch  0 never executed
branch  1 never executed
        -:  569:}
------------------
        -:  570:
        -:  571:template struct linalg::DownscaleSizeOneWindowed2DConvolution<Conv2DNhwcHwcfOp,
        -:  572:                                                              Conv1DNwcWcfOp>;
        -:  573:template struct linalg::DownscaleSizeOneWindowed2DConvolution<Conv2DNchwFchwOp,
        -:  574:                                                              Conv1DNcwFcwOp>;
        -:  575:
        -:  576:FailureOr<DepthwiseConv1DNwcWcOp>
function _ZNK4mlir6linalg33DownscaleDepthwiseConv2DNhwcHwcOp24returningMatchAndRewriteENS0_24DepthwiseConv2DNhwcHwcOpERNS_15PatternRewriterE called 0 returned 0% blocks executed 0%
    #####:  577:DownscaleDepthwiseConv2DNhwcHwcOp::returningMatchAndRewrite(
        -:  578:    DepthwiseConv2DNhwcHwcOp convOp, PatternRewriter &rewriter) const {
    #####:  579:  if (convOp.hasBufferSemantics())
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  580:    return failure(); // To be implemented.
        -:  581:
    #####:  582:  Value input = convOp.getInputs().front();
call    0 never executed
branch  1 never executed
branch  2 never executed
    #####:  583:  Value kernel = convOp.getInputs().back();
call    0 never executed
call    1 never executed
    #####:  584:  Value output = convOp.getOutputs().front();
call    0 never executed
branch  1 never executed
branch  2 never executed
        -:  585:
    #####:  586:  auto inputType = input.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  587:  auto kernelType = kernel.getType().dyn_cast<RankedTensorType>();
call    0 never executed
    #####:  588:  auto outputType = output.getType().dyn_cast<RankedTensorType>();
call    0 never executed
        -:  589:
    #####:  590:  auto kernelShape = kernelType.getShape();
call    0 never executed
    #####:  591:  auto outputShape = outputType.getShape();
call    0 never executed
        -:  592:
        -:  593:  // Only handle the case where at least one of the window dimensions is
        -:  594:  // of size 1. Other cases can rely on tiling to reduce to such cases.
    #####:  595:  int64_t khSize = kernelShape[0], kwSize = kernelShape[1];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  596:  int64_t ohSize = outputShape[1], owSize = outputShape[2];
branch  0 never executed
branch  1 never executed
branch  2 never executed
branch  3 never executed
    #####:  597:  bool removeH = (khSize == 1 && ohSize == 1);
    #####:  598:  bool removeW = (kwSize == 1 && owSize == 1);
    #####:  599:  if (!removeH && !removeW)
branch  0 never executed
branch  1 never executed
    #####:  600:    return failure();
        -:  601:
        -:  602:  // Get new shapes and types for all operands by removing the size-1
        -:  603:  // dimension.
    #####:  604:  using RTTBuilder = RankedTensorType::Builder;
    #####:  605:  RankedTensorType newInputType =
    #####:  606:      RTTBuilder(inputType).dropDim((removeH ? 1 : 2));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####:  607:  RankedTensorType newKernelType =
    #####:  608:      RTTBuilder(kernelType).dropDim((removeH ? 0 : 1));
call    0 never executed
branch  1 never executed
branch  2 never executed
call    3 never executed
call    4 never executed
branch  5 never executed
branch  6 never executed
    #####:  609:  RankedTensorType newOutputType =
    #####:  610:      RTTBuilder(outputType).dropDim(removeH ? 1 : 2);
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
        -:  611:
        -:  612:  // Rank-reduce operands.
    #####:  613:  Location loc = convOp.getLoc();
call    0 never executed
    #####:  614:  Value newInput = tensor::createCanonicalRankReducingExtractSliceOp(
    #####:  615:      rewriter, loc, input, newInputType);
call    0 never executed
    #####:  616:  Value newKernel = tensor::createCanonicalRankReducingExtractSliceOp(
    #####:  617:      rewriter, loc, kernel, newKernelType);
call    0 never executed
    #####:  618:  Value newOutput = tensor::createCanonicalRankReducingExtractSliceOp(
    #####:  619:      rewriter, loc, output, newOutputType);
call    0 never executed
        -:  620:
        -:  621:  // Rank-reduce strides and dilations too.
        -:  622:  // TODO: dropDim 1-liner helper.
    #####:  623:  auto strides = llvm::to_vector<4>(convOp.getStrides().getValues<int64_t>());
call    0 never executed
call    1 never executed
call    2 never executed
    #####:  624:  strides.erase(strides.begin() + (removeH ? 0 : 1));
branch  0 never executed
branch  1 never executed
call    2 never executed
    #####:  625:  auto stridesAttr = rewriter.getI64VectorAttr(strides);
call    0 never executed
        -:  626:
    #####:  627:  auto dilations =
    #####:  628:      llvm::to_vector<4>(convOp.getDilations().getValues<int64_t>());
call    0 never executed
call    1 never executed
call    2 never executed
branch  3 never executed
branch  4 never executed
    #####:  629:  dilations.erase(dilations.begin() + (removeH ? 0 : 1));
call    0 never executed
    #####:  630:  auto dilationsAttr = rewriter.getI64VectorAttr(dilations);
call    0 never executed
        -:  631:
    #####:  632:  auto conv1DOp = rewriter.create<DepthwiseConv1DNwcWcOp>(
    #####:  633:      loc, newOutputType, ValueRange{newInput, newKernel},
    #####:  634:      ValueRange{newOutput}, stridesAttr, dilationsAttr);
call    0 never executed
call    1 never executed
call    2 never executed
        -:  635:
        -:  636:  // Insert back.
    #####:  637:  Value inserted = tensor::createCanonicalRankReducingInsertSliceOp(
call    0 never executed
    #####:  638:      rewriter, loc, conv1DOp.getResult(0), output);
call    0 never executed
    #####:  639:  rewriter.replaceOp(convOp, inserted);
call    0 never executed
call    1 never executed
        -:  640:
    #####:  641:  return conv1DOp;
branch  0 never executed
branch  1 never executed
        -:  642:}
        -:  643:
function _ZN4mlir6linalg36populateDecomposeConvolutionPatternsERNS_17RewritePatternSetENS_14PatternBenefitE called 0 returned 0% blocks executed 0%
    #####:  644:void linalg::populateDecomposeConvolutionPatterns(RewritePatternSet &patterns,
        -:  645:                                                  PatternBenefit benefit) {
    #####:  646:  patterns.add<DownscaleSizeOneWindowed2DConvolution<linalg::Conv2DNhwcHwcfOp,
        -:  647:                                                     Conv1DNwcWcfOp>,
        -:  648:               DownscaleSizeOneWindowed2DConvolution<linalg::Conv2DNchwFchwOp,
        -:  649:                                                     Conv1DNcwFcwOp>,
    #####:  650:               DownscaleDepthwiseConv2DNhwcHwcOp>(patterns.getContext(),
call    0 never executed
    #####:  651:                                                  benefit);
call    0 never executed
    #####:  652:}
